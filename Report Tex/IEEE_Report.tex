\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

% Packages
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{amssymb} % For \checkmark symbol
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{subfig}
\usepackage{float}
\usepackage{url}
\usepackage[hidelinks]{hyperref}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{Decoding Musical Genres: A Comprehensive Study of Unsupervised Clustering on High-Dimensional Audio Data}

\author{
\IEEEauthorblockN{Anirudh Sharma}
\IEEEauthorblockA{\textit{Department of Computer Science and Engineering} \\
\textit{National Institute of Technology Hamirpur}\\
Hamirpur, India \\
Roll No.: 22dcs002\\
email: 22dcs002@nith.ac.in}
}

\maketitle

\begin{abstract}
\textbf{This paper presents a comprehensive investigation into unsupervised music genre discovery through audio feature learning across multiple diverse datasets. We apply dimensionality reduction and clustering techniques to extract meaningful genre patterns without labeled training data. Our study processes five distinct music datasets: GTZAN (1,000 tracks, 10 genres), FMA Small (8,000 tracks, 8 genres), FMA Medium (17,000 tracks, 16 genres), Ludwig (11,300 tracks, 10 genres), and Indian Bollywood Music (500 tracks, 5 regional genres), collectively totaling 37,800 tracks with 69 normalized audio features per track. Through systematic feature extraction using Librosa, comprehensive data integrity validation achieving 99.99\% cleanliness, IQR-based outlier detection revealing 0.58-1.69\% outlier prevalence across key features, robust normalization using StandardScaler, and Principal Component Analysis (PCA) achieving 95\%+ variance retention with 36-44\% dimensionality reduction, we establish a robust foundation for unsupervised genre classification. The preprocessing pipeline demonstrates consistent performance across datasets, with minimal outliers requiring no removal and PCA reducing computational complexity while maintaining information integrity. Our experimental framework provides insights into the effectiveness of unsupervised learning for music genre discovery across Western and Indian musical traditions, establishing benchmarks for future research in audio content analysis. Results indicate that properly validated, normalized, and dimensionally-reduced features enable effective clustering with significant computational savings.}
\end{abstract}

\begin{IEEEkeywords}
Unsupervised Learning, Music Genre Classification, Audio Feature Extraction, Principal Component Analysis, Clustering Algorithms
\end{IEEEkeywords}

\section{\textbf{Introduction}}
\label{sec:introduction}

Music genre classification represents a fundamental challenge in music information retrieval (MIR), with applications spanning music recommendation systems, content organization, and automated playlist generation. Traditional supervised approaches require extensive labeled datasets, which are costly and time-consuming to create. Unsupervised learning offers a compelling alternative by discovering latent genre structures directly from audio features without manual annotations.

\subsection{\textbf{Motivation}}

The exponential growth of digital music libraries necessitates automated genre classification systems. However, genre boundaries are inherently subjective and culturally dependent, making supervised classification challenging. Unsupervised methods can:

\begin{itemize}
    \item Discover hidden genre patterns without labeled data
    \item Identify sub-genres and emerging music styles
    \item Handle cross-cultural and regional music variations
    \item Reduce annotation costs and human bias
    \item Scale to large music collections efficiently
\end{itemize}

\subsection{\textbf{Research Objectives}}

This study aims to:

\begin{enumerate}
    \item Extract and process comprehensive audio features from diverse music datasets
    \item Apply robust normalization and dimensionality reduction techniques
    \item Evaluate multiple unsupervised clustering algorithms for genre discovery
    \item Compare algorithm performance across different dataset characteristics
    \item Establish reproducible benchmarks for music genre clustering
\end{enumerate}

\subsection{\textbf{Contributions}}

Our primary contributions include:

\begin{itemize}
    \item A comprehensive multi-dataset analysis framework spanning Western and Indian music
    \item Systematic comparison of preprocessing techniques across 34,481 total tracks
    \item PCA-based dimensionality reduction achieving 95\%+ variance retention
    \item Reproducible experimental pipeline with open-source implementation
    \item Detailed performance metrics and visualization for each processing stage
\end{itemize}

\section{\textbf{Related Work}}
\label{sec:related}

\subsection{\textbf{Music Genre Classification}}

Tzanetakis and Cook \cite{tzanetakis2002} pioneered automatic music genre classification using timbral, rhythmic, and pitch-based features. Their work on the GTZAN dataset established foundational benchmarks that remain relevant today. Subsequent research has shifted towards self-supervised learning, exploring contrastive learning of musical representations \cite{spijkervet2021} and general-purpose audio embeddings \cite{saeed2021}.

\subsection{\textbf{Unsupervised Learning in MIR}}

Recent studies have demonstrated the effectiveness of unsupervised methods for music analysis. Castellon et al. \cite{castellon2021} investigated clustering-based approaches using codified audio language models to discover musical patterns. Similarly, metric learning approaches have been applied to disentangle musical concepts like genre and mood without explicit supervision \cite{lee2020}. However, systematic comparisons across multiple datasets with varying characteristics remain limited.

\subsection{\textbf{Feature Engineering for Audio}}

Librosa \cite{mcfee2015} has become the de facto standard for audio feature extraction in Python, providing robust implementations of MFCCs, chromagrams, and spectral features. Comprehensive feature sets combining temporal, spectral, and cepstral information have shown superior performance compared to single-feature approaches \cite{sturm2013}.

\subsection{\textbf{Dimensionality Reduction Techniques}}

Principal Component Analysis (PCA) remains widely used for dimensionality reduction in audio applications due to its computational efficiency and interpretability. Alternative approaches include t-SNE for visualization \cite{maaten2008}, autoencoders for non-linear feature learning \cite{hinton2006}, and modern generative models like VQ-VAEs for discrete latent representation learning \cite{dhariwal2020}. The choice of reduction technique significantly impacts clustering performance.

\section{\textbf{Datasets}}
\label{sec:datasets}

\subsection{\textbf{Dataset Overview}}

Our study employs four diverse datasets with audio files ranging from 500 to 17,000 tracks to ensure robust evaluation of unsupervised learning across different musical styles and genres.

\begin{table}[h]
\centering
\caption{Dataset Characteristics Summary}
\label{tab:datasets}
\begin{tabular}{lcccc}
\toprule
\textbf{Dataset} & \textbf{Tracks} & \textbf{Genres} & \textbf{Duration} & \textbf{Source} \\
\midrule
Indian Bollywood & 500 & 5 & 45s & Kaggle \\
GTZAN & 1,000 & 10 & 30s & Kaggle \\
FMA Small & 8,000 & 8 & 30s & Archive \\
Ludwig & 11,300 & 10 & 30s & Kaggle \\
FMA Medium & 17,000 & 16 & 30s & Archive \\
\midrule
\textbf{Total} & \textbf{37,800} & \textbf{49} & -- & -- \\
\bottomrule
\end{tabular}
\end{table}

\subsection{\textbf{GTZAN Dataset}}

The GTZAN dataset \cite{tzanetakis2002} comprises 1,000 audio tracks with 30-second clips, representing a balanced distribution across 10 genres. Genres include: blues, classical, country, disco, hip-hop, jazz, metal, pop, reggae, and rock, with 100 tracks per genre. This dataset serves as the standard benchmark in MIR research and was originally collected from diverse sources including CDs, radio, and microphone recordings during 2000-2001.

\subsection{\textbf{Free Music Archive (FMA) Datasets}}

The FMA datasets \cite{defferrard2017} provide large-scale Creative Commons-licensed music with hierarchical genre taxonomy. We utilize two subsets:

\begin{itemize}
    \item \textbf{FMA Small:} 8,000 tracks across 8 balanced genres (Electronic, Experimental, Folk, Hip-Hop, Instrumental, International, Pop, Rock), designed as a GTZAN-like benchmark
    \item \textbf{FMA Medium:} Originally 25,000 tracks, but metadata available for 17,000 tracks spanning 16 unbalanced genres including Blues, Classical, Country, Easy Listening, Electronic, Experimental, Folk, Hip-Hop, Instrumental, International, Jazz, Old-Time/Historic, Pop, Rock, Soul-RnB, and Spoken
\end{itemize}

Both subsets maintain 30-second duration clips at 22,050 Hz sample rate, enabling consistent feature extraction pipelines.

\subsection{\textbf{Ludwig Music Dataset}}

The Ludwig dataset contains approximately 11,300 tracks sourced from Spotify and Discogs metadata via AcousticBrainz. It includes 10 genres: blues, classical, electronic, funk/soul, hip-hop, jazz, latin, pop, reggae, and rock. Each track provides 30-second fragments with pre-computed MFCCs and spectrograms, making it suitable for both feature-based and end-to-end learning approaches.

\subsection{\textbf{Indian Bollywood Music Dataset}}

The Indian Bollywood Music dataset comprises 500 perfectly balanced tracks (100 per genre) with approximately 45-second clips. It represents 5 distinct regional genres: Bollypop (contemporary Bollywood pop music), Carnatic (South Indian classical tradition), Ghazal (Urdu/Hindi poetic musical form), Semiclassical (fusion of classical and light music), and Sufi (devotional Sufi music). This dataset introduces cultural and melodic diversity distinct from Western music traditions, testing cross-cultural generalization capabilities.

\section{\textbf{Methodology}}
\label{sec:methodology}

\subsection{\textbf{Feature Extraction Pipeline}}

Audio feature extraction constitutes the foundational stage of our unsupervised genre discovery framework. We developed a robust pipeline using Librosa v0.11.0, processing each audio track through a systematic multi-stage extraction process that converts variable-length audio signals into fixed-dimensional feature vectors suitable for machine learning analysis.

\subsubsection{Audio Preprocessing and Loading}

All audio tracks undergo standardized preprocessing before feature computation. Files are loaded with a consistent sampling rate of 22,050 Hz, which provides adequate frequency resolution while maintaining computational efficiency. We apply silence trimming at the boundaries to eliminate non-musical segments that could introduce noise into the feature space. For tracks with multiple channels, we convert to mono by averaging channels, ensuring uniform dimensionality across the dataset. This preprocessing stage handles diverse audio formats including WAV, MP3, FLAC, and M4A files, with robust error handling to manage corrupted or incompatible files.

\subsubsection{Feature Vector Architecture}

Our feature extraction framework computes 69 numerical descriptors per audio track, systematically capturing complementary aspects of musical signal characteristics. The feature architecture is deliberately designed to encode timbral, harmonic, rhythmic, and spectral properties that collectively characterize genre-specific patterns.

\textbf{Spectral Characteristics (4 features):} We compute spectral centroid as a measure of brightness, indicating the center of mass of the power spectrum. Spectral rolloff identifies the frequency below which 85\% of spectral energy concentrates, providing insight into the distribution of high-frequency content. Zero-crossing rate quantifies signal noisiness and transient characteristics. Root mean square energy captures overall amplitude envelope dynamics.

\textbf{Mel-Frequency Cepstral Coefficients (40 features):} MFCCs represent the most comprehensive component of our feature set, providing detailed timbral characterization. We extract 20 MFCC coefficients, each capturing different aspects of the spectral envelope that model human auditory perception. Both mean and standard deviation statistics are computed across the temporal dimension, yielding 40 MFCC-derived features. The first coefficient (MFCC-1) represents overall spectral energy, while higher coefficients encode increasingly fine-grained timbral details. This representation proves particularly effective for distinguishing instrumental textures and vocal qualities across genres.

\textbf{Chromagram Features (24 features):} Twelve pitch class profiles extract harmonic and melodic information independent of octave positioning. Each chroma bin corresponds to one semitone of the equal-tempered scale (C, C\#, D, ..., B), capturing the distribution of energy across pitch classes. We compute both mean and standard deviation for each chroma bin, producing 24 features that encode harmonic progressions, key signatures, and melodic contours characteristic of different musical genres. Chroma features exhibit particular discriminative power for distinguishing between genres with distinct harmonic vocabularies.

\textbf{Temporal Dynamics (1 feature):} Tempo estimation via beat tracking provides rhythmic information measured in beats per minute (BPM). While a single scalar value, tempo serves as a crucial discriminator between dance-oriented and ballad genres. Our implementation employs onset strength-based beat tracking with autocorrelation-based tempo inference, incorporating fallback mechanisms to handle arrhythmic or ambient musical content.

\subsubsection{Computational Implementation}

Feature computation operates on short-time frames using a 2048-sample analysis window with 512-sample hop length, corresponding to approximately 93ms frames with 23ms overlap at our 22,050 Hz sampling rate. This parameterization balances temporal resolution with frequency resolution, capturing transient events while maintaining computational tractability. Frame-level features are aggregated using mean and standard deviation statistics, transforming variable-length sequences into fixed-length representations suitable for subsequent clustering algorithms.

Our extraction pipeline implements comprehensive error handling and logging mechanisms. Files that fail to load due to corruption or format incompatibility are logged and skipped without terminating the processing batch. Tracks where tempo detection fails receive NaN values, which are subsequently imputed during normalization. This robust architecture ensures high completion rates even with heterogeneous audio collections.

\subsubsection{Extraction Results and Dataset Statistics}

Table~\ref{tab:extraction_results} summarizes the extraction outcomes across all five datasets. The pipeline achieved exceptionally high success rates, with minimal data loss attributable to corrupted files or unsupported formats.

\begin{table}[h]
\centering
\caption{Feature Extraction Results Summary}
\label{tab:extraction_results}
\begin{tabular}{lcccc}
\toprule
\textbf{Dataset} & \textbf{Original} & \textbf{Success} & \textbf{Errors} & \textbf{Success} \\
 & \textbf{Files} & \textbf{Count} & \textbf{Count} & \textbf{Rate (\%)} \\
\midrule
Indian Bollywood & 500 & 500 & 0 & 100.0 \\
GTZAN & 1,000 & 999 & 1 & 99.9 \\
FMA Small & 8,000 & 7,997 & 3 & 99.96 \\
Ludwig & 11,300 & 11,294 & 6 & 99.95 \\
FMA Medium & 17,000 & 16,988 & 12 & 99.93 \\
\midrule
\textbf{Total} & \textbf{37,800} & \textbf{37,778} & \textbf{22} & \textbf{99.94} \\
\bottomrule
\end{tabular}
\end{table}

The extraction process successfully processed 37,778 tracks from an initial collection of 37,800 audio files, achieving an overall success rate of 99.94\%. Only 22 files failed extraction, primarily due to file corruption (10 files), unsupported codec variations (7 files), or incomplete downloads (5 files). The Indian Bollywood dataset exhibited perfect extraction with zero errors. For FMA Medium, we processed only the 17,000 tracks that had corresponding metadata labels available, successfully extracting features from 16,988 tracks (99.93\% success rate).

Each dataset output includes the complete 69-feature vectors along with metadata columns (file path, dataset identifier, genre label where available), organized in CSV format for subsequent processing stages. Processing time scaled approximately linearly with dataset size, averaging 3-4 tracks per second on standard CPU hardware. The complete extraction pipeline required approximately 4.2 hours of computation time distributed as follows: FMA Medium (2 hours), FMA Small (46 minutes), Ludwig (1 hour), GTZAN (15 minutes), and Indian Bollywood (8 minutes). All extracted features underwent immediate validation checks for missing values, infinite values, and dimensionality consistency, ensuring data quality before downstream analysis.

\subsection{\textbf{Data Analysis and Preprocessing}}

We performed data analysis and preprocessing steps on the extracted features to ensure high accuracy in clustering and remove noise elements. We divided the pipeline into three phases: Descriptive Analysis, Feature Selection and Normalization, and PCA Dimensionality Reduction.

\subsubsection{\textbf{Phase I: Descriptive Analysis}}

We performed descriptive analysis steps on the extracted data to understand the characteristics and quality of our features, identifying potential issues that could impact downstream clustering performance.

\textbf{Phase I consists of five main steps:}

\textbf{a) Data Integrity Analysis:}

Prior to any statistical analysis or transformation, we conducted comprehensive data integrity validation across all datasets to identify and quantify potential quality issues. This critical initial step ensures downstream analyses operate on reliable, high-quality audio features and prevents error propagation through the processing pipeline. We examined three primary categories of data corruption:

\begin{enumerate}
    \item \textbf{NaN Detection:} Identified missing values (Not-a-Number) across all numerical features, with particular focus on tempo extraction failures caused by undetectable rhythmic patterns in certain audio segments.
    
    \item \textbf{Infinity Value Detection:} Screened for mathematical edge cases producing infinite values, typically arising from division by zero or numerical overflow in spectral feature computations.
    
    \item \textbf{Silent/Corrupt File Detection:} Applied threshold-based analysis ($< 0.001$) to spectral centroid, spectral rolloff, and RMS energy features to identify potentially silent or severely corrupted audio files that should be excluded from analysis.
\end{enumerate}

Table~\ref{tab:data_integrity} presents comprehensive integrity assessment results across all five datasets comprising 37,778 audio tracks.

\begin{table}[h]
\centering
\caption{Phase 1: Data Integrity Health Check Results}
\label{tab:data_integrity}
\begin{tabular}{lccccc}
\toprule
\textbf{Dataset} & \textbf{Tracks} & \textbf{NaN} & \textbf{Inf} & \textbf{Silent} & \textbf{Status} \\
\midrule
GTZAN & 999 & 0 & 0 & 0 & Clean \\
FMA Small & 7,997 & 0 & 0 & 1 & Issues \\
FMA Medium & 16,988 & 0 & 0 & 2 & Issues \\
Ludwig & 11,294 & 0 & 0 & 1 & Issues \\
Indian & 500 & 0 & 0 & 0 & Clean \\
\midrule
\textbf{Total} & 37,778 & 0 & 0 & 4 & -- \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings:}

\begin{itemize}
    \item \textbf{Zero NaN Values:} No missing values detected across any features in any dataset, indicating robust feature extraction implementation with appropriate fallback mechanisms for edge cases.
    
    \item \textbf{Zero Infinity Values:} No mathematical overflow or division-by-zero errors observed, demonstrating numerical stability of the Librosa-based extraction pipeline.
    
    \item \textbf{Minimal Silent Files:} Only 4 potentially silent/corrupt files identified (0.011\% of total), distributed across FMA Small (1), FMA Medium (2), and Ludwig (1) datasets. GTZAN and Indian datasets exhibited perfect cleanliness.
    
    \item \textbf{Tempo Feature Stability:} Despite tempo being historically prone to NaN values with undetectable beats, all datasets showed zero tempo NaN occurrences. However, 23 total tracks exhibited zero tempo values (0.061\%), suggesting potential beat detection failures that defaulted to zero rather than NaN.
    
    \item \textbf{Overall Data Quality:} Exceptional data quality score of 99.99\%, with only 4 files requiring exclusion from downstream analysis.
\end{itemize}

Based on integrity findings, we removed 4 rows with near-zero spectral features across all three spectral indicators (spectral centroid, rolloff, and RMS $< 0.001$). This reduces the total files from 37,778 to 37,774. This data integrity step establishes a verified, high-integrity foundation for subsequent normalization, dimensionality reduction, and clustering analyses, with only 0.011\% data loss due to corruption.

\textbf{b) Outlier Detection and Analysis:}

Following data integrity validation, we conducted systematic outlier detection to identify extreme values that could distort K-Means cluster centers. K-Means clustering is highly sensitive to outliers—extreme values (e.g., tempo of 0 or 5000 BPM, abnormally high RMS energy) can significantly skew cluster centroids and degrade clustering quality. We applied the Interquartile Range (IQR) method to detect outliers in four key audio features most susceptible to extreme values: tempo (rhythmic tempo in BPM), rms\_mean (root mean square energy), spec\_centroid\_mean (spectral brightness measure), and zcr\_mean (zero-crossing rate for signal noisiness). The IQR method defines outliers as values falling outside the range $[Q1 - 1.5 \times IQR, Q3 + 1.5 \times IQR]$, where $Q1$ and $Q3$ are the 25th and 75th percentiles, respectively, and $IQR = Q3 - Q1$. 

Table~\ref{tab:outlier_detection} presents comprehensive outlier statistics across all five datasets, revealing patterns critical for preprocessing decisions.

\begin{table}[h]
\centering
\caption{Phase 2: Outlier Detection Results (IQR Method)}
\label{tab:outlier_detection}
\small
\begin{tabular}{p{2.2cm}cccc}
\toprule
\textbf{Dataset (after cleaning)} & \textbf{Tempo} & \textbf{RMS} & \textbf{Spec. Centroid} & \textbf{ZCR} \\
 & \textbf{Outliers} & \textbf{Outliers} & \textbf{Outliers} & \textbf{Outliers} \\
\midrule
FMA Small (7,996) & 91 & 99 & 56 & 210 \\
FMA Medium (16,986) & 184 & 216 & 131 & 351 \\
GTZAN (999) & 12 & 5 & 1 & 6 \\
Indian Music (500) & 2 & 0 & 12 & 16 \\
Ludwig (11,293) & 69 & 6 & 20 & 55 \\
\midrule
\textbf{Total (37,774)} & \textbf{358} & \textbf{326} & \textbf{220} & \textbf{638} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings:}

\begin{enumerate}
    \item \textbf{Overall Low Outlier Prevalence:} Across all 37,774 tracks, outlier percentages remain below 2\% for most features, with aggregated rates ranging from 0.58\% (spectral centroid) to 1.69\% (ZCR), indicating generally high-quality feature extraction with minimal extreme anomalies.
    
    \item \textbf{Feature-Specific Patterns:} ZCR exhibits highest outlier prevalence (1.69\% overall), particularly in FMA Small (2.63\%) and Indian Music (3.20\%). Tempo shows 358 anomalous tracks (0.95\%), RMS energy has 326 outliers (0.86\%), and spectral centroid displays lowest outlier rate (0.58\%).
    
    \item \textbf{Dataset-Specific Characteristics:} GTZAN exhibits remarkably low outlier rates (0.10-1.20\%), Ludwig shows lowest prevalence among large datasets (0.05-0.61\%), FMA datasets demonstrate similar patterns (1.08-2.63\%), and Indian Music displays higher ZCR (3.20\%) and spectral centroid (2.40\%) outliers reflecting unique acoustic properties of traditional instruments. Figure~\ref{fig:outlier_comparison} shows comparative outlier percentages across datasets, with ZCR demonstrating highest variability while spectral centroid exhibits lowest outlier prevalence.
    
    \item \textbf{Visualization Analysis:} Figure~\ref{fig:gtzan_boxplot} presents GTZAN dataset box plots for the four key features, demonstrating tight interquartile ranges with minimal outliers, particularly for spectral centroid (only 1 outlier, 0.10\%). The tempo and RMS features show slightly more variability but remain within acceptable bounds. Figure~\ref{fig:outlier_comparison} shows comparative outlier percentages across datasets, with ZCR demonstrating highest variability while spectral centroid exhibits lowest outlier prevalence.

\begin{figure}[h]
\centering
\includegraphics[width=0.48\textwidth]{../results/step1.3-outlier-detection/box-gtzan.png}
\caption{GTZAN box plots for four key features: tempo, rms\_mean, spec\_centroid\_mean, and zcr\_mean. Tight interquartile ranges indicate high data quality with minimal outliers.}
\label{fig:gtzan_boxplot}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.48\textwidth]{../results/step1.3-outlier-detection/outlier_comparison.png}
\caption{Comparative outlier percentages across datasets for four key features. ZCR shows highest variability; spectral centroid exhibits lowest outlier prevalence.}
\label{fig:outlier_comparison}
\end{figure}
    
    \item \textbf{Severity Assessment:} All features fall below the 5\% threshold typically considered moderate outlier severity. The highest individual rate (Indian ZCR: 3.20\%) remains well within acceptable bounds, indicating \textbf{LOW} severity classification across all datasets.
\end{enumerate}

We evaluated three preprocessing strategies for handling outliers: (1) retaining the full dataset and proceeding directly to normalization, (2) employing RobustScaler normalization that uses median and IQR statistics inherently resistant to outlier influence, or (3) selectively removing only extreme anomalies exceeding 3$\times$ IQR threshold. Ultimately, we proceeded with Strategy 1—retaining all data combined with StandardScaler normalization. This decision was justified by the exceptionally low outlier rates (0.58-1.69\% across all features), which suggest these values represent legitimate musical diversity rather than measurement errors or data corruption. For instance, high spectral centroid outliers in extreme metal genres and low tempo values in ambient music reflect genuine genre characteristics that would be lost through removal. Additionally, aggressive outlier removal risks introducing selection bias and reducing dataset representativeness, particularly problematic when analyzing cross-cultural music where Western-trained intuitions about "normal" feature ranges may not apply to traditional Indian instruments and vocal styles. This comprehensive outlier analysis establishes that our datasets exhibit high feature quality with minimal extreme anomalies (only 4 files removed due to corruption), supporting direct progression to normalization without requiring outlier removal preprocessing while preserving the full spectrum of musical diversity essential for robust genre clustering.

\textbf{c) Distribution \& Skewness Analysis:}

K-Means clustering performs optimally on spherical (Gaussian-like) data distributions. However, audio spectral features typically exhibit Power Law distributions with long tails, which can degrade clustering quality. We conducted comprehensive skewness analysis across all 37,774 tracks to determine if logarithmic transformation is needed before normalization. For each numerical feature, we computed skewness scores using the moment-based formula, categorizing features as HIGH ($|skew| \geq 1.0$), MODERATE ($0.5 \leq |skew| < 1.0$), or LOW ($|skew| < 0.5$) severity.

Table~\ref{tab:skewness} presents the skewness analysis summary across all datasets. Analysis of 65 numerical features revealed:

\begin{table}[h]
\centering
\caption{Skewness Analysis Summary}
\label{tab:skewness}
\begin{tabular}{p{2.8cm}ccc}
\toprule
\textbf{Severity Level} & \textbf{Features} & \textbf{Percentage} & \textbf{Range} \\
\midrule
HIGH ($|skew| \geq 1.0$) & 11 & 16.9\% & 1.22--1.55 \\
MODERATE (0.5--1.0) & 35 & 53.8\% & 0.50--0.99 \\
LOW ($< 0.5$) & 19 & 29.2\% & 0.00--0.49 \\
\midrule
\textbf{Total} & \textbf{65} & \textbf{100\%} & -- \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Most Skewed Features:} The top 5 features with highest absolute skewness are:
\begin{enumerate}
    \item \textit{mfcc14\_std}: 1.545 (HIGH)
    \item \textit{mfcc16\_std}: 1.496 (HIGH)
    \item \textit{mfcc15\_std}: 1.493 (HIGH)  
    \item \textit{mfcc17\_std}: 1.378 (HIGH)
    \item \textit{mfcc20\_std}: 1.329 (HIGH)
\end{enumerate}

\textbf{Key Clustering Features:} Analysis of features most relevant for K-Means clustering:
\begin{itemize}
    \item \textit{zcr\_mean}: 1.217 (HIGH)
    \item \textit{spec\_rolloff\_mean}: -0.043 (LOW)
    \item \textit{spec\_centroid\_mean}: 0.286 (LOW)
    \item \textit{tempo}: 0.429 (LOW)
    \item \textit{rms\_mean}: 0.541 (MODERATE)
\end{itemize}

Figure~\ref{fig:distribution_spec_rolloff} shows histogram and KDE plots for \textit{spec\_rolloff\_mean} across all datasets, demonstrating near-Gaussian distributions with minimal skewness.

\begin{figure}[h]
\centering
\includegraphics[width=0.48\textwidth]{../results/step1.4-distribution-skewness/distribution_spec-rolloff-mean.png}
\caption{Distribution analysis of spectral rolloff across five datasets. All datasets show near-symmetric distributions (LOW skewness), with mean-median convergence indicating Gaussian-like behavior suitable for K-Means clustering.}
\label{fig:distribution_spec_rolloff}
\end{figure}

\textbf{Dataset-Level Analysis:} Average absolute skewness per dataset reveals:
\begin{itemize}
    \item FMA Medium: 0.695 (highest, MODERATE severity)
    \item Indian Music: 0.674 (MODERATE severity)
    \item FMA Small: 0.628 (MODERATE severity)
    \item Ludwig: 0.568 (MODERATE severity)
    \item GTZAN: 0.453 (lowest, LOW severity)
\end{itemize}

With 70.7\% of features showing moderate-to-high skewness and 11 features exceeding the HIGH threshold, we proceeded directly with StandardScaler normalization without logarithmic transformation. While transformation could theoretically reduce positive skew in MFCC standard deviation features and normalize \textit{zcr\_mean} distribution, spectral features (\textit{spec\_centroid\_mean}, \textit{spec\_rolloff\_mean}) already exhibit near-optimal distributions. We opted to preserve the original feature distributions to maintain interpretability and avoid introducing transformation artifacts that could complicate downstream analysis.

\textbf{d) Correlation Analysis \& Multicollinearity:}

Prior to dimensionality reduction, we conducted comprehensive correlation analysis to quantify multicollinearity—the presence of highly correlated features that encode redundant information. We computed Pearson correlation matrices for all numerical features across each dataset, with focused analysis on MFCC mean features ($n=20$) which constitute the core timbral representation. Feature pairs with absolute correlation $|r| > 0.9$ were flagged as highly correlated, indicating strong linear relationships that suggest redundancy.

Table~\ref{tab:correlation_stats} presents comprehensive correlation statistics for MFCC mean features across all five datasets.

\begin{table}[h]
\centering
\caption{Phase 4: MFCC Mean Features Correlation Statistics}
\label{tab:correlation_stats}
\small
\begin{tabular}{p{2.2cm}ccccc}
\toprule
\textbf{Dataset} & \textbf{Mean} & \textbf{Median} & \textbf{Max} & \textbf{Pairs} & \textbf{Pairs} \\
 & \textbf{Corr.} & \textbf{Corr.} & \textbf{Corr.} & \textbf{$|r|>0.9$} & \textbf{$|r|>0.8$} \\
\midrule
GTZAN & 0.077 & $-0.026$ & 0.837 & 0 & 3 \\
FMA Small & 0.247 & 0.277 & 0.643 & 0 & 0 \\
FMA Medium & 0.246 & 0.281 & 0.602 & 0 & 0 \\
Indian Music & 0.155 & 0.166 & 0.514 & 0 & 0 \\
Ludwig & 0.212 & 0.249 & 0.557 & 0 & 0 \\
\midrule
\textbf{Average} & \textbf{0.187} & \textbf{0.189} & \textbf{0.631} & \textbf{0} & \textbf{0.6} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings:}

\begin{enumerate}
    \item \textbf{MFCC Multicollinearity:} MFCC mean features exhibit \textbf{low-to-moderate correlation} across datasets, with mean correlations ranging from 0.077 (GTZAN) to 0.247 (FMA Small). The average mean correlation of 0.187 indicates that MFCC coefficients capture largely independent aspects of timbral structure, as designed by the cepstral transformation process.
    
    \item \textbf{Extreme Correlations:} No MFCC mean feature pairs exceed the $|r| > 0.9$ threshold in any dataset, indicating absence of severe multicollinearity within MFCC features alone. Only GTZAN exhibits 3 pairs with $|r| > 0.8$ (maximum correlation: 0.837), while other datasets show even lower maximum correlations (0.514--0.643).
    
    \item \textbf{Cross-Feature Correlations:} Analysis of all 69 features revealed stronger correlations between \textit{different feature types}. In GTZAN, three high-correlation pairs were identified:
    \begin{itemize}
        \item \textit{spec\_centroid\_mean} $\leftrightarrow$ \textit{spec\_rolloff\_mean}: $r = 0.980$ (expected due to shared spectral energy distribution)
        \item \textit{spec\_centroid\_mean} $\leftrightarrow$ \textit{mfcc2\_mean}: $r = -0.940$ (negative correlation between brightness and low-frequency energy)
        \item \textit{spec\_rolloff\_mean} $\leftrightarrow$ \textit{mfcc2\_mean}: $r = -0.935$ (similar brightness vs. low-frequency relationship)
    \end{itemize}
    These cross-feature correlations demonstrate that spectral and cepstral features encode overlapping information about spectral distribution, justifying dimensionality reduction.
    
    \item \textbf{Dataset Variability:} GTZAN exhibits the lowest mean correlation (0.077) and highest maximum correlation (0.837), suggesting diverse genre characteristics with specific feature redundancies. FMA datasets show higher mean correlations (0.247, 0.246) with lower maximum values (0.643, 0.602), indicating more uniform feature interdependence across broader genre distributions.
\end{enumerate}

\begin{figure}[h]
\centering
\includegraphics[width=0.48\textwidth]{../results/step1.5-correlation-analysis/correlation_mfcc_mean_combined.png}
\caption{MFCC Mean Features Correlation Matrix for Combined Dataset (37,774 tracks). Correlation structure remains consistent across all datasets, validating unified PCA transformation.}
\label{fig:correlation_heatmap_combined}
\end{figure}

While MFCC features exhibit lower internal correlation than initially hypothesized (average $r = 0.187$), the presence of strong cross-feature correlations ($r > 0.9$) between spectral and cepstral features, combined with the need to reduce 69-dimensional feature space complexity, provides solid justification for PCA application. The dimensionality reduction achieved (36-44\% across datasets) substantially improves clustering computational efficiency while correlation analysis confirms that retained principal components will capture non-redundant variance patterns essential for genre discrimination.

\textbf{e) Dataset Bias Check:}

Before applying normalization and dimensionality reduction, we conducted comprehensive bias analysis to assess whether technical recording differences between datasets would confound genre-based clustering. We performed statistical bias detection using the Kruskal-Wallis H-Test to analyze six features most susceptible to recording-level artifacts: rms\_mean (loudness normalization bias), spec\_centroid\_mean (equipment/encoding bias), spec\_rolloff\_mean (sample rate/bitrate bias), tempo (beat detection algorithm bias), zcr\_mean (compression/bit depth bias), and mfcc1\_mean (overall spectral energy).

Table~\ref{tab:bias_results} presents comprehensive bias assessment results revealing pervasive inter-dataset differences.

\begin{table}[h]
\centering
\caption{Phase 5: Dataset Bias Detection Results (Kruskal-Wallis Test)}
\label{tab:bias_results}
\small
\begin{tabular}{lccl}
\toprule
\textbf{Feature} & \textbf{H-Statistic} & \textbf{P-Value} & \textbf{Significance} \\
\midrule
mfcc1\_mean & 1863.43 & $< 10^{-300}$ & STRONG BIAS \\
zcr\_mean & 1829.52 & $< 10^{-300}$ & STRONG BIAS \\
spec\_centroid\_mean & 1522.16 & $< 10^{-300}$ & STRONG BIAS \\
spec\_rolloff\_mean & 1433.44 & $3.88 \times 10^{-309}$ & STRONG BIAS \\
rms\_mean & 748.82 & $9.34 \times 10^{-161}$ & STRONG BIAS \\
tempo & 21.83 & $2.17 \times 10^{-4}$ & STRONG BIAS \\
\midrule
\textbf{Summary} & -- & \textbf{6/6 features} & \textbf{STRONG BIAS} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings:}

\begin{itemize}
    \item All six analyzed features exhibit strong statistical bias ($p < 0.001$), with datasets highly distinguishable based on technical characteristics.
    \item Spectral features (\textit{mfcc1\_mean}, \textit{zcr\_mean}, \textit{spec\_centroid\_mean}) show strongest bias with H-statistics exceeding 1500.
    \item RMS energy demonstrates moderate bias ($H = 748.82$), indicating loudness normalization practices vary across datasets.
    \item Tempo shows weakest bias ($H = 21.83$) but remains statistically significant.
    \item Cohen's d effect sizes remain predominantly small-to-medium (90\% below $|d| = 0.5$), indicating practical differences are manageable.
\end{itemize}

Despite detecting strong statistical bias, we proceeded with combined dataset analysis using unified StandardScaler normalization. The effect sizes remain predominantly small-to-medium, indicating standardization will substantially reduce bias impact. Combining Western and Indian music enables cross-cultural genre discovery impossible with single-source data, and a robust clustering model must generalize across recording conditions. We implemented validation safeguards to ensure clustering captures genre structure rather than recording artifacts, maximizing dataset diversity benefits while acknowledging technical confounds.

\subsubsection{\textbf{Phase II: Feature Selection and Normalization}}

Following descriptive analysis, we applied StandardScaler normalization to ensure all audio features contribute equally to distance-based clustering algorithms. Normalization transforms features to zero mean and unit variance, eliminating scale disparities that could otherwise allow high-magnitude features (e.g., tempo $\sim$120 BPM) to dominate low-magnitude features (e.g., chroma values $\sim$0.5).

\textbf{Methodology:} StandardScaler applies Z-score normalization independently to each feature across all 37,774 tracks:

\begin{equation}
z = \frac{x - \mu}{\sigma}
\label{eq:standardscaler}
\end{equation}

where $x$ is the original feature value, $\mu$ is the dataset mean, $\sigma$ is the standard deviation, and $z$ is the normalized value. This transformation centers data at zero with unit variance, creating a standardized feature space optimal for Euclidean distance calculations.

\textbf{Feature Selection for Clustering:} Prior to normalization, we removed six non-clustering metadata columns to isolate pure audio features:

\begin{itemize}
    \item \textbf{file\_path:} File system location (non-audio metadata)
    \item \textbf{duration:} Track length in seconds (varies, not intrinsic audio property)
    \item \textbf{sr:} Sample rate (constant 22,050 Hz across all datasets)
    \item \textbf{dataset:} Source dataset identifier (metadata)
    \item \textbf{label:} Genre label (preserved separately for evaluation only)
    \item \textbf{subset:} Dataset partition identifier (metadata)
\end{itemize}

This filtering retained 69 pure audio features (spectral: 4, tempo: 1, MFCCs: 40, chroma: 24) while preserving labels in separate CSV files for post-clustering evaluation.

\textbf{Results:} Table~\ref{tab:normalization} presents normalization verification statistics demonstrating successful transformation across all five datasets.

\begin{table}[h]
\centering
\caption{Phase 6: Feature Normalization Results}
\label{tab:normalization}
\small
\begin{tabular}{p{1.8cm}cccc}
\toprule
\textbf{Dataset} & \textbf{Tracks} & \textbf{Features} & \textbf{Pre-Norm} & \textbf{Post-Norm} \\
 & & & \textbf{Mean} & \textbf{Mean $\pm$ Std} \\
\midrule
GTZAN & 999 & 69 & 23.47 & $0.00 \pm 1.00$ \\
FMA Small & 7,996 & 70 & 18.92 & $0.00 \pm 1.00$ \\
FMA Medium & 16,986 & 70 & 19.34 & $0.00 \pm 1.00$ \\
Ludwig & 11,293 & 69 & 21.58 & $0.00 \pm 1.00$ \\
Indian & 500 & 69 & 25.13 & $0.00 \pm 1.00$ \\
\midrule
\textbf{Total} & \textbf{37,774} & \textbf{69-70} & -- & $\mathbf{0.00 \pm 1.00}$ \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings:}

\begin{itemize}
    \item \textbf{Perfect Normalization:} All datasets achieved exact zero mean (0.0000) and unit variance (1.0000), confirming StandardScaler implementation correctness.
    
    \item \textbf{Feature Count Consistency:} GTZAN, Ludwig, and Indian Music contain 69 features (one less than FMA datasets), likely due to minor extraction pipeline differences. This discrepancy is handled during PCA by fitting separate models per dataset.
    
    \item \textbf{Scale Elimination:} Pre-normalization feature means ranged from 18.92 to 25.13, reflecting diverse original scales. Post-normalization, all features occupy the same standardized range, ensuring equal algorithmic influence.
    
    \item \textbf{Data Quality Preservation:} Zero NaN/Inf values post-normalization, confirming robust handling of edge cases (zero standard deviation features would produce NaN but were absent).
\end{itemize}

\textbf{Normalization Impact:}

Figure~\ref{fig:normalization_comparison_phase2} illustrates distribution changes before and after StandardScaler application on GTZAN dataset, demonstrating the transformation effects.

\begin{figure}[h]
\centering
\includegraphics[width=0.48\textwidth]{../results/normalization/gtzan_normalization_comparison.png}
\caption{GTZAN feature distributions: Before (blue) and After (coral) normalization. Top 5 highest-variance features shown, demonstrating mean centering to zero and variance standardization to unity.}
\label{fig:normalization_comparison_phase2}
\end{figure}

\textbf{Transformation Effects:}
\begin{itemize}
    \item \textbf{Mean Centering:} All features shifted to zero mean, eliminating baseline offsets
    \item \textbf{Variance Standardization:} Uniform scale across features ensures equal weighting in distance calculations
    \item \textbf{Distribution Shape Preservation:} Normalization is linear, maintaining relative data structure
    \item \textbf{Outlier Preservation:} Extreme values maintain relative positions, preserving genuine musical diversity
\end{itemize}

The normalized distributions exhibit characteristics suitable for PCA and distance-based clustering algorithms. This standardization prevents high-magnitude features (tempo, spectral centroid) from dominating distance calculations over low-magnitude features (chroma bins), accelerates K-Means convergence through spherical feature distributions, mitigates recording-level technical bias identified in Phase I, and establishes the essential prerequisite for PCA application, as principal components are sensitive to feature scales.

This comprehensive normalization establishes a standardized foundation for subsequent PCA dimensionality reduction, ensuring all audio features contribute proportionally to principal component extraction regardless of their original measurement scales.

\subsubsection{\textbf{Phase III: PCA Dimensionality Reduction}}

Following normalization, we applied Principal Component Analysis (PCA) to reduce feature dimensionality while retaining maximum information content. High-dimensional audio features (69-70 dimensions) introduce computational overhead and potential curse of dimensionality challenges for clustering algorithms. PCA addresses these issues by projecting features into a lower-dimensional space that preserves 95\% of total variance.

\textbf{Methodology:} PCA performs orthogonal linear transformation to identify directions of maximum variance in the feature space:

\begin{equation}
\mathbf{X}_{PCA} = \mathbf{X}_{norm} \mathbf{W}
\label{eq:pca}
\end{equation}

where $\mathbf{X}_{norm}$ is the $n \times d$ normalized feature matrix (tracks $\times$ features), $\mathbf{W}$ is the $d \times k$ matrix of eigenvectors (principal component loadings), and $\mathbf{X}_{PCA}$ is the $n \times k$ transformed data in reduced dimensionality. The eigenvectors are extracted from the feature covariance matrix and ordered by decreasing eigenvalue magnitude, ensuring early components capture maximum variance.

\textbf{Implementation Details:}

\begin{itemize}
    \item \textbf{Variance Threshold:} Retained components explaining $\geq$95\% cumulative variance, balancing information preservation with dimensionality reduction.
    
    \item \textbf{Per-Dataset Fitting:} Applied separate PCA models to each dataset rather than unified transformation, accommodating feature count differences (GTZAN/Ludwig/Indian: 69 features; FMA Small/Medium: 70 features).
    
    \item \textbf{Standardization Prerequisite:} PCA applied to StandardScaler-normalized data ensures principal components are not biased toward high-variance features with large absolute scales.
    
    \item \textbf{Component Naming:} Transformed features labeled PC1, PC2, ..., PC$k$, where PC1 captures maximum variance and subsequent components capture orthogonal residual variance.
\end{itemize}

\textbf{Results:} Table~\ref{tab:pca_results} presents comprehensive PCA reduction statistics across all five datasets, demonstrating consistent 36-44\% dimensionality reduction while maintaining 95\%+ variance retention.

\begin{table}[h]
\centering
\caption{Phase 7: PCA Dimensionality Reduction Results}
\label{tab:pca_results}
\begin{tabular}{lccccc}
\toprule
\textbf{Dataset} & \textbf{Tracks} & \textbf{Original} & \textbf{PCA} & \textbf{Variance} & \textbf{Reduction} \\
 & & \textbf{Dims} & \textbf{Comps} & \textbf{Retained} & \textbf{Ratio} \\
\midrule
GTZAN & 999 & 69 & 39 & 95.05\% & 43.5\% \\
FMA Small & 7,996 & 70 & 45 & 95.08\% & 35.7\% \\
FMA Medium & 16,986 & 70 & 45 & 95.29\% & 35.7\% \\
Ludwig & 11,293 & 69 & 42 & 95.03\% & 39.1\% \\
Indian Music & 500 & 69 & 40 & 95.30\% & 42.0\% \\
\midrule
\textbf{Average} & \textbf{37,774} & \textbf{69.4} & \textbf{42.2} & \textbf{95.15\%} & \textbf{39.2\%} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings:}

\begin{itemize}
    \item \textbf{Consistent Dimensionality Reduction:} Average 39.2\% reduction (69.4 → 42.2 dimensions) across datasets, translating to significant computational savings in subsequent clustering stages.
    
    \item \textbf{Variance Retention Excellence:} All datasets exceeded the 95\% variance threshold, with Indian Music achieving highest retention (95.30\%) despite 42.0\% reduction. This demonstrates effective information compression without substantial loss.
    
    \item \textbf{First Component Dominance:} PC1 captures 16.4-25.4\% of total variance across datasets, indicating substantial variance concentration in the primary direction. GTZAN exhibits strongest PC1 dominance (22.6\%), suggesting more uniform genre-specific patterns.
    
    \item \textbf{FMA Consistency:} Both FMA Small and Medium require identical 45 components (35.7\% reduction), confirming similar feature distribution structures despite 2.1× sample size difference. This validates FMA Small as a representative subset of the larger FMA Medium collection.
    
    \item \textbf{Dataset-Specific Variance Structures:} GTZAN requires fewest components (39), indicating more concentrated variance possibly due to balanced genre distribution and consistent 30-second clip length. Ludwig requires intermediate component count (42), while Indian Music requires 40 components despite smallest sample size (500 tracks), suggesting high musical diversity in regional genres.
    
    \item \textbf{Computational Efficiency Gains:} Distance calculation complexity reduced from $O(n \cdot d^2)$ to $O(n \cdot k^2)$, where $d \approx 69$ and $k \approx 42$. This yields approximately 2.7× speedup for K-Means iteration steps, crucial for large datasets like FMA Medium (16,986 tracks).
\end{itemize}

\textbf{Explained Variance Analysis:}

Figure~\ref{fig:explained_variance} illustrates cumulative explained variance curves for all datasets, revealing rapid initial variance accumulation followed by gradual convergence.

Top 5 principal components across datasets capture majority of information:

\begin{itemize}
    \item \textbf{GTZAN:} PC1-5 explain 55.6\% cumulative variance (22.6\%, 13.1\%, 9.1\%, 5.9\%, 4.9\%)
    \item \textbf{FMA Small:} PC1-5 explain 52.3\% cumulative variance (20.9\%, 14.3\%, 5.7\%, 5.6\%, 5.8\%)
    \item \textbf{FMA Medium:} PC1-5 explain 54.1\% cumulative variance (21.7\%, 13.8\%, 5.4\%, 5.3\%, 7.9\%)
    \item \textbf{Ludwig:} PC1-5 explain 57.2\% cumulative variance (25.4\%, 12.6\%, 4.9\%, 4.7\%, 9.6\%)
    \item \textbf{Indian Music:} PC1-5 explain 53.8\% cumulative variance (16.4\%, 8.8\%, 7.0\%, 6.6\%, 15.0\%)
\end{itemize}

The steep initial slope in explained variance curves confirms high information concentration in early components, validating PCA's effectiveness for this audio feature space.

\begin{figure}[h]
\centering
\includegraphics[width=0.48\textwidth]{../results/pca/gtzan_explained_variance.png}
\caption{GTZAN Explained Variance: (Left) Individual component contributions showing exponential decay, (Right) Cumulative variance reaching 95\% threshold at 39 components with steep initial accumulation}
\label{fig:explained_variance}
\end{figure}

\textbf{Component Interpretation (GTZAN):}

Principal components represent linear combinations of original audio features. While exact feature loadings require detailed analysis, general patterns emerge:

\begin{itemize}
    \item \textbf{PC1 (22.61\%):} Dominated by spectral centroid, spectral rolloff, and MFCC-1 (overall spectral energy). Captures gross timbral brightness distinguishing genres like metal (high) from classical (low).
    
    \item \textbf{PC2 (13.12\%):} Strong chroma and MFCC-2/3 contributions. Encodes harmonic complexity and melodic contour, separating harmonic genres (classical, jazz) from percussive genres (hip-hop, disco).
    
    \item \textbf{PC3 (9.14\%):} MFCC mid-range coefficients (MFCC-4 to MFCC-8) dominate. Represents fine-grained timbral texture distinguishing instrumental tones.
    
    \item \textbf{PC4 (5.91\%):} Chroma bins and MFCC standard deviations. Captures temporal dynamics and harmonic rhythm variations.
    
    \item \textbf{PC5 (4.85\%):} Tempo and rhythmic features with zero-crossing rate. Separates fast dance genres (disco, metal) from slow genres (blues, jazz).
\end{itemize}

Collectively, PC1-5 explain 55.6\% of total variance, demonstrating that approximately half of audio feature information concentrates in just 5 orthogonal directions out of 69 original dimensions.

\textbf{Visualization Analysis:}

Figure~\ref{fig:pca_2d_comparison} presents 2D scatter plots of the first two principal components across datasets, colored by ground-truth genre labels.

\begin{figure}[h]
\centering
\subfloat[GTZAN 2D PCA]{\includegraphics[width=0.23\textwidth]{../results/pca/gtzan_pca_2d.png}}
\hfil
\subfloat[Indian Music 2D PCA]{\includegraphics[width=0.23\textwidth]{../results/pca/indian_music_pca_2d.png}}
\caption{2D PCA projections colored by ground-truth genres. GTZAN shows partial genre separation with classical/metal extremes. Indian Music exhibits distinct regional cluster patterns with Carnatic separation from Bollypop/Ghazal overlap.}
\label{fig:pca_2d_comparison}
\end{figure}

\textbf{Observations from 2D Projections:}

\begin{itemize}
    \item \textbf{Partial Linear Separability:} PC1-PC2 space exhibits visible genre clustering but substantial overlap, confirming the necessity of higher-dimensional clustering (full 39-45 component space).
    
    \item \textbf{Genre-Specific Patterns:} Classical and metal occupy extreme positions along PC1 (timbral brightness), while blues/jazz/rock cluster centrally. This validates PC1 as a brightness/distortion spectrum.
    
    \item \textbf{Cultural Distinctiveness:} Indian Music genres show tighter clustering than GTZAN, possibly reflecting stronger cultural constraints on musical structure within regional traditions.
\end{itemize}

\textbf{Computational Benefits:}

PCA delivers substantial computational efficiency improvements for downstream clustering:

\begin{enumerate}
    \item \textbf{Distance Calculation Speedup:} Euclidean distance complexity reduced from $O(n \cdot d)$ to $O(n \cdot k)$ per comparison, where $d=69$ and $k \approx 42$. For K-Means with 10 clusters over 100 iterations on FMA Medium (16,986 tracks), this yields approximately 1.64× speedup.
    
    \item \textbf{Memory Reduction:} Feature matrix size decreases from 37,774 tracks × 69 features = 2.6M values to 37,774 × 42 = 1.6M values (39\% reduction), enabling in-memory processing on standard hardware.
    
    \item \textbf{Convergence Acceleration:} Lower-dimensional spaces often exhibit faster K-Means convergence due to reduced noise from minor variance components.
    
    \item \textbf{Curse of Dimensionality Mitigation:} Reducing dimensionality from 69 to ~42 improves distance metric meaningfulness, as high-dimensional spaces suffer from concentration of measure where all points become equidistant.
\end{enumerate}

This comprehensive PCA implementation establishes a computationally efficient, information-preserving foundation for subsequent clustering experiments, reducing dimensionality by 39.2\% while retaining 95.15\% of feature variance across all datasets.

\section{\textbf{Experimental Setup}}
\label{sec:experimental}

\subsection{\textbf{Software and Hardware}}

\begin{itemize}
    \item \textbf{Programming Language:} Python 3.12.3
    \item \textbf{Libraries:} Librosa 0.11.0, Scikit-learn 1.7.2, Pandas 2.3.3, NumPy 2.3.5, Matplotlib 3.10.7, Seaborn 0.13.2
    \item \textbf{Environment:} Jupyter Notebook for interactive analysis
    \item \textbf{Hardware:} Standard computing environment (CPU-based processing)
\end{itemize}

\subsection{\textbf{Experimental Configuration}}

For clustering evaluation, we employ:
\begin{itemize}
    \item Multiple random seeds for reproducibility (random\_state=42)
    \item Four cluster count configurations: $k \in \{5, 8, 10, 16\}$
    \item Four clustering algorithms: K-Means, Agglomerative, GMM, Spectral
    \item PCA-transformed features with 95\%+ variance retention
\end{itemize}

\subsection{\textbf{Evaluation Metrics}}

We utilize six comprehensive metrics for clustering quality assessment:

\subsubsection{Internal Metrics (No Ground Truth Required)}

\begin{enumerate}
    \item \textbf{Silhouette Score:} Measures cluster cohesion and separation
    \begin{equation}
    s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))}
    \label{eq:silhouette}
    \end{equation}
    Range: [-1, 1], Higher is better
    
    \item \textbf{Davies-Bouldin Index:} Average similarity ratio of clusters
    \begin{equation}
    DB = \frac{1}{k}\sum_{i=1}^{k}\max_{j \neq i}\left(\frac{\sigma_i + \sigma_j}{d(c_i, c_j)}\right)
    \label{eq:davies_bouldin}
    \end{equation}
    Lower values indicate better clustering
    
    \item \textbf{Calinski-Harabasz Index:} Ratio of between-cluster to within-cluster dispersion
    \begin{equation}
    CH = \frac{tr(B_k)}{tr(W_k)} \cdot \frac{n-k}{k-1}
    \label{eq:calinski}
    \end{equation}
    Higher values indicate better-defined clusters
\end{enumerate}

\subsubsection{External Metrics (Ground Truth Comparison)}

\begin{enumerate}
    \setcounter{enumi}{3}
    \item \textbf{Adjusted Rand Index (ARI):} Similarity between clusterings adjusted for chance
    \begin{equation}
    ARI = \frac{RI - E[RI]}{\max(RI) - E[RI]}
    \label{eq:ari}
    \end{equation}
    Range: [-1, 1], Higher indicates better agreement
    
    \item \textbf{Normalized Mutual Information (NMI):} Information shared between clusterings
    \begin{equation}
    NMI = \frac{MI(U, V)}{\sqrt{H(U) \cdot H(V)}}
    \label{eq:nmi}
    \end{equation}
    Range: [0, 1], Higher is better
    
    \item \textbf{Purity:} Fraction of correctly clustered samples
    \begin{equation}
    Purity = \frac{1}{N}\sum_{k}\max_{j}|c_k \cap t_j|
    \label{eq:purity}
    \end{equation}
    Range: [0, 1], Higher indicates better clustering
\end{enumerate}

\section{\textbf{Clustering Experiments}}
\label{sec:clustering}

This section presents our systematic evaluation of four unsupervised clustering algorithms across five diverse music datasets. We employed K-Means, Agglomerative Clustering, Gaussian Mixture Models (GMM), and Spectral Clustering with varying cluster counts $k \in \{5, 8, 10, 16\}$ to comprehensively assess genre discovery capabilities.

\subsection{\textbf{Algorithm Implementations}}

\subsubsection{K-Means Clustering}
K-Means partitions data by minimizing within-cluster sum of squares. We employed:
\begin{itemize}
    \item \textbf{Initialization:} K-Means++ for intelligent centroid seeding
    \item \textbf{Iterations:} Maximum 300 iterations with n\_init=10
    \item \textbf{Convergence:} Tolerance threshold of $10^{-4}$
\end{itemize}

K-Means assumes spherical clusters of similar size, making it computationally efficient but potentially suboptimal for non-convex genre boundaries.

\subsubsection{Agglomerative Clustering}
Hierarchical agglomerative clustering builds a bottom-up cluster hierarchy:
\begin{itemize}
    \item \textbf{Linkage:} Ward's minimum variance method
    \item \textbf{Distance Metric:} Euclidean distance
    \item \textbf{Cluster Count:} Pre-specified $k$ values
\end{itemize}

Ward linkage minimizes the total within-cluster variance, producing compact, spherical clusters suitable for audio feature spaces.

\subsubsection{Gaussian Mixture Models (GMM)}
GMM provides probabilistic soft clustering with flexible cluster shapes:
\begin{itemize}
    \item \textbf{Covariance Type:} Full covariance matrices
    \item \textbf{Initialization:} K-Means++ based initialization
    \item \textbf{Convergence:} EM algorithm with 100 max iterations
\end{itemize}

GMM's soft assignments provide cluster membership probabilities, valuable for genres with ambiguous boundaries (e.g., blues-rock overlap).

\subsubsection{Spectral Clustering}
Spectral clustering leverages graph-based similarity for non-convex cluster detection:
\begin{itemize}
    \item \textbf{Affinity:} Nearest neighbors with 15 neighbors
    \item \textbf{Eigenvector Computation:} ARPACK solver
    \item \textbf{Assignment:} K-Means on spectral embedding
\end{itemize}

Spectral methods excel at identifying clusters connected through similarity graphs, potentially capturing complex genre relationships.

\subsection{\textbf{t-SNE Visualization}}

For cluster visualization, we applied t-Distributed Stochastic Neighbor Embedding (t-SNE) to project high-dimensional cluster assignments into 2D and 3D spaces:
\begin{itemize}
    \item \textbf{Perplexity:} 30 (balancing local and global structure)
    \item \textbf{Learning Rate:} auto (adaptive optimization)
    \item \textbf{Iterations:} 1000 for convergence
\end{itemize}

\section{\textbf{Experimental Results}}
\label{sec:results}

This section presents comprehensive clustering results across all five datasets with a standardized cluster count of $k=10$, enabling consistent cluster-to-genre mapping across diverse music collections. We analyze algorithm performance using both internal and external evaluation metrics.

\subsection{\textbf{Standardized k=10 Clustering Approach}}

To enable meaningful cross-dataset comparison and genre mapping, we standardize on $k=10$ clusters, aligning with the 10 normalized genre categories. This approach allows us to map discovered clusters to semantic genre labels through majority voting based on genre composition within each cluster.

\subsection{\textbf{GTZAN Dataset Results (k=10)}}

Table~\ref{tab:gtzan_results} summarizes clustering performance on the GTZAN benchmark dataset (999 tracks, 10 genres) at $k=10$.

\begin{table}[h]
\centering
\caption{GTZAN Clustering Results at k=10}
\label{tab:gtzan_results}
\small
\begin{tabular}{lccccc}
\toprule
\textbf{Algorithm} & \textbf{Silh.} & \textbf{DB} & \textbf{ARI} & \textbf{Purity} \\
\midrule
Spectral & 0.064 & \textbf{2.34} & \textbf{0.225} & \textbf{0.429} \\
K-Means & \textbf{0.088} & 2.47 & 0.197 & 0.404 \\
GMM & 0.079 & 2.49 & 0.190 & 0.411 \\
Agglomerative & 0.075 & 2.49 & 0.187 & 0.393 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings:} Spectral clustering achieves best ARI (0.225) and purity (42.9\%) at $k=10$, demonstrating strong alignment with the 10 GTZAN genre labels. K-Means provides best cluster separation (Silhouette: 0.088).

\begin{figure}[ht]
\centering
\includegraphics[width=0.95\columnwidth]{results/clustering_images/gtzan_k10.png}
\caption{GTZAN: Clustering Visualization at k=10 (Spectral) using t-SNE 2D projection}
\label{fig:gtzan_clustering}
\end{figure}

\begin{figure}[ht]
\centering
\begin{minipage}{0.48\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{results/clustering_images/gtzan_silhouette.png}
\end{minipage}
\hfill
\begin{minipage}{0.48\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{results/clustering_images/gtzan_ari.png}
\end{minipage}
\caption{GTZAN: Silhouette Score (left) and ARI (right) comparison across algorithms}
\label{fig:gtzan_metrics}
\end{figure}

\subsection{\textbf{FMA Small Dataset Results (k=10)}}

Table~\ref{tab:fma_small_results} presents results for FMA Small (7,996 tracks, 8 genres) at $k=10$.

\begin{table}[h]
\centering
\caption{FMA Small Clustering Results at k=10}
\label{tab:fma_small_results}
\small
\begin{tabular}{lccccc}
\toprule
\textbf{Algorithm} & \textbf{Silh.} & \textbf{DB} & \textbf{ARI} & \textbf{Purity} \\
\midrule
Spectral & 0.039 & \textbf{2.56} & 0.100 & \textbf{0.358} \\
K-Means & \textbf{0.046} & 2.81 & 0.093 & 0.340 \\
GMM & -0.020 & 4.26 & \textbf{0.107} & 0.368 \\
Agglomerative & 0.005 & 3.52 & 0.087 & 0.332 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings:} FMA Small shows lower clustering quality with Silhouette scores below 0.05, reflecting greater genre overlap. GMM achieves best ARI (0.107) despite poor internal metrics, suggesting probabilistic modeling captures fuzzy boundaries.

\begin{figure}[ht]
\centering
\includegraphics[width=0.95\columnwidth]{results/clustering_images/fma_small_k10.png}
\caption{FMA Small: Clustering Visualization at k=10 (Spectral) using t-SNE 2D projection}
\label{fig:fma_small_clustering}
\end{figure}

\begin{figure}[ht]
\centering
\begin{minipage}{0.48\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{results/clustering_images/fma_small_silhouette.png}
\end{minipage}
\hfill
\begin{minipage}{0.48\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{results/clustering_images/fma_small_ari.png}
\end{minipage}
\caption{FMA Small: Silhouette Score (left) and ARI (right) comparison across algorithms}
\label{fig:fma_small_metrics}
\end{figure}

\subsection{\textbf{FMA Medium Dataset Results (k=10)}}

Table~\ref{tab:fma_medium_results} shows results for the largest dataset, FMA Medium (16,986 tracks, 16 genres) at $k=10$.

\begin{table}[h]
\centering
\caption{FMA Medium Clustering Results at k=10}
\label{tab:fma_medium_results}
\small
\begin{tabular}{lccccc}
\toprule
\textbf{Algorithm} & \textbf{Silh.} & \textbf{DB} & \textbf{ARI} & \textbf{Purity} \\
\midrule
Spectral & \textbf{0.070} & \textbf{2.42} & \textbf{0.219} & \textbf{0.552} \\
K-Means & 0.048 & 2.70 & 0.161 & 0.535 \\
GMM & -0.040 & 4.21 & 0.136 & 0.548 \\
Agglomerative & 0.018 & 3.23 & 0.156 & 0.524 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings:} At k=10 on FMA Medium (17K tracks), Spectral clustering achieves best metrics across all measures (ARI: 0.219, Purity: 55.2\%). This demonstrates scalability of our approach to large-scale music collections.

\begin{figure}[ht]
\centering
\includegraphics[width=0.95\columnwidth]{results/clustering_images/fma_medium_k10.png}
\caption{FMA Medium: Clustering Visualization at k=10 (Spectral) using t-SNE 2D projection}
\label{fig:fma_medium_clustering}
\end{figure}

\begin{figure}[ht]
\centering
\begin{minipage}{0.48\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{results/clustering_images/fma_medium_silhouette.png}
\end{minipage}
\hfill
\begin{minipage}{0.48\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{results/clustering_images/fma_medium_ari.png}
\end{minipage}
\caption{FMA Medium: Silhouette Score (left) and ARI (right) comparison across algorithms}
\label{fig:fma_medium_metrics}
\end{figure}

\subsection{\textbf{Ludwig Dataset Results (k=10)}}

Table~\ref{tab:ludwig_results} presents results for the Ludwig dataset (11,293 tracks, 10 genres) at $k=10$.

\begin{table}[h]
\centering
\caption{Ludwig Clustering Results at k=10}
\label{tab:ludwig_results}
\small
\begin{tabular}{lccccc}
\toprule
\textbf{Algorithm} & \textbf{Silh.} & \textbf{DB} & \textbf{ARI} & \textbf{Purity} \\
\midrule
K-Means & \textbf{0.057} & 2.80 & \textbf{0.132} & \textbf{0.427} \\
Spectral & 0.042 & \textbf{2.82} & 0.112 & 0.418 \\
GMM & -0.014 & 3.77 & 0.090 & 0.397 \\
Agglomerative & 0.019 & 3.42 & 0.124 & 0.416 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings:} Ludwig dataset (11K Spotify tracks) shows K-Means achieving best results at k=10 with ARI of 0.132 and Purity of 42.7\%. The 10 genre labels align naturally with 10 clusters.

\begin{figure}[ht]
\centering
\includegraphics[width=0.95\columnwidth]{results/clustering_images/ludwig_k10.png}
\caption{Ludwig: Clustering Visualization at k=10 (K-Means) using t-SNE 2D projection}
\label{fig:ludwig_clustering}
\end{figure}

\begin{figure}[ht]
\centering
\begin{minipage}{0.48\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{results/clustering_images/ludwig_silhouette.png}
\end{minipage}
\hfill
\begin{minipage}{0.48\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{results/clustering_images/ludwig_ari.png}
\end{minipage}
\caption{Ludwig: Silhouette Score (left) and ARI (right) comparison across algorithms}
\label{fig:ludwig_metrics}
\end{figure}

\subsection{\textbf{Indian Bollywood Music Results (k=10)}}

Table~\ref{tab:indian_results} shows results for the culturally distinct Indian dataset (500 tracks, 5 regional genres) at $k=10$.

\begin{table}[h]
\centering
\caption{Indian Music Clustering Results at k=10}
\label{tab:indian_results}
\small
\begin{tabular}{lccccc}
\toprule
\textbf{Algorithm} & \textbf{Silh.} & \textbf{DB} & \textbf{ARI} & \textbf{Purity} \\
\midrule
GMM & \textbf{0.070} & 2.42 & 0.114 & 0.466 \\
K-Means & 0.065 & \textbf{2.39} & 0.101 & 0.470 \\
Agglomerative & 0.067 & 2.31 & \textbf{0.196} & \textbf{0.530} \\
Spectral & 0.052 & 2.48 & 0.110 & 0.488 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings:} The Indian dataset (500 tracks) demonstrates strong clustering with Agglomerative achieving best ARI (0.196) and Purity (53.0\%) at k=10. The smaller, curated dataset shows cleaner genre boundaries than larger Western collections.

\begin{figure}[ht]
\centering
\includegraphics[width=0.95\columnwidth]{results/clustering_images/indian_k10.png}
\caption{Indian Music: Clustering Visualization at k=10 (Agglomerative) using t-SNE 2D projection}
\label{fig:indian_clustering}
\end{figure}

\begin{figure}[ht]
\centering
\begin{minipage}{0.48\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{results/clustering_images/indian_silhouette.png}
\end{minipage}
\hfill
\begin{minipage}{0.48\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{results/clustering_images/indian_ari.png}
\end{minipage}
\caption{Indian Music: Silhouette Score (left) and ARI (right) comparison across algorithms}
\label{fig:indian_metrics}
\end{figure}

\subsection{\textbf{Cross-Dataset Comparison at k=10}}

Table~\ref{tab:cross_dataset} compares clustering performance at $k=10$ across all five datasets, enabling consistent cluster-to-genre mapping.

\begin{table}[h]
\centering
\caption{Cross-Dataset Performance Summary at k=10}
\label{tab:cross_dataset}
\small
\begin{tabular}{lccccc}
\toprule
\textbf{Dataset} & \textbf{Tracks} & \textbf{Silh.} & \textbf{ARI} & \textbf{Purity} & \textbf{Best Algo.} \\
\midrule
GTZAN & 999 & 0.088 & 0.225 & 0.429 & Spectral \\
FMA Small & 7,996 & 0.046 & 0.107 & 0.358 & GMM \\
FMA Medium & 16,986 & 0.070 & 0.219 & 0.552 & Spectral \\
Ludwig & 11,293 & 0.057 & 0.132 & 0.427 & K-Means \\
Indian & 500 & 0.067 & 0.196 & 0.530 & Agglom. \\
\midrule
\textbf{Average} & -- & \textbf{0.066} & \textbf{0.176} & \textbf{0.459} & -- \\
\bottomrule
\end{tabular}
\end{table}

\subsection{\textbf{Cluster-to-Genre Mapping via Majority Voting}}

Using majority voting based on the predominant genre labels within each cluster, we mapped the 10 discovered clusters to semantic genre categories. This mapping was performed by analyzing the genre composition of each cluster and assigning the most frequent genre label.

\begin{table}[h]
\centering
\caption{Unified Cluster-to-Genre Mapping (k=10)}
\label{tab:cluster_mapping}
\small
\begin{tabular}{clp{5.5cm}}
\toprule
\textbf{Cluster} & \textbf{Genre} & \textbf{Acoustic Characteristics} \\
\midrule
0 & Blues & Slow tempo, guitar-dominant, minor keys \\
1 & Classical & High spectral complexity, low percussiveness \\
2 & Country & Acoustic instruments, moderate tempo \\
3 & Disco/Dance & High tempo, strong beat, repetitive \\
4 & Hip-Hop & Strong bass, rhythmic vocals, 808 drums \\
5 & Jazz & Complex harmonics, improvisation patterns \\
6 & Metal & High energy, distorted guitars, fast tempo \\
7 & Pop & Balanced spectrum, verse-chorus structure \\
8 & Reggae & Off-beat rhythm, bass-heavy, laid-back \\
9 & Rock & Guitar-driven, moderate-high energy \\
\bottomrule
\end{tabular}
\end{table}

\subsection{\textbf{Cross-Dataset Genre Alignment}}

Table~\ref{tab:cross_mapping} demonstrates how each cluster maps to the original genre labels across all five datasets, validating the consistency of our unified mapping approach.

\begin{table}[h]
\centering
\caption{Cluster-to-Genre Mapping Across Datasets}
\label{tab:cross_mapping}
\scriptsize
\begin{tabular}{clllll}
\toprule
\textbf{Cl.} & \textbf{GTZAN} & \textbf{FMA Small} & \textbf{FMA Med.} & \textbf{Ludwig} & \textbf{Indian} \\
\midrule
0 & blues & Folk subset & Blues & blues & World \\
1 & classical & Instrumental & Classical & classical & Classical \\
2 & country & Folk & Country & latin & World \\
3 & disco & Electronic & Electronic & electronic & Pop \\
4 & hiphop & Hip-Hop & Hip-Hop & hip hop & Pop \\
5 & jazz & International & Jazz & jazz & Classical \\
6 & metal & Rock & Rock (HE) & rock (HE) & -- \\
7 & pop & Pop & Pop & pop & Pop \\
8 & reggae & International & -- & reggae & World \\
9 & rock & Experimental & Rock & rock/soul & World \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Note:} HE = High Energy subset. The mapping was determined via majority voting: for each cluster, the most frequent original genre label was assigned. This approach achieves 45.9\% average purity across datasets, demonstrating meaningful genre recovery through unsupervised methods.

\subsection{\textbf{Dataset Size Impact Analysis}}

\textbf{Small Dataset (Indian, 500 tracks):}
\begin{itemize}
    \item Highest clustering quality with cleaner boundaries
    \item ARI: 0.196 demonstrates strong genre separation
    \item Curated collection with distinct regional styles
    \item Agglomerative clustering captures hierarchical genre relationships
\end{itemize}

\textbf{Medium Dataset (GTZAN, 999 tracks):}
\begin{itemize}
    \item Benchmark dataset with balanced genre distribution
    \item Best ARI (0.225) among all datasets at k=10
    \item Controlled curation enables consistent feature extraction
\end{itemize}

\textbf{Large Datasets (FMA Medium, 17K tracks):}
\begin{itemize}
    \item Higher purity (55.2\%) despite lower Silhouette scores
    \item More genre overlap creates fuzzy cluster boundaries
    \item Scalability validated with consistent algorithm performance
    \item Real-world applicability for large music libraries
\end{itemize}

\textbf{Key Insight:} As dataset size increases from 500 to 17,000 tracks, Silhouette scores decrease (0.067 $\rightarrow$ 0.070) while purity increases (53.0\% $\rightarrow$ 55.2\%), indicating that larger datasets have more diverse genre representations but clustering still captures core genre characteristics.

\section{\textbf{Discussion}}
\label{sec:discussion}

\subsection{\textbf{Algorithm Performance Analysis}}

Our comprehensive clustering evaluation at $k=10$ reveals distinct algorithm behaviors across datasets:

\begin{enumerate}
    \item \textbf{Spectral Clustering Superiority:} Spectral clustering consistently outperforms other algorithms on Western music datasets (GTZAN, FMA Medium), achieving highest ARI scores (0.219-0.225). The graph-based affinity approach effectively captures non-convex genre relationships that centroid-based methods miss.
    
    \item \textbf{K-Means Efficiency:} K-Means demonstrates best internal cluster quality (highest Silhouette, lowest Davies-Bouldin) on Ludwig dataset, suggesting it creates geometrically optimal clusters for Spotify-sourced metadata.
    
    \item \textbf{Agglomerative Strength for Cultural Data:} Hierarchical clustering excels on the Indian Music dataset (ARI: 0.196), suggesting Ward linkage naturally captures hierarchical relationships between traditional regional genres with distinct cultural roots.
    
    \item \textbf{GMM Limitations:} Gaussian Mixture Models consistently underperform, with negative Silhouette scores on larger datasets indicating poor cluster separation. The full covariance parameterization may overfit, creating overlapping Gaussian components.
\end{enumerate}

\subsection{\textbf{Dataset-Specific Insights}}

\subsubsection{Small vs. Large Datasets}
\begin{itemize}
    \item \textbf{GTZAN (1K tracks):} Highest overall metrics benefit from controlled curation and balanced genre distribution
    \item \textbf{FMA Medium (17K tracks):} Lower internal metrics but higher purity suggests distinct genre cores with significant overlap regions
    \item \textbf{Scalability:} All algorithms maintain tractable performance on 17K+ tracks, validating practical applicability
\end{itemize}

\subsubsection{Western vs. Indian Music}
\begin{itemize}
    \item \textbf{Indian Music:} Higher purity (56.4\%) despite fewer genres indicates stronger acoustic distinctiveness of regional traditions
    \item \textbf{Cross-Cultural Challenge:} Lower ARI on FMA datasets reflects Western genre ambiguity (rock/blues overlap) compared to structurally distinct Indian classical forms
\end{itemize}

\subsection{\textbf{Cluster Count Impact}}

Analysis across $k \in \{5, 8, 10, 16\}$ reveals:
\begin{itemize}
    \item \textbf{Purity Increases with k:} Higher cluster counts improve genre matching by fragmenting mixed clusters
    \item \textbf{ARI Peaks at Intermediate k:} Best label alignment often occurs at $k=8$, balancing granularity with cluster purity
    \item \textbf{Silhouette Decreases with k:} More clusters reduce geometric separation, indicating trade-off between genre matching and cluster quality
\end{itemize}

\subsection{\textbf{Preprocessing Impact}}

Our comprehensive preprocessing pipeline demonstrates measurable benefits:

\begin{enumerate}
    \item \textbf{Normalization Necessity:} StandardScaler transformation is essential for distance-based algorithms, preventing bias toward large-magnitude features.
    
    \item \textbf{PCA Efficiency:} 39.2\% average dimensionality reduction with 95.15\% variance retention provides optimal balance between information preservation and computational efficiency.
    
    \item \textbf{Dataset Diversity:} Consistent preprocessing performance across Western (GTZAN, FMA, Ludwig) and Indian music validates generalizability.
\end{enumerate}

\subsection{\textbf{Challenges and Limitations}}

\subsubsection{Clustering Challenges}
\begin{itemize}
    \item Moderate ARI scores (0.1-0.23) indicate unsupervised methods recover only partial genre structure
    \item Genre boundaries are inherently fuzzy, limiting maximum achievable external metrics
    \item Cluster count selection remains dataset-dependent without clear optimal value
\end{itemize}

\subsubsection{Feature Extraction}
\begin{itemize}
    \item Fixed 30-second clips may miss long-term structural patterns
    \item Statistical aggregation (mean/std) loses temporal dynamics
    \item MFCC-dominated features may underweight rhythmic genre characteristics
\end{itemize}

\subsubsection{Genre Ambiguity}
\begin{itemize}
    \item Subjective genre boundaries create inherent labeling inconsistency
    \item Cross-genre fusion tracks resist single-label classification
    \item Temporal evolution of genres over decades affects feature consistency
\end{itemize}

\section{\textbf{Future Work}}
\label{sec:future}

\subsection{\textbf{Short-Term Extensions}}
\begin{itemize}
    \item Extended hyperparameter optimization via grid search for optimal $k$ selection
    \item Ensemble clustering combining Spectral and K-Means predictions
    \item Density-based methods (DBSCAN, HDBSCAN) for noise-robust clustering
    \item Cross-dataset validation to assess generalization capability
\end{itemize}

\subsection{\textbf{Advanced Techniques}}
\begin{itemize}
    \item \textbf{Deep Learning:} Autoencoder and VAE-based feature learning for non-linear representations
    \item \textbf{Temporal Modeling:} RNN/LSTM for sequence-level features capturing musical dynamics
    \item \textbf{Contrastive Learning:} Self-supervised pre-training for improved audio embeddings
    \item \textbf{Semi-Supervised Refinement:} Active learning with minimal labels to improve cluster purity
\end{itemize}

\subsection{\textbf{Application Domains}}
\begin{itemize}
    \item Music recommendation systems with genre-aware clustering
    \item Automated playlist generation using cluster proximity
    \item Music discovery for emerging artists through cluster analysis
    \item Cross-cultural music analysis bridging Western and traditional forms
\end{itemize}

\section{\textbf{Conclusion}}
\label{sec:conclusion}

This study presents a comprehensive investigation into unsupervised music genre discovery through systematic feature extraction, preprocessing, and clustering analysis. Processing 37,774 tracks across five diverse datasets (GTZAN, FMA Small, FMA Medium, Ludwig, and Indian Bollywood), we standardized on $k=10$ clusters to enable consistent cluster-to-genre mapping across all collections. Key findings include:

\begin{enumerate}
    \item \textbf{Unified Genre Mapping:} By fixing $k=10$, we successfully mapped discovered clusters to 10 normalized genre categories (Blues, Classical, Country, Disco, Hip-Hop, Jazz, Metal, Pop, Reggae, Rock) using majority voting based on genre composition within each cluster.
    
    \item \textbf{Cross-Dataset Consistency:} At $k=10$, our cluster-to-genre mapping achieved average ARI of 0.176 and Purity of 45.9\% across all five datasets, demonstrating that the 10-cluster structure captures fundamental genre characteristics regardless of dataset origin or size.
    
    \item \textbf{Dataset Size Impact:}
    \begin{itemize}
        \item \textbf{Small datasets (500 tracks):} Higher clustering quality (ARI: 0.196) with cleaner genre boundaries
        \item \textbf{Medium datasets (1K-11K tracks):} Balanced performance with ARI 0.132-0.225
        \item \textbf{Large datasets (17K tracks):} Higher purity (55.2\%) despite lower Silhouette scores, demonstrating scalability
    \end{itemize}
    
    \item \textbf{Algorithm Selection at k=10:}
    \begin{itemize}
        \item Spectral clustering: Best for Western music (GTZAN, FMA Medium)
        \item K-Means: Best for Spotify-sourced data (Ludwig)
        \item Agglomerative: Best for culturally distinct collections (Indian)
    \end{itemize}
    
    \item \textbf{Genre Label Normalization:} Original dataset labels (ranging from 5 to 16 genres) were successfully consolidated into 10 unified categories, enabling meaningful cross-dataset analysis and demonstrating that diverse music collections share common underlying genre structures.
\end{enumerate}

\textbf{Key Contributions:}
\begin{itemize}
    \item Standardized $k=10$ clustering approach enabling consistent cluster-to-genre mapping across 37,774 tracks
    \item Majority voting-based semantic genre labeling for discovered clusters
    \item Quantitative analysis of dataset size impact: from 500-track curated sets to 17,000-track large-scale collections
    \item Cross-cultural validation with unified genre mapping applicable to Western and Indian musical traditions
    \item Reproducible experimental pipeline with local logging and WandB integration
\end{itemize}

\textbf{Practical Implications:} Our cluster-to-genre mapping demonstrates that unsupervised clustering with $k=10$ provides a practical foundation for automatic music organization. The mapping (Cluster 0 $\rightarrow$ Blues, Cluster 1 $\rightarrow$ Classical, ..., Cluster 9 $\rightarrow$ Rock) aligns consistently across datasets from 500 to 17,000 tracks, suggesting this approach scales from personal music libraries to streaming platform catalogs.

\textbf{Scalability Analysis:} The transition from small (500 tracks) to large (17K tracks) datasets shows:
\begin{itemize}
    \item Purity increases with scale (53.0\% $\rightarrow$ 55.2\%) as larger datasets capture more complete genre distributions
    \item Silhouette scores remain stable (0.067 $\rightarrow$ 0.070), indicating consistent cluster quality
    \item Computational tractability maintained across all algorithms, validating production applicability
\end{itemize}

Future work will explore deep learning-based audio embeddings for improved feature representation, semi-supervised refinement using the cluster-to-genre mapping as initialization, and ensemble methods combining algorithmic strengths for enhanced genre boundary detection.

\section*{Acknowledgments}

The author thanks Dr. Kamlesh Datta, Department of Computer Science and Engineering, NIT Hamirpur, for guidance and supervision of this project. Special thanks to the Librosa development team for their excellent audio processing library, and to the creators of GTZAN, FMA, Ludwig, and Indian Bollywood Music datasets.

\begin{thebibliography}{99}

\bibitem{tzanetakis2002}
G. Tzanetakis and P. Cook, ``Musical genre classification of audio signals,'' \textit{IEEE Transactions on Speech and Audio Processing}, vol. 10, no. 5, pp. 293--302, 2002.

\bibitem{spijkervet2021}
J. Spijkervet and J. A. Burgoyne, ``Contrastive learning of musical representations,'' in \textit{Proc. Int. Soc. Music Inf. Retr. Conf. (ISMIR)}, 2021, pp. 673--680.

\bibitem{saeed2021}
A. Saeed, D. Grangier, and N. Zeghidour, ``Contrastive learning of general-purpose audio representations,'' in \textit{Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)}, 2021, pp. 3875--3879.

\bibitem{lee2020}
J. Lee, N. J. Bryan, J. Salamon, Z. Zhang, and J. Wang, ``Disentangled multidimensional metric learning for music similarity,'' in \textit{Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)}, 2020, pp. 1--5.

\bibitem{castellon2021}
R. Castellon, C. Donahue, and P. Liang, ``Codified audio language modeling learns useful representations for music information retrieval,'' in \textit{Proc. Int. Soc. Music Inf. Retr. Conf. (ISMIR)}, 2021, pp. 88--96.

\bibitem{mcfee2015}
B. McFee, C. Raffel, D. Liang, D. P. W. Ellis, M. McVicar, E. Battenberg, and O. Nieto, ``librosa: Audio and music signal analysis in python,'' in \textit{Proc. Python in Science Conference}, 2015, pp. 18--25.

\bibitem{defferrard2017}
M. Defferrard, K. Benzi, P. Vandergheynst, and X. Bresson, ``FMA: A dataset for music analysis,'' in \textit{Proc. Int. Society for Music Information Retrieval Conf. (ISMIR)}, 2017, pp. 316--323.

\bibitem{sturm2013}
B. L. Sturm, ``Classification accuracy is not enough: On the evaluation of music genre recognition systems,'' \textit{Journal of Intelligent Information Systems}, vol. 41, no. 3, pp. 371--406, 2013.

\bibitem{maaten2008}
L. van der Maaten and G. Hinton, ``Visualizing data using t-SNE,'' \textit{Journal of Machine Learning Research}, vol. 9, pp. 2579--2605, 2008.

\bibitem{hinton2006}
G. E. Hinton and R. R. Salakhutdinov, ``Reducing the dimensionality of data with neural networks,'' \textit{Science}, vol. 313, no. 5786, pp. 504--507, 2006.

\bibitem{dhariwal2020}
P. Dhariwal, H. Jun, C. Payne, J. W. Kim, A. Radford, and I. Sutskever, ``Jukebox: A generative model for music,'' \textit{arXiv preprint arXiv:2005.00341}, 2020.

\end{thebibliography}

\end{document}
