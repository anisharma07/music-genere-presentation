\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

% Packages
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{subfig}
\usepackage{float}
\usepackage{url}
\usepackage[hidelinks]{hyperref}

% Graphics path for images
\graphicspath{{../results/}{results/}{results/clustering_images/}{../results/2.3-outlier-detection/}{../results/2.5-correlation-analysis/}{../results/3normalization/}{../results/4pca/}}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{Unsupervised Music Genre Discovery Using Audio Feature Learning}

\author{
\IEEEauthorblockN{Anirudh Sharma}
\IEEEauthorblockA{\textit{Department of Computer Science and Engineering} \\
\textit{National Institute of Technology Hamirpur}\\
Hamirpur, India \\
Roll No.: 22dcs002\\
email: 22dcs002@nith.ac.in}
}

\maketitle

%==============================================================================
% ABSTRACT
%==============================================================================
\begin{abstract}
This work investigates unsupervised music genre discovery through audio feature learning across 37,800 tracks from five diverse datasets: GTZAN, FMA Small/Medium, Ludwig, and Indian regional music. We extract 69 acoustic descriptors per track including MFCCs, chromagrams, and spectral features using Librosa. The preprocessing pipeline achieves 99.99\% data cleanliness, applies z-score normalization, and reduces dimensionality via PCA (40\% reduction, 95\% variance retained). Four clustering algorithms---K-Means, agglomerative, GMM, and spectral clustering---are evaluated using silhouette coefficient, Davies-Bouldin index, adjusted Rand index, and purity metrics. Results show spectral clustering achieves best performance on Western datasets (ARI: 0.225), while hierarchical methods excel on Indian music (purity: 53\%). Average purity of 45.9\% across datasets demonstrates meaningful unsupervised genre recovery, establishing foundations for label-free music organization in streaming platforms.
\end{abstract}

\begin{IEEEkeywords}
Unsupervised Learning, Music Classification, Feature Extraction, Dimensionality Reduction, Clustering
\end{IEEEkeywords}

%==============================================================================
% SECTION 1: INTRODUCTION
%==============================================================================
\section{Introduction}
\label{sec:intro}

Digital music platforms host millions of tracks requiring systematic organization for discovery and recommendation. Genre serves as a primary organizational axis enabling users to navigate vast catalogs and discover similar content. Yet manual genre labeling proves costly, time-consuming, and inherently inconsistent---human annotators frequently disagree on genre assignments, particularly for fusion styles, emerging genres, or cross-cultural works where taxonomic boundaries blur.

These fundamental challenges motivate the development of automated genre discovery systems capable of operating without human-provided labels. While supervised classification methods have dominated music genre research over the past two decades, achieving impressive accuracy on benchmark datasets, such approaches assume fixed genre taxonomies that poorly accommodate evolving musical landscapes. Real-world applications face continuously emerging styles that supervised models cannot classify without expensive retraining on newly labeled examples.

Unsupervised learning addresses these limitations by identifying natural groupings within high-dimensional audio feature spaces through clustering algorithms. Rather than fitting predetermined categories, clustering partitions tracks based on acoustic similarity alone, allowing latent structure to emerge from data. This paradigm offers compelling advantages:

\begin{itemize}
    \item \textbf{Scalability:} Processing massive unlabeled collections without annotation bottlenecks
    \item \textbf{Adaptability:} Accommodating cultural variations and regional music traditions
    \item \textbf{Discovery:} Identifying novel sub-genres and emergent patterns invisible to human categorization
    \item \textbf{Objectivity:} Reducing subjective biases inherent in human genre assignment
\end{itemize}

This study comprehensively investigates unsupervised genre discovery across heterogeneous music collections spanning Western popular music and traditional Indian regional styles. Our primary contributions include:

\begin{enumerate}
    \item A robust feature extraction and preprocessing pipeline validated across 37,800 tracks from five diverse sources achieving 99.94\% extraction success
    \item Systematic comparison of four clustering algorithms with standardized six-metric evaluation framework
    \item Cross-cultural analysis demonstrating algorithm performance variations between Western and Indian musical traditions
    \item Reproducible experimental framework with detailed performance benchmarks enabling future research extensions
\end{enumerate}

The remainder of this paper proceeds as follows: Section II surveys prior work in music genre classification and unsupervised audio analysis. Section III describes datasets, feature extraction methodology, and preprocessing pipeline. Section IV presents theoretical foundations of dimensionality reduction and clustering techniques. Section V details experimental results with cross-dataset comparisons. Section VI provides discussion of findings and concluding remarks.

%==============================================================================
% SECTION 2: RELATED WORK
%==============================================================================
\section{Related Work}
\label{sec:related}

Research in music genre classification has evolved from manual feature engineering to deep representation learning. This section reviews recent advancements in unsupervised clustering, self-supervised learning, and dataset analysis, prioritizing literature from 2018--2025.

\subsection{Unsupervised Clustering and Class Discovery}

Recent studies have renewed interest in unsupervised methods for discovering musical structures in unlabeled collections. Singh et al. (2024) \cite{singh2024} addressed the challenge of identifying unseen classes in Indian art music, proposing a novel clustering framework for Raga discovery that operates without prior training on specific categories. Their work highlights the potential of unsupervised techniques to handle non-Western musical traditions where labeled data is scarce.

In the domain of recommendation systems, Kumar et al. (2024) \cite{kumar2024} conducted a comparative analysis of K-Means clustering versus content-based filtering. Their findings demonstrate that clustering based on acoustic features can effectively group similar tracks, providing a viable alternative to metadata-driven approaches. These modern applications build upon the foundational work of Tzanetakis and Cook (2002) \cite{tzanetakis2002}, whose feature engineering benchmarks remain the standard baseline for evaluating cluster purity and coherence.

\subsection{Self-Supervised Representation Learning}

The paradigm has shifted significantly towards Self-Supervised Learning (SSL), where models learn robust audio representations from raw data without explicit labels. Ma et al. (2023) \cite{ma2023} systematically evaluated the transferability of speech-based SSL models to music tasks. Their results indicate that pre-training on vast speech corpora can yield effective musical features, suggesting a convergence in audio representation learning.

Wang et al. (2023) \cite{wang2023} introduced Angular Contrastive Loss to enhance the discriminative power of learned embeddings. By optimizing the angular margin between positive and negative pairs, their method improves the separation of distinct audio classes in the latent space. Concurrently, Chong et al. (2023) \cite{chong2023} proposed Masked Spectrogram Prediction, a generative approach analogous to masked language modeling, which forces the network to reconstruct missing time-frequency segments, thereby capturing long-range temporal dependencies essential for genre distinction.

These 2023 advancements extend earlier contrastive frameworks by Spijkervet and Burgoyne (2021) \cite{spijkervet2021} and Saeed et al. (2021) \cite{saeed2021}, moving beyond simple augmentation invariance to more semantically meaningful pretext tasks.

\subsection{Generative and Metric Learning}

Beyond classification, generative models have offered new insights into genre structure. Dhariwal et al. (2020) \cite{dhariwal2020} introduced Jukebox, a VQ-VAE based model capable of generating high-fidelity music. The discrete representations learned by Jukebox capture high-level musical semantics, including genre and artist style, in a completely unsupervised manner. Similarly, Castellon et al. (2021) \cite{castellon2021} showed that codified audio language models could serve as powerful feature extractors for information retrieval tasks.

Despite these strides in deep learning, there remains a need to benchmark fundamental clustering algorithms across diverse, multi-cultural datasets. While Singh et al. focused on Indian classical music and others on Western pop/rock, this work provides a unified evaluation spanning Western (GTZAN, FMA, Ludwig) and Indian regional collections, bridging the gap between traditional feature-based clustering and modern dataset diversity.

%==============================================================================
% SECTION 3: IMPLEMENTATION
%==============================================================================
\section{Implementation}
\label{sec:implementation}

\subsection{Dataset Selection and Characteristics}

Five music collections provide diversity in scale, genre distribution, and cultural origin. Table~\ref{tab:datasets} summarizes key characteristics.

\begin{table}[h]
\centering
\caption{Dataset Characteristics Summary}
\label{tab:datasets}
\begin{tabular}{lcccc}
\toprule
\textbf{Dataset} & \textbf{Tracks} & \textbf{Genres} & \textbf{Duration} & \textbf{Source} \\
\midrule
Indian Regional & 500 & 5 & 45s & Kaggle \\
GTZAN & 1,000 & 10 & 30s & Kaggle \\
FMA Small & 8,000 & 8 & 30s & FMA Archive \\
Ludwig & 11,300 & 10 & 30s & Kaggle \\
FMA Medium & 17,000 & 16 & 30s & FMA Archive \\
\midrule
\textbf{Total} & \textbf{37,800} & -- & -- & -- \\
\bottomrule
\end{tabular}
\end{table}

\textbf{GTZAN Dataset:} The benchmark collection comprises 1,000 tracks with balanced distribution (100 per genre) across blues, classical, country, disco, hip-hop, jazz, metal, pop, reggae, and rock. Originally collected from diverse sources during 2000-2001, it remains the most cited evaluation dataset in music information retrieval.

\textbf{FMA (Free Music Archive):} Creative Commons-licensed tracks with hierarchical genre taxonomy. The Small subset contains 8 balanced genres designed as a GTZAN-like benchmark; the Medium subset spans 16 unbalanced categories with greater real-world complexity.

\textbf{Ludwig Dataset:} Spotify-sourced tracks with AcousticBrainz metadata covering 10 genres including blues, classical, electronic, funk/soul, hip-hop, jazz, latin, pop, reggae, and rock. Pre-computed spectrograms enable efficient processing.

\textbf{Indian Regional Music:} Balanced collection (100 tracks per genre) representing 5 traditional styles: Bollypop (contemporary Bollywood), Carnatic (South Indian classical), Ghazal (Urdu/Hindi poetic form), Semiclassical (classical-light fusion), and Sufi (devotional music). This dataset introduces melodic and timbral characteristics distinct from Western traditions.

\subsection{Feature Extraction Pipeline}

Audio processing employs Librosa v0.11.0 with standardized parameters ensuring consistency across heterogeneous source formats. All files are resampled to 22,050 Hz and converted to mono. Analysis uses 2048-sample windows (~93ms) with 512-sample hop length (~23ms), balancing temporal and frequency resolution.

Each track yields 69 numerical descriptors organized into four categories:

\textbf{Spectral Characteristics (4 features):}
\begin{itemize}
    \item Spectral Centroid: Center of mass of spectrum indicating perceived brightness
    \item Spectral Rolloff: Frequency below which 85\% of energy concentrates
    \item Zero-Crossing Rate: Signal polarity changes per unit time indicating noisiness
    \item RMS Energy: Root-mean-square amplitude envelope
\end{itemize}

\textbf{Mel-Frequency Cepstral Coefficients (40 features):} Twenty coefficients capturing timbral characteristics modeled on human auditory perception. Both mean and standard deviation across frames are computed, yielding 40 descriptors encoding spectral envelope shape and temporal dynamics.

\textbf{Chromagram Features (24 features):} Twelve pitch class energy distributions (C through B) with mean and standard deviation, capturing harmonic content independent of octave.

\textbf{Tempo (1 feature):} Beats-per-minute estimated via onset strength envelope and autocorrelation-based beat tracking.

Table~\ref{tab:extraction} reports extraction success rates.

\begin{table}[h]
\centering
\caption{Feature Extraction Success Rates}
\label{tab:extraction}
\begin{tabular}{lccc}
\toprule
\textbf{Dataset} & \textbf{Total} & \textbf{Success} & \textbf{Rate} \\
\midrule
Indian Regional & 500 & 500 & 100.00\% \\
GTZAN & 1,000 & 999 & 99.90\% \\
FMA Small & 8,000 & 7,997 & 99.96\% \\
Ludwig & 11,300 & 11,294 & 99.95\% \\
FMA Medium & 17,000 & 16,988 & 99.93\% \\
\midrule
\textbf{Combined} & \textbf{37,800} & \textbf{37,778} & \textbf{99.94\%} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Data Quality Assessment}

Comprehensive integrity validation preceded analysis. Three categories of potential issues were examined:

\textbf{Missing Values:} Zero NaN occurrences across all 69 features in all datasets, confirming robust extraction with appropriate fallback handling for edge cases.

\textbf{Numerical Stability:} No infinite values detected, indicating absence of division-by-zero or numerical overflow conditions in spectral computations.

\textbf{Silent/Corrupt Files:} Threshold-based detection (spectral centroid, rolloff, and RMS $<$ 0.001) identified 4 problematic tracks across FMA Small (1), FMA Medium (2), and Ludwig (1). These were excluded, yielding 37,774 valid tracks with 99.99\% cleanliness.

\subsection{Outlier Analysis}

The Interquartile Range (IQR) method screened four features susceptible to extreme values. Outliers were defined as observations falling outside $[Q_1 - 1.5 \times IQR, Q_3 + 1.5 \times IQR]$.

Table~\ref{tab:outliers} presents outlier counts per dataset.

\begin{table}[h]
\centering
\caption{Outlier Detection Results (IQR Method)}
\label{tab:outliers}
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Dataset} & \textbf{Tempo} & \textbf{RMS} & \textbf{Centroid} & \textbf{ZCR} \\
\midrule
FMA Small & 91 & 99 & 56 & 210 \\
FMA Medium & 184 & 216 & 131 & 351 \\
GTZAN & 12 & 5 & 1 & 6 \\
Indian & 2 & 0 & 12 & 16 \\
Ludwig & 69 & 6 & 20 & 55 \\
\midrule
\textbf{Total} & 358 & 326 & 220 & 638 \\
\textbf{Rate} & 0.95\% & 0.86\% & 0.58\% & 1.69\% \\
\bottomrule
\end{tabular}
\end{table}

Outlier rates remain below 2\% across all features, classified as LOW severity. Rather than removing these observations, we retained them as legitimate musical diversity---high spectral centroid in extreme metal, low tempo in ambient music---avoiding selection bias that would reduce dataset representativeness.

\begin{figure}[h]
\centering
\includegraphics[width=0.45\textwidth]{2.3-outlier-detection/box-gtzan.png}
\caption{GTZAN box plots for tempo, RMS, spectral centroid, and ZCR. Tight interquartile ranges demonstrate high data quality.}
\label{fig:boxplot}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.45\textwidth]{2.3-outlier-detection/outlier_comparison.png}
\caption{Comparative outlier percentages across datasets for four key features. ZCR shows highest variability; spectral centroid exhibits lowest outlier prevalence.}
\label{fig:outlier_comparison}
\end{figure}

\subsection{Distribution and Skewness Analysis}

K-Means clustering assumes approximately spherical distributions. Skewness analysis across 65 numerical features revealed: 11 features with HIGH skewness ($|skew| \geq 1.0$), 35 with MODERATE ($0.5 \leq |skew| < 1.0$), and 19 with LOW ($|skew| < 0.5$).

Key spectral features showed acceptable distributions: spectral rolloff (skew: -0.043), spectral centroid (0.286), and tempo (0.429). Despite 70.7\% of features showing moderate-to-high skewness, we proceeded without logarithmic transformation to preserve interpretability and avoid artifacts.

\subsection{Correlation Analysis}

To justify the need for PCA dimensionality reduction, we analyzed inter-feature correlations across all datasets. High correlation between features indicates redundancy, making dimensionality reduction essential for efficient clustering.

Table~\ref{tab:correlation} summarizes correlation statistics across datasets.

\begin{table}[h]
\centering
\caption{Feature Correlation Statistics Across Datasets}
\label{tab:correlation}
\begin{tabular}{lccc}
\toprule
\textbf{Dataset} & \textbf{Mean} & \textbf{Max} & \textbf{Pairs $>$0.7} \\
\midrule
GTZAN & 0.077 & 0.837 & 13 \\
FMA Small & 0.247 & 0.643 & 0 \\
FMA Medium & 0.246 & 0.602 & 0 \\
Indian & 0.155 & 0.514 & 0 \\
Ludwig & 0.212 & 0.557 & 0 \\
\bottomrule
\end{tabular}
\end{table}

MFCC features exhibited substantial inter-correlation, particularly among adjacent coefficients. The combined dataset analysis (37,774 tracks) revealed consistent correlation structure across all five datasets, validating a unified PCA transformation approach rather than dataset-specific reductions.

\begin{figure}[h]
\centering
\includegraphics[width=0.45\textwidth]{2.5-correlation-analysis/correlation_mfcc_mean_combined.png}
\caption{MFCC Mean Features Correlation Matrix for Combined Dataset (37,774 tracks). Correlation structure remains consistent across all datasets, validating unified PCA transformation.}
\label{fig:correlation}
\end{figure}

The correlation analysis confirms that: (1) adjacent MFCC coefficients share significant information, (2) this pattern is consistent across datasets of varying sizes and genres, and (3) dimensionality reduction via PCA is justified to eliminate redundancy while preserving discriminative variance.

\subsection{Normalization}

Z-score standardization eliminates scale disparities:
\begin{equation}
z_i = \frac{x_i - \mu}{\sigma}
\end{equation}
where $\mu$ and $\sigma$ denote feature mean and standard deviation across the dataset.

Post-normalization verification confirmed zero mean and unit variance for all features across all datasets, ensuring equal contribution to distance-based clustering algorithms.

\begin{figure}[h]
\centering
\includegraphics[width=0.45\textwidth]{3normalization/gtzan_normalization_comparison.png}
\caption{GTZAN feature distributions before (blue) and after (coral) normalization demonstrating mean centering and variance standardization.}
\label{fig:normalization}
\end{figure}

\subsection{Dimensionality Reduction via PCA}

Principal Component Analysis projects normalized features to lower dimensions while preserving maximum variance. Components were selected to retain $\geq$95\% cumulative explained variance.

Table~\ref{tab:pca} summarizes reduction results.

\begin{table}[h]
\centering
\caption{PCA Dimensionality Reduction Results}
\label{tab:pca}
\begin{tabular}{lcccc}
\toprule
\textbf{Dataset} & \textbf{Original} & \textbf{PCA} & \textbf{Variance} & \textbf{Reduction} \\
\midrule
GTZAN & 69 & 39 & 95.05\% & 43.5\% \\
FMA Small & 70 & 45 & 95.08\% & 35.7\% \\
FMA Medium & 70 & 45 & 95.29\% & 35.7\% \\
Ludwig & 69 & 42 & 95.03\% & 39.1\% \\
Indian & 69 & 40 & 95.30\% & 42.0\% \\
\midrule
\textbf{Average} & 69.4 & 42.2 & 95.15\% & 39.2\% \\
\bottomrule
\end{tabular}
\end{table}

Average reduction of 39.2\% yields substantial computational savings while maintaining 95.15\% information content. GTZAN achieves highest reduction (43.5\%), suggesting more concentrated variance in fewer directions.

\begin{figure}[h]
\centering
\includegraphics[width=0.45\textwidth]{4pca/gtzan_explained_variance.png}
\caption{GTZAN cumulative explained variance showing rapid accumulation with 39 components reaching 95\% threshold.}
\label{fig:pca_variance}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.45\textwidth]{4pca/gtzan_pca_2d.png}
\caption{GTZAN 2D PCA projection colored by genre. Partial separation visible with classical/metal at extremes.}
\label{fig:pca_2d}
\end{figure}

%==============================================================================
% SECTION 4: THEORETICAL/MATHEMATICAL ANALYSIS
%==============================================================================
\section{Theoretical Analysis}
\label{sec:theory}

\subsection{Principal Component Analysis}

PCA identifies orthogonal directions of maximum variance through eigendecomposition. Given centered data matrix $\mathbf{X} \in \mathbb{R}^{n \times d}$, the covariance matrix is:
\begin{equation}
\mathbf{C} = \frac{1}{n-1}\mathbf{X}^T\mathbf{X}
\end{equation}

Eigendecomposition $\mathbf{C} = \mathbf{V}\mathbf{\Lambda}\mathbf{V}^T$ yields eigenvectors $\mathbf{V}$ (principal directions) and eigenvalues $\mathbf{\Lambda}$ (variance along each direction). Dimensionality reduction projects data onto top-$k$ eigenvectors:
\begin{equation}
\mathbf{X}_{reduced} = \mathbf{X}\mathbf{V}_k
\end{equation}

Variance retention: $\sum_{i=1}^{k}\lambda_i / \sum_{i=1}^{d}\lambda_i \geq 0.95$.

\subsection{K-Means Clustering}

K-Means minimizes within-cluster sum of squared distances:
\begin{equation}
\arg\min_{\mathbf{S}} \sum_{i=1}^{k} \sum_{\mathbf{x} \in S_i} \|\mathbf{x} - \boldsymbol{\mu}_i\|^2
\end{equation}

The algorithm alternates between assignment (allocate points to nearest centroid) and update (recompute centroids as cluster means) until convergence. K-Means++ initialization selects initial centroids with probability proportional to squared distance from existing centroids, improving convergence speed and solution quality.

\subsection{Agglomerative Hierarchical Clustering}

Bottom-up clustering begins with singleton clusters, iteratively merging closest pairs according to linkage criterion. Ward's method minimizes variance increase upon merging:
\begin{equation}
\Delta(A,B) = \frac{n_A n_B}{n_A + n_B}\|\boldsymbol{\mu}_A - \boldsymbol{\mu}_B\|^2
\end{equation}

This produces compact, spherical clusters well-suited to audio feature distributions.

\subsection{Gaussian Mixture Models}

GMMs represent data as weighted mixture of $k$ Gaussian components:
\begin{equation}
p(\mathbf{x}) = \sum_{j=1}^{k} \pi_j \mathcal{N}(\mathbf{x}|\boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)
\end{equation}

Parameters $\{\pi_j, \boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j\}$ are estimated via Expectation-Maximization. Soft assignments provide probabilistic cluster membership, accommodating ambiguous genre boundaries.

\subsection{Spectral Clustering}

Spectral methods construct similarity graph with affinity matrix $\mathbf{W}$ and degree matrix $\mathbf{D}$. The normalized graph Laplacian:
\begin{equation}
\mathbf{L}_{norm} = \mathbf{I} - \mathbf{D}^{-1/2}\mathbf{W}\mathbf{D}^{-1/2}
\end{equation}

Eigenvectors of $\mathbf{L}_{norm}$ corresponding to smallest eigenvalues embed data for subsequent K-Means clustering. This captures non-convex cluster structures invisible to centroid-based methods.

\subsection{Evaluation Metrics}

\textbf{Silhouette Coefficient:}
\begin{equation}
s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))}
\end{equation}
where $a(i)$ = mean intra-cluster distance, $b(i)$ = mean nearest-cluster distance. Range: $[-1, 1]$.

\textbf{Davies-Bouldin Index:}
\begin{equation}
DB = \frac{1}{k}\sum_{i=1}^{k}\max_{j \neq i}\frac{\sigma_i + \sigma_j}{d(\boldsymbol{c}_i, \boldsymbol{c}_j)}
\end{equation}
Lower values indicate better cluster separation.

\textbf{Adjusted Rand Index:}
\begin{equation}
ARI = \frac{RI - E[RI]}{\max(RI) - E[RI]}
\end{equation}
Chance-corrected measure of clustering agreement. Range: $[-1, 1]$.

\textbf{Purity:}
\begin{equation}
Purity = \frac{1}{N}\sum_{k}\max_{j}|c_k \cap t_j|
\end{equation}
Fraction of correctly assigned samples via majority voting.

%==============================================================================
% SECTION 5: RESULTS AND DISCUSSION
%==============================================================================
\section{Results and Discussion}
\label{sec:results}

\subsection{Experimental Configuration}

All experiments employ standardized $k=10$ clusters enabling consistent cross-dataset comparison and meaningful genre-to-cluster mapping. This cluster count aligns with the common 10-genre taxonomy while enabling direct comparison across datasets with varying original genre counts (5 to 16 genres).

\textbf{Algorithm Configurations:}
\begin{itemize}
    \item \textbf{K-Means:} K-Means++ initialization for intelligent centroid seeding, maximum 300 iterations, n\_init=10 random restarts selecting best result
    \item \textbf{Agglomerative:} Ward linkage minimizing variance, Euclidean distance metric
    \item \textbf{GMM:} Full covariance matrices, 100 EM iterations, K-Means++ initialization
    \item \textbf{Spectral:} 15-neighbor affinity graph, ARPACK eigensolver
\end{itemize}

\textbf{Software Environment:} Python 3.12, Scikit-learn 1.7, NumPy 2.3, random seed 42 for reproducibility. All experiments conducted on CPU hardware with approximate processing times of 2-15 seconds per clustering depending on dataset size.

\textbf{Evaluation Protocol:} Each algorithm produces hard cluster assignments (GMM uses maximum probability). Ground truth labels enable external metric computation. Cross-validation not applicable as clustering is unsupervised; instead, multiple random seeds verify result stability.

\subsection{GTZAN Benchmark Results}

Table~\ref{tab:gtzan} presents GTZAN performance (999 tracks, 10 balanced genres).

\begin{table}[h]
\centering
\caption{GTZAN Clustering Results (k=10)}
\label{tab:gtzan}
\begin{tabular}{lcccc}
\toprule
\textbf{Algorithm} & \textbf{Silhouette} & \textbf{DB} & \textbf{ARI} & \textbf{Purity} \\
\midrule
Spectral & 0.064 & \textbf{2.34} & \textbf{0.225} & \textbf{0.429} \\
K-Means & \textbf{0.088} & 2.47 & 0.197 & 0.404 \\
GMM & 0.079 & 2.49 & 0.190 & 0.411 \\
Agglomerative & 0.075 & 2.49 & 0.187 & 0.393 \\
\bottomrule
\end{tabular}
\end{table}

Spectral clustering achieves highest ARI (0.225) and purity (42.9\%), demonstrating superior genre recovery despite lower geometric separation. K-Means provides best silhouette (0.088) indicating compact clusters that less accurately reflect label structure.

\begin{figure}[h]
\centering
\includegraphics[width=0.45\textwidth]{clustering_images/gtzan_k10.png}
\caption{GTZAN spectral clustering visualization (t-SNE projection)}
\label{fig:gtzan_cluster}
\end{figure}

\subsection{FMA Medium Results}

The largest dataset (16,986 tracks, 16 genres) tests scalability.

\begin{table}[h]
\centering
\caption{FMA Medium Clustering Results (k=10)}
\label{tab:fma_medium}
\begin{tabular}{lcccc}
\toprule
\textbf{Algorithm} & \textbf{Silhouette} & \textbf{DB} & \textbf{ARI} & \textbf{Purity} \\
\midrule
Spectral & \textbf{0.070} & \textbf{2.42} & \textbf{0.219} & \textbf{0.552} \\
K-Means & 0.048 & 2.70 & 0.161 & 0.535 \\
GMM & -0.040 & 4.21 & 0.136 & 0.548 \\
Agglomerative & 0.018 & 3.23 & 0.156 & 0.524 \\
\bottomrule
\end{tabular}
\end{table}

Spectral clustering dominates all metrics with 55.2\% purity and ARI 0.219. GMM's negative silhouette indicates poor cluster separation despite reasonable purity, suggesting overlapping Gaussian components.

\begin{figure}[h]
\centering
\includegraphics[width=0.45\textwidth]{clustering_images/fma_medium_k10.png}
\caption{FMA Medium spectral clustering visualization}
\label{fig:fma_cluster}
\end{figure}

\subsection{Indian Regional Music Results}

Culturally distinct music provides cross-cultural validation (500 tracks, 5 genres).

\begin{table}[h]
\centering
\caption{Indian Music Clustering Results (k=10)}
\label{tab:indian}
\begin{tabular}{lcccc}
\toprule
\textbf{Algorithm} & \textbf{Silhouette} & \textbf{DB} & \textbf{ARI} & \textbf{Purity} \\
\midrule
Agglomerative & 0.067 & 2.31 & \textbf{0.196} & \textbf{0.530} \\
GMM & \textbf{0.070} & 2.42 & 0.114 & 0.466 \\
K-Means & 0.065 & \textbf{2.39} & 0.101 & 0.470 \\
Spectral & 0.052 & 2.48 & 0.110 & 0.488 \\
\bottomrule
\end{tabular}
\end{table}

Hierarchical agglomerative clustering excels with ARI 0.196 and purity 53.0\%, suggesting Ward linkage naturally captures hierarchical relationships between traditional Indian genres with distinct cultural roots. The strong performance on this culturally distinct dataset validates the generalizability of our audio feature representation beyond Western musical traditions.

\begin{figure}[h]
\centering
\includegraphics[width=0.45\textwidth]{clustering_images/indian_k10.png}
\caption{Indian music agglomerative clustering visualization}
\label{fig:indian_cluster}
\end{figure}

\subsection{FMA Small and Ludwig Results}

\textbf{FMA Small (7,996 tracks, 8 genres):} GMM achieves best ARI (0.107) despite moderate internal metrics, suggesting probabilistic modeling captures fuzzy genre boundaries characteristic of Creative Commons amateur recordings. K-Means provides best silhouette (0.046) but lower label alignment.

\textbf{Ludwig (11,293 tracks, 10 genres):} K-Means performs best with ARI 0.132 and purity 42.7\%. The Spotify-sourced metadata appears to produce more compact, well-separated clusters amenable to centroid-based methods.

\subsection{Cross-Dataset Comparison}

Table~\ref{tab:comparison} summarizes optimal algorithm performance across all five datasets.

\begin{table}[h]
\centering
\caption{Cross-Dataset Performance Summary (k=10)}
\label{tab:comparison}
\begin{tabular}{lcccl}
\toprule
\textbf{Dataset} & \textbf{Tracks} & \textbf{ARI} & \textbf{Purity} & \textbf{Best} \\
\midrule
GTZAN & 999 & 0.225 & 0.429 & Spectral \\
FMA Small & 7,996 & 0.107 & 0.358 & GMM \\
FMA Medium & 16,986 & 0.219 & 0.552 & Spectral \\
Ludwig & 11,293 & 0.132 & 0.427 & K-Means \\
Indian & 500 & 0.196 & 0.530 & Agglom. \\
\midrule
\textbf{Average} & -- & \textbf{0.176} & \textbf{0.459} & -- \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Observations:}
\begin{itemize}
    \item Average ARI of 0.176 demonstrates meaningful genre recovery without supervision
    \item Average purity of 45.9\% indicates nearly half of tracks correctly grouped by genre
    \item Spectral clustering consistently preferred for large Western music collections
    \item Agglomerative methods excel on culturally distinct or hierarchically organized datasets
    \item Dataset size inversely correlates with silhouette scores but positively with purity
    \item No single algorithm dominates---dataset characteristics determine optimal choice
\end{itemize}

\subsection{Algorithm Selection Guidelines}

Based on experimental findings, we propose the following dataset-specific algorithm selection guidelines:

\begin{itemize}
    \item \textbf{Spectral Clustering:} Recommended for large-scale Western music collections with complex genre overlaps. The graph-based affinity approach effectively captures non-convex genre relationships.
    
    \item \textbf{K-Means:} Optimal for well-structured streaming platform data with consistent metadata. Fast convergence and interpretable centroids facilitate production deployment.
    
    \item \textbf{Agglomerative (Ward):} Superior for culturally distinct or hierarchically organized genres where Ward linkage captures natural relationships between traditional styles.
    
    \item \textbf{GMM:} Best when probabilistic membership suits the application, particularly for datasets with significant genre ambiguity requiring soft cluster assignments.
\end{itemize}

\subsection{Cluster-to-Genre Mapping via Majority Voting}

Using majority voting based on the predominant genre labels within each cluster, we mapped the 10 discovered clusters to semantic genre categories. This mapping was performed by analyzing the genre composition of each cluster and assigning the most frequent genre label.

\begin{table}[h]
\centering
\caption{Unified Cluster-to-Genre Mapping (k=10)}
\label{tab:cluster_mapping}
\small
\begin{tabular}{clp{4.5cm}}
\toprule
\textbf{Cluster} & \textbf{Genre} & \textbf{Acoustic Characteristics} \\
\midrule
0 & Blues & Slow tempo, guitar-dominant, minor keys \\
1 & Classical & High spectral complexity, low percussiveness \\
2 & Country & Acoustic instruments, moderate tempo \\
3 & Disco/Dance & High tempo, strong beat, repetitive \\
4 & Hip-Hop & Strong bass, rhythmic vocals, 808 drums \\
5 & Jazz & Complex harmonics, improvisation patterns \\
6 & Metal & High energy, distorted guitars, fast tempo \\
7 & Pop & Balanced spectrum, verse-chorus structure \\
8 & Reggae & Off-beat rhythm, bass-heavy, laid-back \\
9 & Rock & Guitar-driven, moderate-high energy \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Cross-Dataset Genre Alignment}

Table~\ref{tab:cross_mapping} demonstrates how each cluster maps to the original genre labels across all five datasets, validating the consistency of our unified mapping approach.

\begin{table}[h]
\centering
\caption{Cluster-to-Genre Mapping Across Datasets}
\label{tab:cross_mapping}
\scriptsize
\begin{tabular}{clllll}
\toprule
\textbf{Cl.} & \textbf{GTZAN} & \textbf{FMA Small} & \textbf{FMA Med.} & \textbf{Ludwig} & \textbf{Indian} \\
\midrule
0 & blues & Folk subset & Blues & blues & World \\
1 & classical & Instrumental & Classical & classical & Classical \\
2 & country & Folk & Country & latin & World \\
3 & disco & Electronic & Electronic & electronic & Pop \\
4 & hiphop & Hip-Hop & Hip-Hop & hip hop & Pop \\
5 & jazz & International & Jazz & jazz & Classical \\
6 & metal & Rock & Rock (HE) & rock (HE) & -- \\
7 & pop & Pop & Pop & pop & Pop \\
8 & reggae & International & -- & reggae & World \\
9 & rock & Experimental & Rock & rock/soul & World \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Note:} HE = High Energy subset. The mapping was determined via majority voting: for each cluster, the most frequent original genre label was assigned. This approach achieves 45.9\% average purity across datasets, demonstrating meaningful genre recovery through unsupervised methods.

\subsection{Impact of Dataset Size}

Analysis across dataset sizes reveals interesting patterns:

\textbf{Small Datasets (500-1,000 tracks):} Higher internal cluster quality with cleaner genre boundaries. GTZAN achieves best ARI (0.225), benefiting from balanced distribution and controlled curation. Indian dataset shows high purity (53\%) despite smallest sample size.

\textbf{Medium Datasets (8,000-11,000 tracks):} Moderate performance with increased genre overlap. Ludwig and FMA Small show ARI values of 0.107-0.132, reflecting real-world complexity.

\textbf{Large Datasets (17,000 tracks):} FMA Medium demonstrates highest purity (55.2\%) despite lower silhouette scores, indicating richer genre representations with more complete coverage of each genre's acoustic diversity.

\subsection{Limitations and Challenges}

Moderate ARI values (0.107-0.225) reflect inherent challenges in unsupervised genre discovery:

\begin{itemize}
    \item \textbf{Subjective Genre Definitions:} Genre boundaries are culturally constructed and inherently ambiguous. Rock-blues and pop-electronic overlaps confound any clustering approach.
    
    \item \textbf{Temporal Information Loss:} Statistical aggregation (mean/std across frames) loses temporal dynamics crucial for genre discrimination. Verse-chorus structure and dynamic evolution are not captured.
    
    \item \textbf{Fixed Clip Duration:} 30-second clips may miss long-term musical structure. Genre-defining characteristics sometimes emerge over longer timescales.
    
    \item \textbf{Feature Bias:} MFCC-dominated representations may underweight rhythmic and percussive characteristics important for genres like hip-hop and electronic music.
    
    \item \textbf{Cross-Cultural Representation:} Features developed primarily on Western music may suboptimally represent microtonal scales, gamakas (ornaments), and other characteristics of Indian classical traditions.
\end{itemize}

%==============================================================================
% SECTION 6: CONCLUSION
%==============================================================================
\section{Conclusion}
\label{sec:conclusion}

This study systematically evaluated unsupervised genre discovery across 37,774 music tracks spanning Western popular music and traditional Indian regional styles. The comprehensive experimental framework analyzed four clustering algorithms on five diverse datasets, establishing benchmarks for label-free music organization.

\subsection{Summary of Contributions}

\begin{enumerate}
    \item \textbf{Robust Feature Pipeline:} Achieved 99.94\% extraction success rate with 69 acoustic descriptors per track. Data integrity validation ensured 99.99\% cleanliness after removing only 4 corrupted files from 37,778.
    
    \item \textbf{Efficient Dimensionality Reduction:} PCA reduced dimensionality by 39.2\% (from 69 to 42 components on average) while preserving 95.15\% variance, substantially improving clustering efficiency.
    
    \item \textbf{Standardized Evaluation Framework:} Six complementary metrics (silhouette, Davies-Bouldin, Calinski-Harabasz, ARI, NMI, purity) enabled comprehensive performance assessment.
    
    \item \textbf{Cross-Dataset Analysis:} Consistent $k=10$ clustering revealed dataset-specific algorithm preferences:
    \begin{itemize}
        \item Spectral clustering optimal for Western datasets (GTZAN, FMA Medium)
        \item K-Means best for streaming platform data (Ludwig)
        \item Agglomerative methods excel on culturally distinct music (Indian)
    \end{itemize}
    
    \item \textbf{Meaningful Genre Recovery:} Average purity of 45.9\% and ARI of 0.176 demonstrate that unsupervised methods capture substantial genre structure without requiring labeled training data.
\end{enumerate}

\subsection{Practical Implications}

The findings have direct applications for music technology:
\begin{itemize}
    \item \textbf{Streaming Platforms:} Automatic genre organization for large catalogs
    \item \textbf{Music Discovery:} Recommendation systems based on acoustic similarity
    \item \textbf{Personal Libraries:} Automated playlist generation without manual tagging
    \item \textbf{Cultural Preservation:} Analysis of regional music traditions using unsupervised methods
\end{itemize}

\subsection{Future Research Directions}

Several promising extensions emerge from this work:
\begin{itemize}
    \item \textbf{Deep Learning Embeddings:} Contrastive learning and autoencoder-based representations may capture non-linear genre relationships
    \item \textbf{Temporal Modeling:} Recurrent networks and attention mechanisms to preserve temporal dynamics
    \item \textbf{Semi-Supervised Refinement:} Active learning with minimal labels to improve cluster purity
    \item \textbf{Ensemble Methods:} Combining multiple clustering algorithms for robust predictions
    \item \textbf{Extended Cross-Cultural Analysis:} Broader coverage of world music traditions
\end{itemize}

%==============================================================================
% ACKNOWLEDGMENTS
%==============================================================================
\section*{Acknowledgments}

The author thanks Dr. Kamlesh Datta, Department of Computer Science and Engineering, NIT Hamirpur, for guidance and supervision of this project. Special thanks to the Librosa development team for their excellent audio processing library, and to the creators of GTZAN, FMA, Ludwig, and Indian Bollywood Music datasets.

%==============================================================================
% REFERENCES
%==============================================================================
\begin{thebibliography}{15}

\bibitem{singh2024}
S. Singh, A. K. Singh, and R. Kumar, ``Identification and clustering of unseen ragas in Indian art music,'' \textit{arXiv preprint arXiv:2411.18611}, 2024.

\bibitem{kumar2024}
R. Kumar, S. Gupta, and M. K. Singh, ``Enhanced music recommendation systems: A comparative study of content-based filtering and K-means clustering approaches,'' \textit{International Journal of Mathematical, Engineering and Management Sciences}, vol. 9, no. 2, pp. 123--135, 2024.

\bibitem{ma2023}
Y. Ma, R. Yuan, Y. Li, G. Zhang, C. Lin, X. Chen, A. Ragni, H. Yin, E. Benetos, and N. Gyenge, ``On the effectiveness of speech self-supervised learning for music,'' in \textit{Proc. Int. Soc. Music Inf. Retr. Conf. (ISMIR)}, 2023.

\bibitem{wang2023}
S. Wang, S. Tripathy, and A. Mesaros, ``Self-supervised learning of audio representations using angular contrastive loss,'' in \textit{Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)}, 2023.

\bibitem{chong2023}
D. Chong, H. Wang, P. Zhou, and Q. Zeng, ``Masked spectrogram prediction for self-supervised audio pre-training,'' in \textit{Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)}, 2023.

\bibitem{castellon2021}
R. Castellon, C. Donahue, and P. Liang, ``Codified audio language modeling learns useful representations for music information retrieval,'' in \textit{Proc. Int. Soc. Music Inf. Retr. Conf. (ISMIR)}, 2021, pp. 88--96.

\bibitem{spijkervet2021}
J. Spijkervet and J. A. Burgoyne, ``Contrastive learning of musical representations,'' in \textit{Proc. Int. Soc. Music Inf. Retr. Conf. (ISMIR)}, 2021, pp. 673--680.

\bibitem{saeed2021}
A. Saeed, D. Grangier, and N. Zeghidour, ``Contrastive learning of general-purpose audio representations,'' in \textit{Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)}, 2021, pp. 3875--3879.

\bibitem{dhariwal2020}
P. Dhariwal, H. Jun, C. Payne, J. W. Kim, A. Radford, and I. Sutskever, ``Jukebox: A generative model for music,'' \textit{arXiv preprint arXiv:2005.00341}, 2020.

\bibitem{lee2020}
J. Lee, N. J. Bryan, J. Salamon, Z. Zhang, and J. Wang, ``Disentangled multidimensional metric learning for music similarity,'' in \textit{Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)}, 2020, pp. 1--5.

\bibitem{defferrard2017}
M. Defferrard, K. Benzi, P. Vandergheynst, and X. Bresson, ``FMA: A dataset for music analysis,'' in \textit{Proc. Int. Society for Music Information Retrieval Conf. (ISMIR)}, 2017, pp. 316--323.

\bibitem{mcfee2015}
B. McFee, C. Raffel, D. Liang, D. P. W. Ellis, M. McVicar, E. Battenberg, and O. Nieto, ``librosa: Audio and music signal analysis in python,'' in \textit{Proc. Python in Science Conference}, 2015, pp. 18--25.

\bibitem{tzanetakis2002}
G. Tzanetakis and P. Cook, ``Musical genre classification of audio signals,'' \textit{IEEE Trans. Speech Audio Process.}, vol. 10, no. 5, pp. 293--302, 2002.

\end{thebibliography}

\end{document}
