{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78ffa90f",
   "metadata": {},
   "source": [
    "# Music Genre Discovery - Code Implementation\n",
    "\n",
    "**Project:** Unsupervised Music Genre Discovery Using Audio Feature Learning  \n",
    "**Author:** Anirudh Sharma  \n",
    "**Date:** November 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements unsupervised clustering algorithms for music genre discovery, including:\n",
    "- K-Means Clustering\n",
    "- MiniBatch K-Means\n",
    "- Spectral Clustering\n",
    "- DBSCAN\n",
    "- Gaussian Mixture Models (GMM)\n",
    "\n",
    "**Evaluation Metrics:**\n",
    "- Silhouette Score\n",
    "- Davies-Bouldin Index\n",
    "- Calinski-Harabasz Index\n",
    "- Normalized Mutual Information (NMI)\n",
    "- Adjusted Rand Index (ARI)\n",
    "- V-Measure\n",
    "- Cluster Accuracy (Hungarian Algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bde6658",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e425c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "from time import time\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans, SpectralClustering, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score,\n",
    "    davies_bouldin_score,\n",
    "    calinski_harabasz_score,\n",
    "    normalized_mutual_info_score,\n",
    "    adjusted_rand_score,\n",
    "    v_measure_score\n",
    ")\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# Ignore warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "print(\"✓ Libraries imported successfully!\")\n",
    "print(f\"  - pandas version: {pd.__version__}\")\n",
    "print(f\"  - numpy version: {np.__version__}\")\n",
    "print(f\"  - scikit-learn imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fa48a3",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b64704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "CONFIG = {\n",
    "    'N_CLUSTERS': 10,\n",
    "    'N_PCA_COMPONENTS': 20,\n",
    "    'RANDOM_STATE': 42,\n",
    "    'DBSCAN_EPS': 2.5,\n",
    "    'DBSCAN_MIN_SAMPLES': 5,\n",
    "    'SPLIT_RATIOS': [(50, 50), (60, 40), (70, 30), (80, 20)]\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(\"=\" * 60)\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865e46af",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc0df20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned dataset\n",
    "df = pd.read_csv('gtzan/features_30_sec_cleaned.csv')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATASET LOADED\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nShape: {df.shape}\")\n",
    "print(f\"Samples: {len(df)}\")\n",
    "print(f\"Features: {df.shape[1] - 2} (excluding 'filename' and 'label')\")\n",
    "print(f\"Genres: {df['label'].nunique()}\")\n",
    "print(f\"\\nGenre Distribution:\")\n",
    "print(df['label'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903be97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and labels\n",
    "features = [col for col in df.columns if col not in ['filename', 'label']]\n",
    "X = df[features].values\n",
    "y = df['label'].values\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "print(f\"\\n✓ Data prepared:\")\n",
    "print(f\"  - Feature matrix shape: {X.shape}\")\n",
    "print(f\"  - Label array shape: {y_encoded.shape}\")\n",
    "print(f\"  - Genre mapping: {dict(enumerate(label_encoder.classes_))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60986d2",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d1f6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FEATURE SCALING (StandardScaler)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n✓ Features standardized\")\n",
    "print(f\"  - Mean: {X_scaled.mean():.6f}\")\n",
    "print(f\"  - Std: {X_scaled.std():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78ab388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=CONFIG['N_PCA_COMPONENTS'], random_state=CONFIG['RANDOM_STATE'])\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "variance_explained = pca.explained_variance_ratio_.sum()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DIMENSIONALITY REDUCTION (PCA)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n✓ PCA applied\")\n",
    "print(f\"  - Original dimensions: {X_scaled.shape[1]}\")\n",
    "print(f\"  - Reduced dimensions: {X_pca.shape[1]}\")\n",
    "print(f\"  - Variance explained: {variance_explained*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba856f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize explained variance\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), \n",
    "         pca.explained_variance_ratio_, 'bo-', linewidth=2)\n",
    "plt.xlabel('Principal Component', fontweight='bold')\n",
    "plt.ylabel('Variance Explained', fontweight='bold')\n",
    "plt.title('PCA - Explained Variance by Component', fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), \n",
    "         np.cumsum(pca.explained_variance_ratio_), 'ro-', linewidth=2)\n",
    "plt.axhline(y=variance_explained, color='g', linestyle='--', label=f'{variance_explained*100:.2f}%')\n",
    "plt.xlabel('Number of Components', fontweight='bold')\n",
    "plt.ylabel('Cumulative Variance Explained', fontweight='bold')\n",
    "plt.title('PCA - Cumulative Variance Explained', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/pca_variance.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n✓ PCA variance plot saved: results/pca_variance.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25db88a4",
   "metadata": {},
   "source": [
    "## 5. Clustering Algorithms Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b77f44c",
   "metadata": {},
   "source": [
    "### 5.1 Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4da836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cluster_accuracy(y_true, y_pred):\n",
    "    \"\"\"Calculate clustering accuracy using Hungarian algorithm.\"\"\"\n",
    "    # Create confusion matrix\n",
    "    cm = np.zeros((len(np.unique(y_true)), len(np.unique(y_pred))), dtype=int)\n",
    "    for i, j in zip(y_true, y_pred):\n",
    "        cm[i, j] += 1\n",
    "    \n",
    "    # Apply Hungarian algorithm\n",
    "    row_ind, col_ind = linear_sum_assignment(-cm)\n",
    "    accuracy = cm[row_ind, col_ind].sum() / len(y_true)\n",
    "    \n",
    "    return accuracy * 100  # Return as percentage\n",
    "\n",
    "def evaluate_clustering(X, y_true, labels, noise_points=0):\n",
    "    \"\"\"Evaluate clustering performance with multiple metrics.\"\"\"\n",
    "    # Filter out noise points (label = -1)\n",
    "    valid_mask = labels != -1\n",
    "    X_valid = X[valid_mask]\n",
    "    y_true_valid = y_true[valid_mask]\n",
    "    labels_valid = labels[valid_mask]\n",
    "    \n",
    "    if len(np.unique(labels_valid)) < 2:\n",
    "        return None\n",
    "    \n",
    "    results = {\n",
    "        'silhouette': silhouette_score(X_valid, labels_valid),\n",
    "        'davies_bouldin': davies_bouldin_score(X_valid, labels_valid),\n",
    "        'calinski_harabasz': calinski_harabasz_score(X_valid, labels_valid),\n",
    "        'nmi': normalized_mutual_info_score(y_true_valid, labels_valid),\n",
    "        'ari': adjusted_rand_score(y_true_valid, labels_valid),\n",
    "        'v_measure': v_measure_score(y_true_valid, labels_valid),\n",
    "        'cluster_accuracy': calculate_cluster_accuracy(y_true_valid, labels_valid),\n",
    "        'valid_samples': len(labels_valid),\n",
    "        'noise_points': noise_points\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1fa1cc",
   "metadata": {},
   "source": [
    "### 5.2 K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e019007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_clustering(X_train, y_train, n_clusters):\n",
    "    \"\"\"Perform K-Means clustering.\"\"\"\n",
    "    print(f\"\\n→ Running K-Means (n_clusters={n_clusters})...\")\n",
    "    start_time = time()\n",
    "    \n",
    "    kmeans = KMeans(\n",
    "        n_clusters=n_clusters,\n",
    "        random_state=CONFIG['RANDOM_STATE'],\n",
    "        n_init=10,\n",
    "        max_iter=300\n",
    "    )\n",
    "    labels = kmeans.fit_predict(X_train)\n",
    "    \n",
    "    elapsed = time() - start_time\n",
    "    print(f\"  ✓ Completed in {elapsed:.2f}s\")\n",
    "    \n",
    "    return labels, kmeans\n",
    "\n",
    "print(\"✓ K-Means function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220cead2",
   "metadata": {},
   "source": [
    "### 5.3 MiniBatch K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be863b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_kmeans_clustering(X_train, y_train, n_clusters):\n",
    "    \"\"\"Perform MiniBatch K-Means clustering.\"\"\"\n",
    "    print(f\"\\n→ Running MiniBatch K-Means (n_clusters={n_clusters})...\")\n",
    "    start_time = time()\n",
    "    \n",
    "    mbkmeans = MiniBatchKMeans(\n",
    "        n_clusters=n_clusters,\n",
    "        random_state=CONFIG['RANDOM_STATE'],\n",
    "        batch_size=100,\n",
    "        n_init=10\n",
    "    )\n",
    "    labels = mbkmeans.fit_predict(X_train)\n",
    "    \n",
    "    elapsed = time() - start_time\n",
    "    print(f\"  ✓ Completed in {elapsed:.2f}s\")\n",
    "    \n",
    "    return labels, mbkmeans\n",
    "\n",
    "print(\"✓ MiniBatch K-Means function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bb1c25",
   "metadata": {},
   "source": [
    "### 5.4 Spectral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53442952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_clustering(X_train, y_train, n_clusters):\n",
    "    \"\"\"Perform Spectral clustering.\"\"\"\n",
    "    print(f\"\\n→ Running Spectral Clustering (n_clusters={n_clusters})...\")\n",
    "    start_time = time()\n",
    "    \n",
    "    spectral = SpectralClustering(\n",
    "        n_clusters=n_clusters,\n",
    "        random_state=CONFIG['RANDOM_STATE'],\n",
    "        affinity='nearest_neighbors',\n",
    "        n_neighbors=10\n",
    "    )\n",
    "    labels = spectral.fit_predict(X_train)\n",
    "    \n",
    "    elapsed = time() - start_time\n",
    "    print(f\"  ✓ Completed in {elapsed:.2f}s\")\n",
    "    \n",
    "    return labels, spectral\n",
    "\n",
    "print(\"✓ Spectral Clustering function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39aae7a1",
   "metadata": {},
   "source": [
    "### 5.5 DBSCAN Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8979dab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbscan_clustering(X_train, y_train):\n",
    "    \"\"\"Perform DBSCAN clustering.\"\"\"\n",
    "    print(f\"\\n→ Running DBSCAN (eps={CONFIG['DBSCAN_EPS']}, min_samples={CONFIG['DBSCAN_MIN_SAMPLES']})...\")\n",
    "    start_time = time()\n",
    "    \n",
    "    dbscan = DBSCAN(\n",
    "        eps=CONFIG['DBSCAN_EPS'],\n",
    "        min_samples=CONFIG['DBSCAN_MIN_SAMPLES']\n",
    "    )\n",
    "    labels = dbscan.fit_predict(X_train)\n",
    "    \n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise = list(labels).count(-1)\n",
    "    \n",
    "    elapsed = time() - start_time\n",
    "    print(f\"  ✓ Completed in {elapsed:.2f}s\")\n",
    "    print(f\"  - Clusters found: {n_clusters}\")\n",
    "    print(f\"  - Noise points: {n_noise}\")\n",
    "    \n",
    "    return labels, dbscan\n",
    "\n",
    "print(\"✓ DBSCAN function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698d27f7",
   "metadata": {},
   "source": [
    "### 5.6 Gaussian Mixture Model (GMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7102d2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmm_clustering(X_train, y_train, n_clusters):\n",
    "    \"\"\"Perform GMM clustering.\"\"\"\n",
    "    print(f\"\\n→ Running GMM (n_components={n_clusters})...\")\n",
    "    start_time = time()\n",
    "    \n",
    "    gmm = GaussianMixture(\n",
    "        n_components=n_clusters,\n",
    "        random_state=CONFIG['RANDOM_STATE'],\n",
    "        covariance_type='full',\n",
    "        max_iter=100\n",
    "    )\n",
    "    labels = gmm.fit_predict(X_train)\n",
    "    \n",
    "    elapsed = time() - start_time\n",
    "    print(f\"  ✓ Completed in {elapsed:.2f}s\")\n",
    "    \n",
    "    return labels, gmm\n",
    "\n",
    "print(\"✓ GMM function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ff6f35",
   "metadata": {},
   "source": [
    "## 6. Run Clustering Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1abcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage for results\n",
    "all_results = []\n",
    "cluster_visualizations = {}\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RUNNING CLUSTERING EXPERIMENTS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nAlgorithms: K-Means, MiniBatch K-Means, Spectral Clustering, GMM\")\n",
    "print(f\"Split ratios: {CONFIG['SPLIT_RATIOS']}\")\n",
    "print(f\"Total experiments: {len(CONFIG['SPLIT_RATIOS']) * 4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7759ce35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments for each split ratio\n",
    "for split_idx, (train_pct, test_pct) in enumerate(CONFIG['SPLIT_RATIOS'], 1):\n",
    "    print(f\"\\n\" + \"=\" * 80)\n",
    "    print(f\"EXPERIMENT SET {split_idx}: {train_pct}-{test_pct} Train-Test Split\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Split data\n",
    "    test_size = test_pct / 100\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_pca, y_encoded, \n",
    "        test_size=test_size,\n",
    "        random_state=CONFIG['RANDOM_STATE'],\n",
    "        stratify=y_encoded\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTrain samples: {len(X_train)}\")\n",
    "    print(f\"Test samples: {len(X_test)}\")\n",
    "    \n",
    "    split_name = f\"{train_pct}-{test_pct}\"\n",
    "    \n",
    "    # 1. K-Means\n",
    "    labels_km, model_km = kmeans_clustering(X_train, y_train, CONFIG['N_CLUSTERS'])\n",
    "    results_km = evaluate_clustering(X_train, y_train, labels_km)\n",
    "    if results_km:\n",
    "        results_km['algorithm'] = 'K-Means'\n",
    "        results_km['split'] = split_name\n",
    "        all_results.append(results_km)\n",
    "        cluster_visualizations[f'kmeans_{split_name}'] = (X_train, labels_km, y_train)\n",
    "    \n",
    "    # 2. MiniBatch K-Means\n",
    "    labels_mbkm, model_mbkm = minibatch_kmeans_clustering(X_train, y_train, CONFIG['N_CLUSTERS'])\n",
    "    results_mbkm = evaluate_clustering(X_train, y_train, labels_mbkm)\n",
    "    if results_mbkm:\n",
    "        results_mbkm['algorithm'] = 'MiniBatch K-Means'\n",
    "        results_mbkm['split'] = split_name\n",
    "        all_results.append(results_mbkm)\n",
    "    \n",
    "    # 3. Spectral Clustering\n",
    "    labels_sc, model_sc = spectral_clustering(X_train, y_train, CONFIG['N_CLUSTERS'])\n",
    "    results_sc = evaluate_clustering(X_train, y_train, labels_sc)\n",
    "    if results_sc:\n",
    "        results_sc['algorithm'] = 'Spectral Clustering'\n",
    "        results_sc['split'] = split_name\n",
    "        all_results.append(results_sc)\n",
    "        cluster_visualizations[f'spectral_{split_name}'] = (X_train, labels_sc, y_train)\n",
    "    \n",
    "    # 4. GMM\n",
    "    labels_gmm, model_gmm = gmm_clustering(X_train, y_train, CONFIG['N_CLUSTERS'])\n",
    "    results_gmm = evaluate_clustering(X_train, y_train, labels_gmm)\n",
    "    if results_gmm:\n",
    "        results_gmm['algorithm'] = 'GMM'\n",
    "        results_gmm['split'] = split_name\n",
    "        all_results.append(results_gmm)\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ ALL EXPERIMENTS COMPLETED\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711c6dee",
   "metadata": {},
   "source": [
    "## 7. Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c77964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Reorder columns\n",
    "column_order = ['algorithm', 'split', 'silhouette', 'davies_bouldin', \n",
    "                'calinski_harabasz', 'nmi', 'ari', 'v_measure', \n",
    "                'cluster_accuracy', 'valid_samples', 'noise_points']\n",
    "results_df = results_df[column_order]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CLUSTERING RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nAll Results:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv('results/clustering_results.csv', index=False)\n",
    "print(\"\\n✓ Results saved: results/clustering_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccbc6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average performance by algorithm\n",
    "summary = results_df.groupby('algorithm').agg({\n",
    "    'silhouette': 'mean',\n",
    "    'davies_bouldin': 'mean',\n",
    "    'calinski_harabasz': 'mean',\n",
    "    'nmi': 'mean',\n",
    "    'ari': 'mean',\n",
    "    'v_measure': 'mean',\n",
    "    'cluster_accuracy': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"AVERAGE PERFORMANCE BY ALGORITHM\")\n",
    "print(\"=\" * 80)\n",
    "print(summary)\n",
    "\n",
    "# Save summary\n",
    "summary.to_csv('results/summary_table.csv')\n",
    "print(\"\\n✓ Summary saved: results/summary_table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37e9dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best algorithm for each metric\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BEST ALGORITHM BY METRIC\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "metrics = ['silhouette', 'nmi', 'ari', 'v_measure', 'cluster_accuracy']\n",
    "for metric in metrics:\n",
    "    best_algo = summary[metric].idxmax()\n",
    "    best_value = summary[metric].max()\n",
    "    print(f\"{metric:25s}: {best_algo:25s} ({best_value:.4f})\")\n",
    "\n",
    "# Davies-Bouldin (lower is better)\n",
    "best_algo = summary['davies_bouldin'].idxmin()\n",
    "best_value = summary['davies_bouldin'].min()\n",
    "print(f\"{'davies_bouldin':25s}: {best_algo:25s} ({best_value:.4f}) [lower is better]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dba0f8",
   "metadata": {},
   "source": [
    "## 8. Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaa1301",
   "metadata": {},
   "source": [
    "### 8.1 Metrics Comparison Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ea3033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metrics comparison heatmap\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Prepare data for heatmap\n",
    "metrics_to_plot = ['silhouette', 'nmi', 'ari', 'v_measure', 'cluster_accuracy']\n",
    "heatmap_data = summary[metrics_to_plot].T\n",
    "\n",
    "sns.heatmap(heatmap_data, annot=True, fmt='.4f', cmap='YlGnBu', \n",
    "           linewidths=0.5, cbar_kws={'label': 'Score'})\n",
    "plt.title('Clustering Performance Metrics Comparison (Average Across All Splits)', \n",
    "         fontsize=14, fontweight='bold', pad=20)\n",
    "plt.xlabel('Algorithm', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Metric', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/metrics_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Metrics comparison saved: results/metrics_comparison.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc67773",
   "metadata": {},
   "source": [
    "### 8.2 Performance by Split Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b47a366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot performance across different split ratios\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "metrics_to_plot = ['silhouette', 'davies_bouldin', 'nmi', 'ari', 'v_measure', 'cluster_accuracy']\n",
    "metric_labels = ['Silhouette Score', 'Davies-Bouldin Index', 'NMI', 'ARI', 'V-Measure', 'Cluster Accuracy (%)']\n",
    "\n",
    "for idx, (metric, label) in enumerate(zip(metrics_to_plot, metric_labels)):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    for algo in results_df['algorithm'].unique():\n",
    "        algo_data = results_df[results_df['algorithm'] == algo]\n",
    "        ax.plot(algo_data['split'], algo_data[metric], marker='o', label=algo, linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Train-Test Split', fontweight='bold')\n",
    "    ax.set_ylabel(label, fontweight='bold')\n",
    "    ax.set_title(f'{label} by Split Ratio', fontweight='bold')\n",
    "    ax.legend(loc='best', fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/performance_by_split.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Performance by split saved: results/performance_by_split.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2559690f",
   "metadata": {},
   "source": [
    "### 8.3 Radar Chart - Algorithm Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b546e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create radar chart for algorithm comparison\n",
    "from math import pi\n",
    "\n",
    "# Normalize metrics to 0-1 scale\n",
    "normalized_summary = summary[metrics_to_plot].copy()\n",
    "for col in normalized_summary.columns:\n",
    "    min_val = normalized_summary[col].min()\n",
    "    max_val = normalized_summary[col].max()\n",
    "    if max_val > min_val:\n",
    "        normalized_summary[col] = (normalized_summary[col] - min_val) / (max_val - min_val)\n",
    "\n",
    "# Setup radar chart\n",
    "categories = metrics_to_plot\n",
    "N = len(categories)\n",
    "angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "angles += angles[:1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "for algo in normalized_summary.index:\n",
    "    values = normalized_summary.loc[algo].values.tolist()\n",
    "    values += values[:1]\n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=algo)\n",
    "    ax.fill(angles, values, alpha=0.15)\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories, fontsize=11)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], fontsize=9)\n",
    "ax.grid(True)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=11)\n",
    "plt.title('Algorithm Performance Comparison (Normalized Metrics)', \n",
    "         fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/radar_chart.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Radar chart saved: results/radar_chart.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac7c8a2",
   "metadata": {},
   "source": [
    "### 8.4 Cluster Visualizations (2D PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d81013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clusters for selected algorithms\n",
    "def visualize_clusters_2d(X, labels, y_true, title, filename):\n",
    "    \"\"\"Visualize clusters in 2D using first 2 PCA components.\"\"\"\n",
    "    pca_2d = PCA(n_components=2, random_state=CONFIG['RANDOM_STATE'])\n",
    "    X_2d = pca_2d.fit_transform(X)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Plot predicted clusters\n",
    "    scatter1 = axes[0].scatter(X_2d[:, 0], X_2d[:, 1], c=labels, \n",
    "                              cmap='tab10', alpha=0.6, edgecolors='k', s=50)\n",
    "    axes[0].set_xlabel('First Principal Component', fontweight='bold', fontsize=11)\n",
    "    axes[0].set_ylabel('Second Principal Component', fontweight='bold', fontsize=11)\n",
    "    axes[0].set_title(f'{title}\\nPredicted Clusters', fontweight='bold', fontsize=12)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter1, ax=axes[0], label='Cluster')\n",
    "    \n",
    "    # Plot true labels\n",
    "    scatter2 = axes[1].scatter(X_2d[:, 0], X_2d[:, 1], c=y_true, \n",
    "                              cmap='tab10', alpha=0.6, edgecolors='k', s=50)\n",
    "    axes[1].set_xlabel('First Principal Component', fontweight='bold', fontsize=11)\n",
    "    axes[1].set_ylabel('Second Principal Component', fontweight='bold', fontsize=11)\n",
    "    axes[1].set_title(f'{title}\\nTrue Labels', fontweight='bold', fontsize=12)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter2, ax=axes[1], label='Genre')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'results/{filename}', dpi=300, bbox_inches='tight')\n",
    "    print(f\"✓ Cluster visualization saved: results/{filename}\")\n",
    "    plt.show()\n",
    "\n",
    "# Visualize best performing algorithms\n",
    "print(\"\\nGenerating cluster visualizations...\\n\")\n",
    "for key, (X, labels, y_true) in list(cluster_visualizations.items())[:2]:\n",
    "    algo_name = key.split('_')[0].title()\n",
    "    split = key.split('_')[1]\n",
    "    visualize_clusters_2d(X, labels, y_true, \n",
    "                         f'{algo_name} Clustering ({split} split)',\n",
    "                         f'cluster_viz_{key}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f5dcfd",
   "metadata": {},
   "source": [
    "## 9. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5f8693",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"CLUSTERING IMPLEMENTATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n✓ Implementation Complete!\")\n",
    "\n",
    "print(\"\\n1. Algorithms Implemented:\")\n",
    "print(\"   - K-Means Clustering\")\n",
    "print(\"   - MiniBatch K-Means\")\n",
    "print(\"   - Spectral Clustering\")\n",
    "print(\"   - Gaussian Mixture Model (GMM)\")\n",
    "\n",
    "print(\"\\n2. Evaluation Metrics:\")\n",
    "print(\"   - Silhouette Score\")\n",
    "print(\"   - Davies-Bouldin Index\")\n",
    "print(\"   - Calinski-Harabasz Index\")\n",
    "print(\"   - Normalized Mutual Information (NMI)\")\n",
    "print(\"   - Adjusted Rand Index (ARI)\")\n",
    "print(\"   - V-Measure\")\n",
    "print(\"   - Cluster Accuracy (Hungarian Algorithm)\")\n",
    "\n",
    "print(\"\\n3. Experiments:\")\n",
    "print(f\"   - Total experiments run: {len(all_results)}\")\n",
    "print(f\"   - Split ratios tested: {CONFIG['SPLIT_RATIOS']}\")\n",
    "\n",
    "print(\"\\n4. Best Performing Algorithm:\")\n",
    "best_overall = summary['cluster_accuracy'].idxmax()\n",
    "best_acc = summary.loc[best_overall, 'cluster_accuracy']\n",
    "print(f\"   - Algorithm: {best_overall}\")\n",
    "print(f\"   - Average Accuracy: {best_acc:.2f}%\")\n",
    "print(f\"   - Improvement over random: {(best_acc/10 - 1)*100:.2f}% (baseline: 10%)\")\n",
    "\n",
    "print(\"\\n5. Generated Files:\")\n",
    "print(\"   - results/clustering_results.csv\")\n",
    "print(\"   - results/summary_table.csv\")\n",
    "print(\"   - results/pca_variance.png\")\n",
    "print(\"   - results/metrics_comparison.png\")\n",
    "print(\"   - results/performance_by_split.png\")\n",
    "print(\"   - results/radar_chart.png\")\n",
    "print(\"   - results/cluster_viz_*.png\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PROJECT COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
