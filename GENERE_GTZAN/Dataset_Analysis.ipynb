{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf7ee8ef",
   "metadata": {},
   "source": [
    "# Music Genre Discovery - Dataset Analysis\n",
    "\n",
    "**Project:** Unsupervised Music Genre Discovery Using Audio Feature Learning  \n",
    "**Author:** Anirudh Sharma  \n",
    "**Date:** November 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook performs comprehensive data analysis on the GTZAN music genre dataset, including:\n",
    "- Data adequacy and quality checks\n",
    "- Class balance analysis\n",
    "- Descriptive statistical analysis\n",
    "- Outlier detection and removal\n",
    "- Missing value handling\n",
    "- Distribution pattern analysis\n",
    "- Percentile and quartile analysis\n",
    "- Trimmed statistics\n",
    "- Correlation analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de55a266",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab11570c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import trim_mean\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Ignore warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "print(\"✓ Libraries imported successfully!\")\n",
    "print(f\"  - pandas version: {pd.__version__}\")\n",
    "print(f\"  - numpy version: {np.__version__}\")\n",
    "print(f\"  - matplotlib version: {plt.matplotlib.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804e5bb4",
   "metadata": {},
   "source": [
    "## 2. Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56f8e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('gtzan/features_30_sec.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(\"=\" * 80)\n",
    "print(\"DATASET LOADED SUCCESSFULLY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nDataset Shape: {df.shape}\")\n",
    "print(f\"  - Total Samples: {len(df)}\")\n",
    "print(f\"  - Total Columns: {df.shape[1]}\")\n",
    "print(f\"  - Total Features: {df.shape[1] - 2}  (excluding 'filename' and 'label')\")\n",
    "print(f\"\\nGenres: {df['label'].nunique()}\")\n",
    "print(f\"Genre List: {sorted(df['label'].unique())}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"First 5 Rows of Dataset:\")\n",
    "print(\"=\" * 80)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cde4017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset info\n",
    "print(\"Dataset Information:\")\n",
    "print(\"=\" * 80)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45aac3c7",
   "metadata": {},
   "source": [
    "## 3. Data Adequacy Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea62016e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare feature columns\n",
    "label_col = 'label'\n",
    "features = [col for col in df.columns if col not in ['filename', 'label']]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA ADEQUACY CHECK\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Basic statistics\n",
    "total_samples = len(df)\n",
    "total_features = len(features)\n",
    "genres = df[label_col].unique()\n",
    "\n",
    "print(f\"\\n1. Dataset Size:\")\n",
    "print(f\"   - Total samples: {total_samples}\")\n",
    "print(f\"   - Total features: {total_features}\")\n",
    "print(f\"   - Number of genres: {len(genres)}\")\n",
    "\n",
    "# Sample to feature ratio\n",
    "ratio = total_samples / total_features\n",
    "print(f\"\\n2. Sample-to-Feature Ratio: {ratio:.2f}\")\n",
    "if ratio > 10:\n",
    "    print(\"   ✓ ADEQUATE: Good ratio for machine learning (>10)\")\n",
    "else:\n",
    "    print(\"   ⚠ WARNING: Low ratio, consider dimensionality reduction\")\n",
    "\n",
    "# Minimum samples per genre\n",
    "min_samples_per_genre = total_samples / len(genres)\n",
    "print(f\"\\n3. Average samples per genre: {min_samples_per_genre:.0f}\")\n",
    "if min_samples_per_genre >= 50:\n",
    "    print(\"   ✓ ADEQUATE: Sufficient samples per genre (≥50)\")\n",
    "else:\n",
    "    print(\"   ⚠ WARNING: Low samples per genre\")\n",
    "\n",
    "# Adequacy summary\n",
    "adequacy_report = {\n",
    "    'total_samples': total_samples,\n",
    "    'total_features': total_features,\n",
    "    'num_genres': len(genres),\n",
    "    'sample_feature_ratio': ratio,\n",
    "    'avg_samples_per_genre': min_samples_per_genre,\n",
    "    'is_adequate': ratio > 10 and min_samples_per_genre >= 50\n",
    "}\n",
    "\n",
    "print(f\"\\n4. Overall Assessment: {'✓ ADEQUATE' if adequacy_report['is_adequate'] else '⚠ NEEDS ATTENTION'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1425d6a9",
   "metadata": {},
   "source": [
    "## 4. Class Balance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe42973",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"CLASS BALANCE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Count samples per genre\n",
    "class_counts = df[label_col].value_counts().sort_index()\n",
    "class_percentages = (class_counts / len(df) * 100).round(2)\n",
    "\n",
    "# Create distribution dataframe\n",
    "distribution = pd.DataFrame({\n",
    "    'Genre': class_counts.index,\n",
    "    'Count': class_counts.values,\n",
    "    'Percentage': class_percentages.values\n",
    "})\n",
    "\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(distribution.to_string(index=False))\n",
    "\n",
    "# Check balance\n",
    "max_count = class_counts.max()\n",
    "min_count = class_counts.min()\n",
    "imbalance_ratio = max_count / min_count\n",
    "\n",
    "print(f\"\\nBalance Metrics:\")\n",
    "print(f\"  - Maximum samples: {max_count}\")\n",
    "print(f\"  - Minimum samples: {min_count}\")\n",
    "print(f\"  - Imbalance ratio: {imbalance_ratio:.2f}\")\n",
    "\n",
    "if imbalance_ratio <= 1.5:\n",
    "    print(\"  ✓ BALANCED: Dataset is well balanced (ratio ≤ 1.5)\")\n",
    "elif imbalance_ratio <= 3:\n",
    "    print(\"  ⚠ MODERATELY IMBALANCED: Consider balancing techniques\")\n",
    "else:\n",
    "    print(\"  ✗ HIGHLY IMBALANCED: Balancing required\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26e9f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Bar chart\n",
    "axes[0].bar(distribution['Genre'], distribution['Count'], color='skyblue', edgecolor='navy')\n",
    "axes[0].set_xlabel('Genre', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Number of Samples', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Class Distribution - Counts', fontsize=14, fontweight='bold')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Pie chart\n",
    "colors = plt.cm.Set3(range(len(distribution)))\n",
    "axes[1].pie(distribution['Percentage'], labels=distribution['Genre'], \n",
    "            autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "axes[1].set_title('Class Distribution - Percentages', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/class_balance.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n✓ Visualization saved: results/class_balance.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a69c15",
   "metadata": {},
   "source": [
    "## 5. Descriptive Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022591eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"DESCRIPTIVE STATISTICAL ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Basic statistics\n",
    "desc_stats = df[features].describe()\n",
    "\n",
    "# Additional statistics\n",
    "additional_stats = pd.DataFrame({\n",
    "    'variance': df[features].var(),\n",
    "    'skewness': df[features].skew(),\n",
    "    'kurtosis': df[features].kurtosis(),\n",
    "    'range': df[features].max() - df[features].min(),\n",
    "    'iqr': df[features].quantile(0.75) - df[features].quantile(0.25)\n",
    "}).T\n",
    "\n",
    "# Combine all statistics\n",
    "full_stats = pd.concat([desc_stats, additional_stats])\n",
    "\n",
    "print(\"\\nKey Statistics (first 5 features):\")\n",
    "print(full_stats.iloc[:, :5].to_string())\n",
    "print(\"\\n... (showing first 5 features only)\")\n",
    "\n",
    "# Save complete statistics\n",
    "full_stats.to_csv('results/descriptive_statistics.csv')\n",
    "print(\"\\n✓ Full statistics saved: results/descriptive_statistics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324a29f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary of key statistics\n",
    "print(\"\\nSummary of Statistical Properties:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Identify features with high skewness\n",
    "high_skew = df[features].skew().abs().sort_values(ascending=False).head(5)\n",
    "print(\"\\nTop 5 Features with Highest Skewness:\")\n",
    "for feat, skew_val in high_skew.items():\n",
    "    print(f\"  {feat}: {skew_val:.2f}\")\n",
    "\n",
    "# Identify features with high kurtosis\n",
    "high_kurt = df[features].kurtosis().abs().sort_values(ascending=False).head(5)\n",
    "print(\"\\nTop 5 Features with Highest Kurtosis:\")\n",
    "for feat, kurt_val in high_kurt.items():\n",
    "    print(f\"  {feat}: {kurt_val:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05300fd5",
   "metadata": {},
   "source": [
    "## 6. Missing Value Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef11b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"MISSING VALUE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check for missing values\n",
    "missing_count = df.isnull().sum()\n",
    "missing_percentage = (missing_count / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Feature': missing_count.index,\n",
    "    'Missing_Count': missing_count.values,\n",
    "    'Percentage': missing_percentage.values\n",
    "})\n",
    "\n",
    "missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values(\n",
    "    'Missing_Count', ascending=False)\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    print(f\"\\nFound {len(missing_df)} features with missing values:\")\n",
    "    print(missing_df.to_string(index=False))\n",
    "    \n",
    "    # Fill missing values with mean\n",
    "    print(\"\\n→ Filling missing values with column means...\")\n",
    "    for feature in missing_df['Feature']:\n",
    "        if feature != 'label':\n",
    "            mean_value = df[feature].mean()\n",
    "            df[feature].fillna(mean_value, inplace=True)\n",
    "    \n",
    "    print(\"✓ Missing values handled\")\n",
    "else:\n",
    "    print(\"\\n✓ No missing values found in the dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87658cf5",
   "metadata": {},
   "source": [
    "## 7. Outlier Detection and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea39ff46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_iqr(data, feature):\n",
    "    \"\"\"Detect outliers using the IQR method.\"\"\"\n",
    "    Q1 = data[feature].quantile(0.25)\n",
    "    Q3 = data[feature].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = data[(data[feature] < lower_bound) | \n",
    "                   (data[feature] > upper_bound)].index\n",
    "    \n",
    "    return outliers, Q1, Q3, IQR, lower_bound, upper_bound\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"OUTLIER DETECTION (IQR Method)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "outlier_info = {}\n",
    "\n",
    "for feature in features:\n",
    "    outliers, Q1, Q3, IQR, lower, upper = detect_outliers_iqr(df, feature)\n",
    "    outlier_info[feature] = {\n",
    "        'count': len(outliers),\n",
    "        'percentage': (len(outliers) / len(df)) * 100,\n",
    "        'Q1': Q1,\n",
    "        'Q3': Q3,\n",
    "        'IQR': IQR,\n",
    "        'lower_bound': lower,\n",
    "        'upper_bound': upper\n",
    "    }\n",
    "\n",
    "# Sort by outlier percentage\n",
    "sorted_features = sorted(outlier_info.items(), \n",
    "                        key=lambda x: x[1]['percentage'], \n",
    "                        reverse=True)\n",
    "\n",
    "print(\"\\nTop 10 features with most outliers:\")\n",
    "print(f\"{'Feature':<35} {'Count':<10} {'Percentage':<12}\")\n",
    "print(\"-\" * 60)\n",
    "for feat, info in sorted_features[:10]:\n",
    "    print(f\"{feat:<35} {info['count']:<10} {info['percentage']:<12.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa159f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boxplots for top features with outliers\n",
    "features_to_plot = [feat for feat, _ in sorted_features[:10]]\n",
    "\n",
    "fig, axes = plt.subplots(5, 2, figsize=(15, 20))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(features_to_plot):\n",
    "    ax = axes[idx]\n",
    "    df.boxplot(column=feature, ax=ax)\n",
    "    ax.set_title(f'{feature}\\n({outlier_info[feature][\"count\"]} outliers, '\n",
    "                f'{outlier_info[feature][\"percentage\"]:.1f}%)',\n",
    "                fontsize=10, fontweight='bold')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/outlier_boxplots.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n✓ Boxplots saved: results/outlier_boxplots.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472d548e",
   "metadata": {},
   "source": [
    "## 8. Distribution Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac98d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"DISTRIBUTION PATTERN ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "distribution_info = {}\n",
    "\n",
    "for feature in features:\n",
    "    # Normality test (Shapiro-Wilk)\n",
    "    try:\n",
    "        sample_data = df[feature].dropna()\n",
    "        if len(sample_data) > 5000:\n",
    "            sample_data = sample_data.sample(5000, random_state=42)\n",
    "        \n",
    "        statistic, p_value = stats.shapiro(sample_data)\n",
    "        is_normal = p_value > 0.05\n",
    "    except:\n",
    "        is_normal = False\n",
    "        p_value = 0\n",
    "    \n",
    "    distribution_info[feature] = {\n",
    "        'mean': df[feature].mean(),\n",
    "        'median': df[feature].median(),\n",
    "        'std': df[feature].std(),\n",
    "        'skewness': df[feature].skew(),\n",
    "        'kurtosis': df[feature].kurtosis(),\n",
    "        'is_normal': is_normal,\n",
    "        'normality_p_value': p_value\n",
    "    }\n",
    "\n",
    "# Count normal distributions\n",
    "normal_count = sum(1 for info in distribution_info.values() if info['is_normal'])\n",
    "\n",
    "print(f\"\\nDistribution Summary:\")\n",
    "print(f\"  - Total features: {len(features)}\")\n",
    "print(f\"  - Normal distributions: {normal_count}\")\n",
    "print(f\"  - Non-normal distributions: {len(features) - normal_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f28e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution for selected features\n",
    "sample_features = features[:6]  # First 6 features\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(sample_features):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Histogram with KDE\n",
    "    df[feature].hist(bins=50, ax=ax, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax2 = ax.twinx()\n",
    "    df[feature].plot(kind='kde', ax=ax2, color='red', linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Value', fontweight='bold')\n",
    "    ax.set_ylabel('Frequency', fontweight='bold')\n",
    "    ax2.set_ylabel('Density', fontweight='bold', color='red')\n",
    "    \n",
    "    title = f'{feature}\\n'\n",
    "    title += f\"Skew: {distribution_info[feature]['skewness']:.2f}, \"\n",
    "    title += f\"Kurt: {distribution_info[feature]['kurtosis']:.2f}\"\n",
    "    ax.set_title(title, fontsize=10, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/distribution_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n✓ Distribution plots saved: results/distribution_analysis.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8848d9d",
   "metadata": {},
   "source": [
    "## 9. Percentile and Quartile Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677aed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PERCENTILE AND QUARTILE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "stats_dict = {}\n",
    "\n",
    "for feature in features:\n",
    "    stats_dict[feature] = {\n",
    "        'Mean': df[feature].mean(),\n",
    "        'Median (M)': df[feature].median(),\n",
    "        'Q1 (25th percentile)': df[feature].quantile(0.25),\n",
    "        'Q3 (75th percentile)': df[feature].quantile(0.75),\n",
    "        'P75 (75th percentile)': df[feature].quantile(0.75),\n",
    "        'P25 (25th percentile)': df[feature].quantile(0.25),\n",
    "        'IQR': df[feature].quantile(0.75) - df[feature].quantile(0.25)\n",
    "    }\n",
    "\n",
    "stats_df = pd.DataFrame(stats_dict).T\n",
    "\n",
    "print(\"\\nPercentile Statistics (first 5 features):\")\n",
    "print(stats_df.head().to_string())\n",
    "print(\"\\n... (showing first 5 features)\")\n",
    "\n",
    "stats_df.to_csv('results/percentile_quartile_stats.csv')\n",
    "print(\"\\n✓ Full statistics saved: results/percentile_quartile_stats.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa96a054",
   "metadata": {},
   "source": [
    "## 10. Trimmed Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194c3edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"TRIMMED STATISTICS (trim fraction: 0.1)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "trim_fraction = 0.1\n",
    "trimmed_stats = {}\n",
    "\n",
    "for feature in features:\n",
    "    data = df[feature].dropna().values\n",
    "    \n",
    "    # Trimmed mean\n",
    "    trimmed_mean_val = trim_mean(data, trim_fraction)\n",
    "    \n",
    "    # Trimmed median\n",
    "    lower_p = trim_fraction * 100\n",
    "    upper_p = 100 - (trim_fraction * 100)\n",
    "    trimmed_data = data[(data >= np.percentile(data, lower_p)) & \n",
    "                       (data <= np.percentile(data, upper_p))]\n",
    "    trimmed_median_val = np.median(trimmed_data)\n",
    "    \n",
    "    # Trimmed standard deviation\n",
    "    trimmed_std = np.std(trimmed_data)\n",
    "    \n",
    "    trimmed_stats[feature] = {\n",
    "        'Original_Mean': np.mean(data),\n",
    "        'Trimmed_Mean': trimmed_mean_val,\n",
    "        'Original_Median': np.median(data),\n",
    "        'Trimmed_Median': trimmed_median_val,\n",
    "        'Original_Std': np.std(data),\n",
    "        'Trimmed_Std': trimmed_std\n",
    "    }\n",
    "\n",
    "trimmed_df = pd.DataFrame(trimmed_stats).T\n",
    "\n",
    "print(f\"\\nTrimmed Statistics (first 5 features):\")\n",
    "print(trimmed_df.head().to_string())\n",
    "print(\"\\n... (showing first 5 features)\")\n",
    "\n",
    "trimmed_df.to_csv('results/trimmed_statistics.csv')\n",
    "print(\"\\n✓ Full statistics saved: results/trimmed_statistics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0de3949",
   "metadata": {},
   "source": [
    "## 11. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd91ed0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"CORRELATION ANALYSIS (Pearson method)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = df[features].corr(method='pearson')\n",
    "\n",
    "# Find top correlations\n",
    "corr_pairs = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        corr_pairs.append({\n",
    "            'Feature_1': corr_matrix.columns[i],\n",
    "            'Feature_2': corr_matrix.columns[j],\n",
    "            'Correlation': corr_matrix.iloc[i, j]\n",
    "        })\n",
    "\n",
    "corr_df = pd.DataFrame(corr_pairs)\n",
    "corr_df['Abs_Correlation'] = corr_df['Correlation'].abs()\n",
    "corr_df = corr_df.sort_values('Abs_Correlation', ascending=False)\n",
    "\n",
    "print(f\"\\nTop 20 Feature Correlations:\")\n",
    "print(corr_df.head(20)[['Feature_1', 'Feature_2', 'Correlation']].to_string(index=False))\n",
    "\n",
    "# Save correlation matrix\n",
    "corr_matrix.to_csv('results/correlation_matrix.csv')\n",
    "print(f\"\\n✓ Full correlation matrix saved: results/correlation_matrix.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4445e9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation heatmap\n",
    "plt.figure(figsize=(20, 16))\n",
    "\n",
    "# Select subset of features for better visualization\n",
    "n_features = min(30, len(features))\n",
    "selected_features = features[:n_features]\n",
    "corr_subset = df[selected_features].corr(method='pearson')\n",
    "\n",
    "sns.heatmap(corr_subset, annot=False, cmap='coolwarm', center=0,\n",
    "           square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title(f'Feature Correlation Heatmap (Pearson)\\n(First {n_features} features)',\n",
    "         fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Correlation heatmap saved: results/correlation_heatmap.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2d4e23",
   "metadata": {},
   "source": [
    "## 12. Outlier Removal and Clean Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361c209a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"OUTLIER REMOVAL (IQR method)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "initial_size = len(df)\n",
    "\n",
    "# IQR method\n",
    "outlier_mask = pd.Series([False] * len(df))\n",
    "\n",
    "for feature in features:\n",
    "    Q1 = df[feature].quantile(0.25)\n",
    "    Q3 = df[feature].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    \n",
    "    feature_outliers = (df[feature] < lower) | (df[feature] > upper)\n",
    "    outlier_mask = outlier_mask | feature_outliers\n",
    "\n",
    "cleaned_df = df[~outlier_mask].copy()\n",
    "\n",
    "final_size = len(cleaned_df)\n",
    "removed = initial_size - final_size\n",
    "\n",
    "print(f\"\\nOutlier Removal Summary:\")\n",
    "print(f\"  - Initial samples: {initial_size}\")\n",
    "print(f\"  - Samples removed: {removed} ({(removed/initial_size)*100:.2f}%)\")\n",
    "print(f\"  - Final samples: {final_size}\")\n",
    "print(f\"  ✓ Cleaned dataset created\")\n",
    "\n",
    "# Save cleaned dataset\n",
    "cleaned_df.to_csv('gtzan/features_30_sec_cleaned.csv', index=False)\n",
    "print(\"\\n✓ Cleaned dataset saved: gtzan/features_30_sec_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55353a72",
   "metadata": {},
   "source": [
    "## 13. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1bb722",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"DATA ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n✓ Dataset Analysis Complete!\")\n",
    "print(\"\\nKey Findings:\")\n",
    "print(f\"  1. Dataset Adequacy: {'✓ ADEQUATE' if adequacy_report['is_adequate'] else '⚠ NEEDS ATTENTION'}\")\n",
    "print(f\"  2. Class Balance: {imbalance_ratio:.2f} ratio ({'✓ BALANCED' if imbalance_ratio <= 1.5 else '⚠ IMBALANCED'})\")\n",
    "print(f\"  3. Missing Values: {len(missing_df)} features with missing values\")\n",
    "print(f\"  4. Normal Distributions: {normal_count}/{len(features)} features ({(normal_count/len(features)*100):.1f}%)\")\n",
    "print(f\"  5. Outliers Removed: {removed} samples ({(removed/initial_size)*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nGenerated Files:\")\n",
    "print(\"  - results/class_balance.png\")\n",
    "print(\"  - results/descriptive_statistics.csv\")\n",
    "print(\"  - results/outlier_boxplots.png\")\n",
    "print(\"  - results/distribution_analysis.png\")\n",
    "print(\"  - results/percentile_quartile_stats.csv\")\n",
    "print(\"  - results/trimmed_statistics.csv\")\n",
    "print(\"  - results/correlation_matrix.csv\")\n",
    "print(\"  - results/correlation_heatmap.png\")\n",
    "print(\"  - gtzan/features_30_sec_cleaned.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Ready for Clustering Implementation!\")\n",
    "print(\"Proceed to: Code_Implementation.ipynb\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
