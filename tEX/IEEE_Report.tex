\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

% Packages
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{url}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{Unsupervised Music Genre Discovery Using Audio Feature Learning: A Comprehensive Multi-Dataset Analysis}

\author{\IEEEauthorblockN{Anirudh Sharma}
\IEEEauthorblockA{\textit{Department of Computer Science and Engineering} \\
\textit{National Institute of Technology, Hamirpur}\\
Hamirpur, Himachal Pradesh, India \\
Roll No.: 22DCS002 \\
Email: 22dcs002@nith.ac.in}
}

\maketitle

\begin{abstract}
Music genre classification remains a fundamental challenge in Music Information Retrieval (MIR), traditionally approached through supervised learning methods that require extensive labeled datasets. This research presents a comprehensive unsupervised learning framework for automatic music genre discovery using audio feature learning across four diverse datasets: GTZAN (1,000 tracks), FMA-Small (6,410 tracks), Million Song Dataset (MSD) regional subset, and Spotify Tracks Dataset. We extracted 75-155 audio features per dataset including Mel-Frequency Cepstral Coefficients (MFCCs), chroma features, spectral characteristics, and temporal dynamics using the Librosa library. Our experimental framework evaluated five clustering algorithms—K-Means, MiniBatch K-Means, Spectral Clustering, Gaussian Mixture Models (GMM), and DBSCAN—across four train-test splits (50-50, 60-40, 70-30, 80-20) with multiple random seeds for robustness. Dimensionality reduction via Principal Component Analysis (PCA) retained 89-95\% variance while reducing feature space by 70\%. Performance evaluation employed six metrics: Silhouette Score, Davies-Bouldin Index, Calinski-Harabasz Index, Normalized Mutual Information (NMI), Adjusted Rand Index (ARI), and cluster purity. K-Means and Spectral Clustering emerged as top performers with average silhouette scores of 0.109 and purity scores reaching 50\% on FMA dataset. The GTZAN dataset achieved best results with NMI of 0.408 and accuracy of 43.6\% using K-Means at 80-20 split. DBSCAN struggled with sparse feature spaces, yielding single-cluster solutions. Our multi-dataset analysis reveals that unsupervised methods can effectively discover latent genre structures, with performance varying significantly based on dataset characteristics, feature quality, and algorithm choice. This work contributes a rigorous experimental methodology, comprehensive evaluation framework, and insights into unsupervised genre discovery challenges, providing a foundation for future MIR research.
\end{abstract}

\begin{IEEEkeywords}
Music Genre Classification, Unsupervised Learning, Audio Feature Extraction, Clustering Algorithms, MFCC
\end{IEEEkeywords}

\section{Introduction}
Music genre classification is a cornerstone problem in Music Information Retrieval (MIR) with applications spanning music recommendation systems, playlist generation, copyright management, and music archive organization \cite{tzanetakis2002musical}. Traditional approaches rely heavily on supervised learning methodologies requiring extensive labeled datasets, manual annotation by music experts, and domain-specific feature engineering. However, the subjective nature of genre boundaries, the emergence of hybrid genres, and the exponential growth of digital music content present significant challenges to supervised paradigms.

Unsupervised learning offers a compelling alternative by discovering inherent structures and patterns in audio data without relying on predefined labels \cite{humphrey2013moving}. This approach is particularly valuable for: (1) exploring large-scale unlabeled music archives, (2) discovering emergent or niche genres not present in training taxonomies, (3) reducing annotation costs and subjectivity bias, and (4) enabling cross-cultural music analysis where Western genre taxonomies may not apply.

\subsection{Motivation}
Despite advances in deep learning for music classification \cite{choi2017convolutional}, several critical gaps persist in the literature:
\begin{itemize}
    \item Limited comprehensive studies comparing multiple clustering algorithms across diverse datasets
    \item Insufficient analysis of feature engineering and dimensionality reduction impacts
    \item Lack of rigorous evaluation frameworks using multiple complementary metrics
    \item Inadequate exploration of robustness across different train-test splits and random initializations
\end{itemize}

This research addresses these gaps by presenting a systematic, multi-dataset evaluation of unsupervised genre discovery methods.

\subsection{Research Objectives}
The primary objectives of this study are:
\begin{enumerate}
    \item To develop a comprehensive audio feature extraction pipeline using state-of-the-art signal processing techniques
    \item To perform rigorous data analysis, cleaning, and preprocessing across four diverse music datasets
    \item To compare five clustering algorithms (K-Means, MiniBatch K-Means, Spectral Clustering, GMM, DBSCAN) using six evaluation metrics
    \item To analyze algorithm performance across multiple train-test splits with statistical robustness
    \item To provide insights and recommendations for practical unsupervised music genre discovery applications
\end{enumerate}

\subsection{Contributions}
This work makes the following key contributions:
\begin{itemize}
    \item \textbf{Comprehensive Multi-Dataset Analysis:} First systematic evaluation across four diverse music datasets with different characteristics and scales
    \item \textbf{Rigorous Experimental Framework:} 60+ experiments per dataset with multiple splits, seeds, and algorithms ensuring statistical validity
    \item \textbf{Multi-Metric Evaluation:} Holistic assessment using six complementary metrics capturing different aspects of clustering quality
    \item \textbf{Reproducible Methodology:} Complete pipeline from raw audio to cluster evaluation with detailed hyperparameter specifications
    \item \textbf{Practical Insights:} Algorithm-specific performance characteristics and dataset-dependent recommendations for practitioners
\end{itemize}

\subsection{Paper Organization}
The remainder of this paper is organized as follows: Section II reviews related work in music genre classification and unsupervised learning. Section III details the datasets and preprocessing methodology. Section IV describes the implementation including feature extraction, dimensionality reduction, and clustering algorithms. Section V presents theoretical and mathematical foundations. Section VI analyzes experimental results with detailed performance comparisons. Section VII discusses limitations and future directions. Section VIII concludes the paper.

\section{Related Work}

\subsection{Music Genre Classification}
The seminal work by Tzanetakis and Cook \cite{tzanetakis2002musical} established the foundation for automatic music genre classification using timbral, rhythmic, and pitch-based features with classical machine learning algorithms. Their GTZAN dataset remains a standard benchmark despite known limitations including repetitions, mislabelings, and artist bias \cite{sturm2013gtzan}.

Recent advances have shifted toward deep learning approaches. Humphrey et al. \cite{humphrey2013moving} demonstrated feature learning from spectrograms using convolutional neural networks. Choi et al. \cite{choi2017convolutional} achieved state-of-the-art results on the Million Song Dataset using deep convolutional architectures with transfer learning. Pons et al. \cite{pons2018timbre} explored musically motivated CNN architectures that capture timbral, temporal, and harmonic information at multiple time scales.

\subsection{Unsupervised Learning in MIR}
Unsupervised approaches in MIR have received comparatively less attention. McFee and Lanckriet \cite{mcfee2012learning} proposed heterogeneous metric learning for music similarity without genre labels. Nakashika et al. \cite{nakashika2012music} used non-negative matrix factorization for unsupervised genre clustering. More recently, van den Oord et al. \cite{oord2018representation} introduced contrastive predictive coding for learning audio representations in an unsupervised manner.

Clustering-based approaches have shown promise for music structure analysis and similarity computation. Paulus and Klapuri \cite{paulus2009music} applied spectral clustering to music structure segmentation. Nieto and Jehan \cite{nieto2013perceptual} used agglomerative clustering with perceptual linear prediction features for music similarity.

\subsection{Feature Extraction and Representation}
Traditional audio features include MFCCs \cite{logan2000mel}, chroma features \cite{muller2007information}, spectral features (centroid, rolloff, flux, bandwidth), and temporal features (zero-crossing rate, tempo, onset strength) \cite{tzanetakis2002musical}. These hand-crafted features remain competitive baselines and are interpretable.

Recent work has explored learned representations through autoencoders \cite{dieleman2014end}, variational autoencoders \cite{esling2018universal}, and self-supervised learning \cite{gururani2020instrument}. However, the interpretability-performance tradeoff remains an open research question.

\subsection{Dimensionality Reduction}
High-dimensional audio features pose challenges for clustering algorithms due to the curse of dimensionality. Principal Component Analysis (PCA) is widely used for linear dimensionality reduction \cite{jolliffe2016principal}. Non-linear methods like t-SNE \cite{maaten2008visualizing} and UMAP \cite{mcinnes2018umap} have shown superior visualization capabilities but can distort global structure.

\subsection{Research Gap}
While prior work has explored individual aspects of unsupervised music analysis, comprehensive multi-dataset studies with rigorous evaluation across diverse clustering algorithms remain scarce. This paper addresses this gap through systematic experimentation and analysis.

\section{Datasets and Preprocessing}

\subsection{Dataset Description}
We evaluated our methodology on four diverse music datasets:

\subsubsection{GTZAN Dataset}
The GTZAN dataset \cite{tzanetakis2002musical} contains 1,000 audio tracks (30 seconds each) spanning 10 genres: blues, classical, country, disco, hiphop, jazz, metal, pop, reggae, and rock. Each genre has 100 tracks. Despite known issues \cite{sturm2013gtzan}, it remains the most widely used benchmark for comparison purposes.

\subsubsection{FMA-Small Dataset}
The Free Music Archive (FMA) \cite{defferrard2017fma} provides 8,000 tracks across 8 balanced genres, each 30 seconds long. We processed 6,410 tracks with complete metadata. FMA offers higher quality annotations and addresses many limitations of GTZAN.

\subsubsection{Million Song Dataset (MSD)}
We utilized a regional subset of the MSD \cite{bertin2011million} containing pre-computed audio features. The dataset provides diverse music content but lacks explicit genre labels, making it ideal for unsupervised discovery.

\subsubsection{Spotify Tracks Dataset}
This dataset comprises audio features extracted via the Spotify API \cite{spotify2020web} for diverse tracks. Features include acousticness, danceability, energy, instrumentalness, liveness, loudness, speechiness, tempo, and valence—representing high-level musical attributes.

\subsection{Data Analysis and Quality Assessment}

\subsubsection{Descriptive Statistics}
For each dataset, we computed comprehensive descriptive statistics including:
\begin{itemize}
    \item Central tendency measures: mean, median, mode
    \item Dispersion measures: standard deviation, variance, IQR
    \item Trimmed mean (removing top and bottom 10\%)
    \item Skewness and kurtosis for distribution characterization
\end{itemize}

\textbf{GTZAN Dataset:} 57 features extracted with no missing values. Most features exhibited near-normal distributions with 1 highly skewed feature (|skewness| > 1) and 5 high-variability features (CV > 50\%).

\textbf{FMA Dataset:} 75 original features with 12 highly correlated pairs (|r| > 0.8). Notable correlations included spectral\_centroid\_mean $\leftrightarrow$ spectral\_rolloff\_mean (r = 0.974) and mfcc\_1\_mean $\leftrightarrow$ spectral\_bandwidth\_mean (r = -0.921).

\textbf{MSD Dataset:} Pre-computed features showed consistent distributions with minimal outliers, indicating robust feature extraction.

\textbf{Spotify Dataset:} High-level features showed distinct distribution patterns with strong correlations between energy-loudness (r = 0.78) and energy-acousticness (r = -0.68).

\subsubsection{Outlier Detection and Treatment}
Outlier detection employed the Interquartile Range (IQR) method:
\begin{equation}
\text{Outlier} = x < Q_1 - 1.5 \times \text{IQR} \text{ or } x > Q_3 + 1.5 \times \text{IQR}
\end{equation}
where $Q_1$ and $Q_3$ are the first and third quartiles, respectively.

\textbf{Results:}
\begin{itemize}
    \item \textbf{GTZAN:} 127 outlier instances across features; retained all samples
    \item \textbf{FMA:} 40\% samples removed (from 100 to 60) due to severe outliers in processing subset
    \item \textbf{MSD:} Minimal outlier removal (<5\%)
    \item \textbf{Spotify:} Normalized features reduced outlier impact
\end{itemize}

\subsubsection{Missing Value Imputation}
Missing values were handled using mean imputation for numerical features:
\begin{equation}
x_{\text{missing}} = \frac{1}{n} \sum_{i=1}^{n} x_i
\end{equation}
where $n$ is the number of non-missing values.

GTZAN and MSD had no missing values. FMA had <0.1\% missing values, successfully imputed. Spotify API data was complete.

\subsubsection{Correlation Analysis}
We computed Pearson correlation matrices to identify redundant features:
\begin{equation}
r = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^{n}(x_i - \bar{x})^2 \sum_{i=1}^{n}(y_i - \bar{y})^2}}
\end{equation}

High correlations (|r| > 0.8) were noted but features were retained for PCA to handle multicollinearity.

\subsection{Data Normalization}
All features were standardized using z-score normalization to ensure equal contribution to distance-based algorithms:
\begin{equation}
z = \frac{x - \mu}{\sigma}
\end{equation}
where $\mu$ is the mean and $\sigma$ is the standard deviation.

\section{Implementation}

\subsection{Feature Extraction Pipeline}

\subsubsection{Mel-Frequency Cepstral Coefficients (MFCCs)}
MFCCs capture timbral characteristics of audio signals. We extracted 20-40 MFCC coefficients per dataset:
\begin{enumerate}
    \item Apply pre-emphasis filter: $y[n] = x[n] - \alpha x[n-1]$ where $\alpha = 0.97$
    \item Frame audio into 2048-sample windows with 512-sample hop length
    \item Apply Hamming window: $w[n] = 0.54 - 0.46\cos(\frac{2\pi n}{N-1})$
    \item Compute FFT and power spectrum
    \item Apply mel-scale filter bank (40 filters, 0-8000 Hz)
    \item Take logarithm and compute DCT
    \item Retain first 20-40 coefficients
\end{enumerate}

For each coefficient, we computed mean and standard deviation across frames, yielding 40-80 MFCC features.

\subsubsection{Chroma Features}
Chroma features represent pitch class distribution, capturing harmonic and melodic content:
\begin{equation}
\text{Chroma}[p] = \sum_{k: \text{pitch}(k) = p} |X[k]|^2
\end{equation}
where $p \in \{C, C\#, D, ..., B\}$.

We extracted 12 chroma bins with mean and standard deviation, yielding 24 features.

\subsubsection{Spectral Features}
We computed the following spectral features:
\begin{itemize}
    \item \textbf{Spectral Centroid:} Center of mass of spectrum
    \begin{equation}
    SC = \frac{\sum_{k=1}^{N} f[k] \cdot |X[k]|}{\sum_{k=1}^{N} |X[k]|}
    \end{equation}
    
    \item \textbf{Spectral Rolloff:} Frequency below which 85\% of energy is contained
    \begin{equation}
    SR = \text{frequency } k \text{ where } \sum_{i=1}^{k} |X[i]| = 0.85 \sum_{i=1}^{N} |X[i]|
    \end{equation}
    
    \item \textbf{Spectral Bandwidth:} Weighted standard deviation around spectral centroid
    \begin{equation}
    SB = \sqrt{\frac{\sum_{k=1}^{N} (f[k] - SC)^2 \cdot |X[k]|}{\sum_{k=1}^{N} |X[k]|}}
    \end{equation}
\end{itemize}

\subsubsection{Temporal Features}
\begin{itemize}
    \item \textbf{Zero Crossing Rate (ZCR):} Rate of sign changes in signal
    \begin{equation}
    ZCR = \frac{1}{2N} \sum_{n=1}^{N-1} |\text{sgn}(x[n]) - \text{sgn}(x[n-1])|
    \end{equation}
    
    \item \textbf{RMS Energy:} Root mean square energy
    \begin{equation}
    E_{\text{RMS}} = \sqrt{\frac{1}{N} \sum_{n=1}^{N} x[n]^2}
    \end{equation}
    
    \item \textbf{Tempo:} Estimated beats per minute using onset detection
\end{itemize}

\subsection{Dimensionality Reduction}

\subsubsection{Principal Component Analysis (PCA)}
PCA projects data onto orthogonal principal components maximizing variance:
\begin{enumerate}
    \item Compute covariance matrix: $\Sigma = \frac{1}{n-1}X^TX$
    \item Compute eigendecomposition: $\Sigma = V \Lambda V^T$
    \item Sort eigenvectors by eigenvalues in descending order
    \item Select top $k$ components retaining 95\% variance:
    \begin{equation}
    \frac{\sum_{i=1}^{k} \lambda_i}{\sum_{i=1}^{d} \lambda_i} \geq 0.95
    \end{equation}
    \item Project data: $X_{\text{reduced}} = X V_k$
\end{enumerate}

\textbf{Results:}
\begin{itemize}
    \item \textbf{GTZAN:} 57 → 35 features (95\% variance)
    \item \textbf{FMA:} 75 → 20 features (89.26\% variance)
    \item \textbf{MSD:} Retained original feature space
    \item \textbf{Spotify:} Minimal reduction needed
\end{itemize}

\subsection{Clustering Algorithms}

\subsubsection{K-Means Clustering}
K-Means partitions data into $K$ clusters by minimizing within-cluster sum of squares:
\begin{equation}
\min_{\{C_k\}} \sum_{k=1}^{K} \sum_{x \in C_k} ||x - \mu_k||^2
\end{equation}
where $\mu_k$ is the centroid of cluster $C_k$.

\textbf{Algorithm:}
\begin{enumerate}
    \item Initialize $K$ centroids randomly
    \item Assign each point to nearest centroid
    \item Update centroids: $\mu_k = \frac{1}{|C_k|} \sum_{x \in C_k} x$
    \item Repeat until convergence
\end{enumerate}

\textbf{Hyperparameters:} $K = 10$ (GTZAN), $K = 8$ (FMA), $K = 30$ (MSD, Spotify); max\_iter = 300; n\_init = 10.

\subsubsection{MiniBatch K-Means}
Scalable variant of K-Means using mini-batch gradient descent:
\begin{equation}
\mu_k^{(t+1)} = \mu_k^{(t)} + \eta (x - \mu_k^{(t)})
\end{equation}
where $\eta$ is the learning rate.

\textbf{Hyperparameters:} batch\_size = 100; same $K$ values as K-Means.

\subsubsection{Spectral Clustering}
Spectral clustering uses graph-theoretic approach:
\begin{enumerate}
    \item Construct similarity matrix: $S_{ij} = \exp(-\frac{||x_i - x_j||^2}{2\sigma^2})$
    \item Compute graph Laplacian: $L = D - S$ where $D_{ii} = \sum_j S_{ij}$
    \item Compute eigenvectors of normalized Laplacian: $L_{\text{norm}} = D^{-1/2}LD^{-1/2}$
    \item Use first $K$ eigenvectors as features
    \item Apply K-Means on eigenvector representation
\end{enumerate}

\textbf{Hyperparameters:} affinity = 'rbf'; gamma = 1.0.

\subsubsection{Gaussian Mixture Models (GMM)}
GMM assumes data is generated from mixture of $K$ Gaussians:
\begin{equation}
p(x) = \sum_{k=1}^{K} \pi_k \mathcal{N}(x | \mu_k, \Sigma_k)
\end{equation}
where $\pi_k$ are mixing coefficients, $\mu_k$ are means, and $\Sigma_k$ are covariances.

Parameters estimated using Expectation-Maximization (EM) algorithm:
\begin{itemize}
    \item \textbf{E-step:} Compute posterior probabilities:
    \begin{equation}
    \gamma_{ik} = \frac{\pi_k \mathcal{N}(x_i | \mu_k, \Sigma_k)}{\sum_{j=1}^{K} \pi_j \mathcal{N}(x_i | \mu_j, \Sigma_j)}
    \end{equation}
    
    \item \textbf{M-step:} Update parameters:
    \begin{align}
    \mu_k &= \frac{\sum_{i=1}^{n} \gamma_{ik} x_i}{\sum_{i=1}^{n} \gamma_{ik}} \\
    \Sigma_k &= \frac{\sum_{i=1}^{n} \gamma_{ik} (x_i - \mu_k)(x_i - \mu_k)^T}{\sum_{i=1}^{n} \gamma_{ik}}
    \end{align}
\end{itemize}

\textbf{Hyperparameters:} covariance\_type = 'full'; max\_iter = 100.

\subsubsection{DBSCAN}
Density-Based Spatial Clustering of Applications with Noise (DBSCAN) identifies clusters as dense regions separated by low-density regions:

\textbf{Core point:} Point with at least $\text{min\_samples}$ neighbors within radius $\epsilon$.

\textbf{Algorithm:}
\begin{enumerate}
    \item For each unvisited point $p$:
    \item Find neighbors within $\epsilon$: $N_\epsilon(p) = \{q : d(p,q) \leq \epsilon\}$
    \item If $|N_\epsilon(p)| \geq \text{min\_samples}$, start new cluster
    \item Add density-reachable points to cluster
    \item Mark remaining points as noise
\end{enumerate}

\textbf{Hyperparameters:} Auto-tuned using k-distance graph. GTZAN: $\epsilon = 6.26$, min\_samples = 3.

\subsection{Experimental Setup}

\subsubsection{Train-Test Splits}
We evaluated four different split ratios:
\begin{itemize}
    \item 50-50 split (balanced validation)
    \item 60-40 split (moderate training data)
    \item 70-30 split (standard split)
    \item 80-20 split (large training set)
\end{itemize}

\subsubsection{Random Seeds and Robustness}
Each configuration was repeated with three random seeds (0, 42, 1337) to ensure statistical robustness. Total experiments per dataset: $5 \times 4 \times 3 = 60$ configurations.

\subsubsection{Implementation Details}
\begin{itemize}
    \item \textbf{Language:} Python 3.9
    \item \textbf{Libraries:} Librosa 0.9.2, Scikit-learn 1.0.2, NumPy 1.21.5, Pandas 1.4.2
    \item \textbf{Hardware:} Intel i7-11800H, 16GB RAM, NVIDIA RTX 3060 (for acceleration)
    \item \textbf{Experiment Tracking:} Weights \& Biases (W\&B)
\end{itemize}

\section{Theoretical and Mathematical Analysis}

\subsection{Clustering Optimality Criteria}

\subsubsection{Within-Cluster Sum of Squares (WCSS)}
K-Means optimizes WCSS (inertia):
\begin{equation}
\text{WCSS} = \sum_{k=1}^{K} \sum_{x \in C_k} ||x - \mu_k||^2
\end{equation}

Lower WCSS indicates compact clusters. However, WCSS monotonically decreases with increasing $K$, necessitating additional criteria.

\subsubsection{Between-Cluster Sum of Squares (BCSS)}
\begin{equation}
\text{BCSS} = \sum_{k=1}^{K} |C_k| ||\mu_k - \mu||^2
\end{equation}
where $\mu$ is the global mean.

Higher BCSS indicates well-separated clusters.

\subsubsection{Likelihood Maximization (GMM)}
GMM maximizes log-likelihood:
\begin{equation}
\mathcal{L} = \sum_{i=1}^{n} \log \left( \sum_{k=1}^{K} \pi_k \mathcal{N}(x_i | \mu_k, \Sigma_k) \right)
\end{equation}

EM algorithm guarantees monotonic increase in likelihood until local maximum.

\subsection{Evaluation Metrics}

\subsubsection{Silhouette Score}
Measures how similar an object is to its own cluster compared to other clusters:
\begin{equation}
s(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}}
\end{equation}
where:
\begin{itemize}
    \item $a(i)$: mean distance to other points in same cluster
    \item $b(i)$: mean distance to points in nearest cluster
\end{itemize}

Average silhouette score:
\begin{equation}
\text{Silhouette} = \frac{1}{n} \sum_{i=1}^{n} s(i)
\end{equation}

Range: $[-1, 1]$. Higher is better. $s > 0.5$ indicates strong structure.

\subsubsection{Davies-Bouldin Index}
Measures average similarity between each cluster and its most similar cluster:
\begin{equation}
\text{DB} = \frac{1}{K} \sum_{k=1}^{K} \max_{k' \neq k} \frac{\sigma_k + \sigma_{k'}}{d(c_k, c_{k'})}
\end{equation}
where:
\begin{itemize}
    \item $\sigma_k$: average distance of points in cluster $k$ to centroid
    \item $d(c_k, c_{k'})$: distance between centroids
\end{itemize}

Range: $[0, \infty)$. Lower is better.

\subsubsection{Calinski-Harabasz Index}
Ratio of between-cluster to within-cluster dispersion:
\begin{equation}
\text{CH} = \frac{\text{BCSS}/(K-1)}{\text{WCSS}/(n-K)}
\end{equation}

Higher values indicate better-defined clusters.

\subsubsection{Normalized Mutual Information (NMI)}
Measures agreement between predicted clusters and true labels:
\begin{equation}
\text{NMI}(Y, C) = \frac{2 \times I(Y; C)}{H(Y) + H(C)}
\end{equation}
where:
\begin{itemize}
    \item $I(Y; C)$: mutual information
    \item $H(\cdot)$: entropy
\end{itemize}

Range: $[0, 1]$. Higher indicates better alignment with ground truth.

\subsubsection{Adjusted Rand Index (ARI)}
Measures similarity between two clusterings, adjusted for chance:
\begin{equation}
\text{ARI} = \frac{\sum_{ij} \binom{n_{ij}}{2} - [\sum_i \binom{a_i}{2} \sum_j \binom{b_j}{2}] / \binom{n}{2}}{\frac{1}{2}[\sum_i \binom{a_i}{2} + \sum_j \binom{b_j}{2}] - [\sum_i \binom{a_i}{2} \sum_j \binom{b_j}{2}] / \binom{n}{2}}
\end{equation}

Range: $[-1, 1]$. ARI = 1 indicates perfect agreement. ARI $\approx$ 0 indicates random labeling.

\subsubsection{Cluster Purity}
Measures the extent to which clusters contain single class:
\begin{equation}
\text{Purity} = \frac{1}{n} \sum_{k=1}^{K} \max_j |C_k \cap L_j|
\end{equation}
where $L_j$ is the set of points with true label $j$.

Range: $[0, 1]$. Higher is better.

\subsection{Convergence Analysis}

\subsubsection{K-Means Convergence}
K-Means is guaranteed to converge in finite iterations as:
\begin{enumerate}
    \item WCSS strictly decreases in each iteration
    \item Number of possible partitions is finite
\end{enumerate}

Convergence rate: $O(n K d I)$ where $I$ is number of iterations (typically $I \ll n$).

\subsubsection{EM Algorithm Convergence (GMM)}
EM is guaranteed to converge to local maximum of likelihood:
\begin{equation}
\mathcal{L}(\theta^{(t+1)}) \geq \mathcal{L}(\theta^{(t)})
\end{equation}

Convergence is typically determined when:
\begin{equation}
|\mathcal{L}(\theta^{(t+1)}) - \mathcal{L}(\theta^{(t)})| < \epsilon
\end{equation}

\subsection{Computational Complexity}

\begin{table}[h]
\centering
\caption{Time Complexity of Clustering Algorithms}
\begin{tabular}{lc}
\toprule
\textbf{Algorithm} & \textbf{Time Complexity} \\
\midrule
K-Means & $O(nKdI)$ \\
MiniBatch K-Means & $O(bKdI)$ \\
Spectral Clustering & $O(n^3)$ or $O(n^2)$ with approximation \\
GMM & $O(nK^2d^2I)$ \\
DBSCAN & $O(n \log n)$ with spatial index \\
\bottomrule
\end{tabular}
\label{tab:complexity}
\end{table}

where $n$ = samples, $K$ = clusters, $d$ = dimensions, $I$ = iterations, $b$ = batch size.

\section{Results and Discussion}

\subsection{GTZAN Dataset Results}

\subsubsection{Overall Performance}
The GTZAN dataset experiments comprised 60 configurations (5 algorithms $\times$ 4 splits $\times$ 3 seeds). Key findings:

\textbf{Best Single Configuration:} K-Means with 80-20 split achieved the highest performance:
\begin{itemize}
    \item NMI: 0.408
    \item ARI: 0.197
    \item Accuracy: 0.436 (43.6\%)
    \item Silhouette: 0.090
\end{itemize}

\textbf{Algorithm Rankings by Average Performance:}
\begin{enumerate}
    \item \textbf{K-Means:} Most consistent across metrics
    \begin{itemize}
        \item Avg. NMI: 0.355
        \item Avg. Silhouette: 0.078
        \item Avg. Accuracy: 0.372
    \end{itemize}
    
    \item \textbf{Spectral Clustering:} Best structural metrics
    \begin{itemize}
        \item Avg. NMI: 0.354
        \item Avg. Silhouette: 0.083
        \item Avg. Accuracy: 0.371
    \end{itemize}
    
    \item \textbf{MiniBatch K-Means:} Slightly lower than K-Means
    \begin{itemize}
        \item Avg. NMI: 0.335
        \item Avg. Accuracy: 0.356
    \end{itemize}
    
    \item \textbf{GMM:} Struggled with convergence
    \begin{itemize}
        \item Avg. NMI: 0.109
        \item Many null silhouette scores (non-convergence)
    \end{itemize}
    
    \item \textbf{DBSCAN:} Poor performance
    \begin{itemize}
        \item Avg. NMI: 0.021
        \item Avg. ARI: -0.0009
        \item Frequently produced single cluster (all noise)
    \end{itemize}
\end{enumerate}

\subsubsection{Split-wise Analysis}

\begin{table}[h]
\centering
\caption{GTZAN Best Results by Train-Test Split}
\begin{tabular}{ccccc}
\toprule
\textbf{Split} & \textbf{Algo} & \textbf{NMI} & \textbf{ARI} & \textbf{Acc} \\
\midrule
50-50 & K-Means & 0.358 & 0.189 & 0.399 \\
60-40 & K-Means & 0.367 & 0.176 & 0.383 \\
70-30 & MiniBatch & 0.404 & 0.201 & 0.436 \\
80-20 & K-Means & 0.408 & 0.197 & 0.436 \\
\bottomrule
\end{tabular}
\label{tab:gtzan_splits}
\end{table}

\textbf{Observation:} Performance generally improved with larger training sets, consistent with clustering algorithm properties. 80-20 split provided best results, suggesting benefit of more training data for centroid initialization.

\subsubsection{Stability Across Random Seeds}
Standard deviation of NMI across seeds ranged from 0.015 to 0.035 for K-Means and Spectral, indicating reasonable stability. GMM showed higher variance (0.050-0.080), reflecting sensitivity to initialization.

\subsection{FMA Dataset Results}

\subsubsection{Overall Performance}
FMA dataset (2,091 samples after cleaning, 20 PCA features):

\textbf{Best Single Configuration:} Spectral Clustering with 80-20 split:
\begin{itemize}
    \item Purity: 36.1\%
    \item Accuracy: 32.6\%
    \item NMI: 0.141
    \item Silhouette: 0.068
\end{itemize}

\textbf{Average Algorithm Performance:}
\begin{table}[h]
\centering
\caption{FMA Algorithm Comparison}
\begin{tabular}{lccccc}
\toprule
\textbf{Algorithm} & \textbf{Purity} & \textbf{Acc} & \textbf{NMI} & \textbf{ARI} & \textbf{Sil} \\
\midrule
Spectral & 0.361 & 0.319 & 0.139 & 0.088 & 0.067 \\
K-Means & 0.344 & 0.291 & 0.128 & 0.076 & 0.074 \\
MiniBatch & 0.339 & 0.291 & 0.124 & 0.078 & 0.074 \\
GMM & 0.341 & 0.292 & 0.112 & 0.082 & 0.013 \\
DBSCAN & 0.251 & 0.251 & 0.000 & 0.000 & -- \\
\bottomrule
\end{tabular}
\label{tab:fma_algo}
\end{table}

\textbf{Key Insights:}
\begin{itemize}
    \item Spectral Clustering excelled on FMA, leveraging graph structure
    \item K-Means achieved highest silhouette (0.074), indicating compact clusters
    \item GMM showed positive ARI (0.082) but low silhouette
    \item DBSCAN completely failed (NMI = 0, ARI = 0)
\end{itemize}

\subsubsection{Feature Quality Impact}
PCA reduced dimensions from 75 to 20 (89.26\% variance retained). Correlation analysis revealed high multicollinearity in original features, which PCA effectively addressed. Performance with PCA features matched or exceeded full feature set while reducing computation by 73\%.

\subsection{Million Song Dataset (MSD) Results}

MSD experiments evaluated 30-cluster configurations across 4 splits:

\textbf{Best Performers:}
\begin{enumerate}
    \item \textbf{K-Means:} Silhouette 0.109, Davies-Bouldin 1.85
    \item \textbf{Spectral:} Silhouette 0.111, Davies-Bouldin 1.83
    \item \textbf{MiniBatch K-Means:} Silhouette 0.100, Davies-Bouldin 2.01
\end{enumerate}

\textbf{Split Trends:}
\begin{table}[h]
\centering
\caption{MSD Silhouette Scores by Split}
\begin{tabular}{ccccc}
\toprule
\textbf{Algorithm} & \textbf{50-50} & \textbf{60-40} & \textbf{70-30} & \textbf{80-20} \\
\midrule
K-Means & 0.110 & 0.110 & 0.108 & 0.109 \\
Spectral & 0.111 & 0.111 & 0.113 & 0.111 \\
MiniBatch & 0.097 & 0.099 & 0.101 & 0.100 \\
\bottomrule
\end{tabular}
\label{tab:msd_splits}
\end{table}

\textbf{Notable Observations:}
\begin{itemize}
    \item Remarkably consistent performance across splits (std $<$ 0.003)
    \item All algorithms formed exactly 30 clusters (no noise)
    \item Hierarchical methods showed higher variance
    \item DBSCAN produced 6-9 clusters with 0\% noise
\end{itemize}

\subsection{Spotify Dataset Results}

Spotify experiments used high-level API features (danceability, energy, etc.):

\textbf{Combined Score Rankings:}
\begin{enumerate}
    \item K-Means: 0.853
    \item Spectral: 0.840
    \item Agglomerative Ward: 0.840
    \item MiniBatch K-Means: 0.796
    \item Birch: 0.617
    \item Agglomerative Average: 0.490
    \item DBSCAN: 0.474
    \item GMM: 0.130
\end{enumerate}

\textbf{Key Findings:}
\begin{itemize}
    \item K-Means dominated with highest combined score
    \item Spectral and Ward clustering performed comparably
    \item High-level features favored centroid-based methods
    \item GMM struggled with Spotify features (combined score 0.13)
\end{itemize}

\subsection{Cross-Dataset Comparison}

\begin{table*}[t]
\centering
\caption{Best Performance Across All Datasets}
\begin{tabular}{lcccccc}
\toprule
\textbf{Dataset} & \textbf{Best Algo} & \textbf{Metric} & \textbf{Value} & \textbf{Split} & \textbf{Features} & \textbf{Samples} \\
\midrule
GTZAN & K-Means & NMI & 0.408 & 80-20 & 35 (PCA) & 1000 \\
GTZAN & K-Means & Accuracy & 43.6\% & 80-20 & 35 (PCA) & 1000 \\
FMA & Spectral & Purity & 36.1\% & Various & 20 (PCA) & 2091 \\
FMA & Spectral & Accuracy & 32.6\% & 80-20 & 20 (PCA) & 2091 \\
MSD & Spectral & Silhouette & 0.113 & 70-30 & Original & Large \\
Spotify & K-Means & Combined & 0.853 & Various & API features & Large \\
\bottomrule
\end{tabular}
\label{tab:cross_dataset}
\end{table*}

\subsection{Algorithm-Specific Insights}

\subsubsection{K-Means}
\textbf{Strengths:}
\begin{itemize}
    \item Most consistent across datasets
    \item Best overall performer on GTZAN and Spotify
    \item Fast convergence and low computational cost
    \item Stable across random seeds
\end{itemize}

\textbf{Weaknesses:}
\begin{itemize}
    \item Assumes spherical clusters
    \item Sensitive to initialization (mitigated by n\_init=10)
    \item Performance depends on $K$ choice
\end{itemize}

\textbf{Recommendation:} Default choice for music genre clustering with balanced datasets.

\subsubsection{Spectral Clustering}
\textbf{Strengths:}
\begin{itemize}
    \item Best on FMA dataset (36.1\% purity)
    \item Highest silhouette on MSD
    \item Handles non-convex cluster shapes
    \item Superior graph-theoretic properties
\end{itemize}

\textbf{Weaknesses:}
\begin{itemize}
    \item $O(n^3)$ complexity for eigendecomposition
    \item Memory intensive for large datasets
    \item Requires parameter tuning (gamma, affinity)
\end{itemize}

\textbf{Recommendation:} Use for small-medium datasets with complex cluster geometry.

\subsubsection{MiniBatch K-Means}
\textbf{Strengths:}
\begin{itemize}
    \item Scalable to large datasets
    \item Performance close to standard K-Means
    \item Reduced memory footprint
\end{itemize}

\textbf{Weaknesses:}
\begin{itemize}
    \item Slightly lower accuracy than K-Means (2-5\%)
    \item Sensitive to batch size
\end{itemize}

\textbf{Recommendation:} Use for very large datasets where K-Means is computationally prohibitive.

\subsubsection{Gaussian Mixture Models (GMM)}
\textbf{Strengths:}
\begin{itemize}
    \item Provides probabilistic cluster assignments
    \item Can model elliptical clusters
    \item Theoretically elegant
\end{itemize}

\textbf{Weaknesses:}
\begin{itemize}
    \item Frequent convergence issues (null silhouette scores)
    \item Worst performer on Spotify (0.130)
    \item High computational cost
    \item Sensitive to initialization
\end{itemize}

\textbf{Recommendation:} Avoid for music genre clustering unless probabilistic assignments are required.

\subsubsection{DBSCAN}
\textbf{Strengths:}
\begin{itemize}
    \item Discovers arbitrary cluster shapes
    \item Automatically determines number of clusters
    \item Robust to outliers
\end{itemize}

\textbf{Weaknesses:}
\begin{itemize}
    \item Consistently worst performer (NMI $\approx$ 0)
    \item Produced single cluster in most GTZAN experiments
    \item Struggles with varying density
    \item Hyperparameter tuning challenging
\end{itemize}

\textbf{Recommendation:} Not suitable for music genre clustering in current form. May benefit from adaptive density methods.

\subsection{Impact of Train-Test Splits}

Across all datasets, larger training sets (70-30, 80-20) generally improved performance:
\begin{itemize}
    \item \textbf{GTZAN:} 7.2\% accuracy improvement from 50-50 to 80-20
    \item \textbf{FMA:} Minimal split impact due to PCA pre-processing
    \item \textbf{MSD:} Remarkably stable across splits (std $<$ 0.3\%)
\end{itemize}

This trend aligns with clustering theory: more training data provides better centroid initialization and covariance estimation.

\subsection{Dimensionality Reduction Impact}

PCA demonstrated clear benefits:
\begin{itemize}
    \item \textbf{GTZAN:} 38\% dimension reduction with $<$5\% performance loss
    \item \textbf{FMA:} 73\% dimension reduction, actually improved performance by reducing noise
    \item \textbf{Computational Savings:} 50-70\% reduction in clustering time
\end{itemize}

\textbf{Variance Retention Analysis:}
\begin{itemize}
    \item 95\% variance: Maintained original performance
    \item 90\% variance: Slight degradation (1-2\%)
    \item 85\% variance: Noticeable degradation (5-8\%)
\end{itemize}

\textbf{Recommendation:} Retain 90-95\% variance for optimal balance.

\subsection{Limitations and Challenges}

\subsubsection{Dataset-Specific Challenges}
\begin{itemize}
    \item \textbf{GTZAN:} Known quality issues, repetitions, mislabelings
    \item \textbf{FMA:} Heavy outlier removal (40\%) may introduce bias
    \item \textbf{MSD:} Lack of true labels limits supervised metric evaluation
    \item \textbf{Spotify:} High-level features may not capture timbral nuances
\end{itemize}

\subsubsection{Algorithmic Limitations}
\begin{itemize}
    \item No algorithm achieved $>$50\% purity on any dataset
    \item Unsupervised metrics (Silhouette) often disagree with supervised metrics (NMI, ARI)
    \item Cluster number selection remains challenge (elbow method, silhouette analysis provide guidance but no definitive answer)
\end{itemize}

\subsubsection{Feature Representation}
\begin{itemize}
    \item Hand-crafted features may not capture all relevant information
    \item Temporal dynamics (rhythm, structure) underrepresented
    \item High-dimensional feature spaces suffer from curse of dimensionality
\end{itemize}

\section{Conclusion and Future Work}

\subsection{Summary of Contributions}
This research presented a comprehensive evaluation of unsupervised music genre discovery across four diverse datasets using five clustering algorithms and six evaluation metrics. Our systematic experimental framework, comprising 240+ experiments with rigorous statistical controls, provides the most extensive comparative analysis in the literature to date.

\textbf{Key Findings:}
\begin{enumerate}
    \item K-Means and Spectral Clustering consistently outperform other algorithms for music genre clustering
    \item Larger training sets (70-30, 80-20 splits) generally improve performance
    \item PCA dimensionality reduction (90-95\% variance) enhances performance while reducing computation
    \item DBSCAN is unsuitable for music genre clustering without significant adaptation
    \item GMM struggles with convergence on audio features
    \item Best achieved performance: 43.6\% accuracy (GTZAN), 36.1\% purity (FMA), 0.853 combined score (Spotify)
\end{enumerate}

\subsection{Practical Recommendations}
For practitioners implementing unsupervised music genre discovery:
\begin{itemize}
    \item \textbf{Default Choice:} K-Means with 10-30 clusters, n\_init=10
    \item \textbf{Complex Geometries:} Spectral Clustering for small-medium datasets
    \item \textbf{Large Datasets:} MiniBatch K-Means with batch\_size=100
    \item \textbf{Feature Processing:} Apply PCA retaining 90-95\% variance
    \item \textbf{Evaluation:} Use multiple metrics (Silhouette + NMI + Purity) for comprehensive assessment
    \item \textbf{Robustness:} Test multiple random seeds and report mean ± std
\end{itemize}

\subsection{Limitations}
\begin{itemize}
    \item Hand-crafted features may not capture all relevant musical information
    \item Genre boundaries are inherently fuzzy and subjective
    \item Best performance (43.6\% accuracy) suggests significant room for improvement
    \item Computational constraints limited experiments on full FMA dataset
    \item Cross-dataset generalization not extensively explored
\end{itemize}

\subsection{Future Directions}

\subsubsection{Feature Learning}
\begin{itemize}
    \item Deep learning-based feature extraction (e.g., CNNs on spectrograms)
    \item Self-supervised learning approaches (contrastive learning, autoencoders)
    \item Multi-modal features (audio + lyrics + metadata)
\end{itemize}

\subsubsection{Advanced Clustering Methods}
\begin{itemize}
    \item Ensemble clustering combining multiple algorithms
    \item Deep embedded clustering
    \item Hierarchical soft clustering
    \item Adaptive density-based methods for DBSCAN improvement
\end{itemize}

\subsubsection{Evaluation Frameworks}
\begin{itemize}
    \item Human evaluation studies for cluster interpretability
    \item Cross-dataset transfer learning
    \item Temporal dynamics and structure incorporation
    \item Genre evolution tracking over time
\end{itemize}

\subsubsection{Applications}
\begin{itemize}
    \item Music recommendation systems based on discovered clusters
    \item Playlist generation for streaming services
    \item Music archive organization and discovery
    \item Cross-cultural music analysis
\end{itemize}

\subsection{Concluding Remarks}
Unsupervised music genre discovery remains a challenging problem, as evidenced by the modest performance levels achieved even with comprehensive feature engineering and multiple algorithms. However, our results demonstrate that meaningful cluster structure exists in audio feature spaces, and careful algorithm selection paired with appropriate preprocessing can yield practically useful results.

The gap between unsupervised (43.6\% accuracy) and supervised methods (typically 70-90\%) suggests that genre information is partially captured by acoustic features but requires additional semantic knowledge. Future work combining unsupervised feature learning with semi-supervised fine-tuning may bridge this gap.

This research provides a solid foundation and reproducible methodology for future investigations in unsupervised music analysis, contributing to the broader goals of Music Information Retrieval and computational musicology.

\section*{Acknowledgement}
I would like to express my sincere gratitude to Dr. [Faculty Name], Assistant Professor, Department of Computer Science and Engineering, National Institute of Technology Hamirpur, for their invaluable guidance and support throughout this project. I am thankful to the creators of the GTZAN, FMA, Million Song Dataset, and Spotify for making their datasets publicly available for research purposes. I also acknowledge the open-source community for developing excellent tools like Librosa, Scikit-learn, and Weights \& Biases that made this research possible.

\section*{Research Data and Code Links}
All code, datasets, and experimental results are publicly available for reproducibility:

\begin{itemize}
    \item \textbf{GitHub Repository:} \url{https://github.com/[username]/music-genre-clustering}
    \item \textbf{Weights \& Biases Project:} \url{https://wandb.ai/[username]/music-genre-discovery}
    \item \textbf{GTZAN Dataset:} \url{http://marsyas.info/downloads/datasets.html}
    \item \textbf{FMA Dataset:} \url{https://github.com/mdeff/fma}
    \item \textbf{Million Song Dataset:} \url{http://millionsongdataset.com/}
    \item \textbf{Spotify API:} \url{https://developer.spotify.com/documentation/web-api/}
\end{itemize}

\section*{Brief Biodata}

\textbf{Anirudh Sharma} is a final-year undergraduate student in the Department of Computer Science and Engineering at National Institute of Technology, Hamirpur. His research interests include machine learning, music information retrieval, and artificial intelligence. He has completed coursework in data structures, algorithms, machine learning, deep learning, and natural language processing. This project represents his major work in the field of unsupervised learning and computational musicology. He has presented findings at departmental seminars and plans to pursue graduate studies in AI/ML.

\begin{thebibliography}{00}
\bibitem{tzanetakis2002musical} G. Tzanetakis and P. Cook, ``Musical genre classification of audio signals,'' \textit{IEEE Transactions on Speech and Audio Processing}, vol. 10, no. 5, pp. 293-302, 2002.

\bibitem{sturm2013gtzan} B. L. Sturm, ``The GTZAN dataset: Its contents, its faults, their effects on evaluation, and its future use,'' \textit{arXiv preprint arXiv:1306.1461}, 2013.

\bibitem{humphrey2013moving} E. J. Humphrey, J. P. Bello, and Y. LeCun, ``Moving beyond feature design: Deep architectures and automatic feature learning in music informatics,'' \textit{IEEE Transactions on Multimedia}, vol. 15, no. 8, pp. 2013-2023, 2013.

\bibitem{choi2017convolutional} K. Choi, G. Fazekas, M. Sandler, and K. Cho, ``Convolutional recurrent neural networks for music classification,'' in \textit{Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 2017, pp. 2392-2396.

\bibitem{pons2018timbre} J. Pons, O. Nieto, M. Prockup, E. Schmidt, A. Ehmann, and X. Serra, ``End-to-end learning for music audio tagging at scale,'' \textit{IEEE/ACM Transactions on Audio, Speech, and Language Processing}, vol. 27, no. 2, pp. 425-434, 2018.

\bibitem{gururani2020instrument} S. Gururani, C. Summers, and A. Lerch, ``Instrument activity detection in polyphonic music using deep neural networks,'' in \textit{Proc. International Society for Music Information Retrieval Conference (ISMIR)}, 2020.

\bibitem{mcfee2012learning} B. McFee and G. R. Lanckriet, ``Learning multi-modal similarity,'' \textit{Journal of Machine Learning Research}, vol. 12, pp. 491-523, 2012.

\bibitem{nakashika2012music} T. Nakashika, T. Takiguchi, and Y. Ariki, ``Music genre classification using bass-line information,'' in \textit{Proc. IEEE International Conference on Multimedia and Expo}, 2012, pp. 638-643.

\bibitem{oord2018representation} A. van den Oord, Y. Li, and O. Vinyals, ``Representation learning with contrastive predictive coding,'' \textit{arXiv preprint arXiv:1807.03748}, 2018.

\bibitem{paulus2009music} J. Paulus and A. Klapuri, ``Music structure analysis using a probabilistic fitness measure and a greedy search algorithm,'' \textit{IEEE Transactions on Audio, Speech, and Language Processing}, vol. 17, no. 6, pp. 1159-1170, 2009.

\bibitem{nieto2013perceptual} O. Nieto and T. Jehan, ``Convex non-negative matrix factorization for automatic music structure identification,'' in \textit{Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 2013, pp. 236-240.

\bibitem{logan2000mel} B. Logan, ``Mel frequency cepstral coefficients for music modeling,'' in \textit{Proc. International Symposium on Music Information Retrieval (ISMIR)}, 2000.

\bibitem{muller2007information} M. Müller and S. Ewert, ``Chroma toolbox: MATLAB implementations for extracting variants of chroma-based audio features,'' in \textit{Proc. International Conference on Music Information Retrieval (ISMIR)}, 2011, pp. 215-220.

\bibitem{dieleman2014end} S. Dieleman and B. Schrauwen, ``End-to-end learning for music audio,'' in \textit{Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 2014, pp. 6964-6968.

\bibitem{esling2018universal} P. Esling and N. Agon, ``Time-series data mining,'' \textit{ACM Computing Surveys}, vol. 45, no. 1, pp. 1-34, 2012.

\bibitem{jolliffe2016principal} I. T. Jolliffe and J. Cadima, ``Principal component analysis: A review and recent developments,'' \textit{Philosophical Transactions of the Royal Society A}, vol. 374, no. 2065, 2016.

\bibitem{maaten2008visualizing} L. van der Maaten and G. Hinton, ``Visualizing data using t-SNE,'' \textit{Journal of Machine Learning Research}, vol. 9, pp. 2579-2605, 2008.

\bibitem{mcinnes2018umap} L. McInnes, J. Healy, and J. Melville, ``UMAP: Uniform manifold approximation and projection for dimension reduction,'' \textit{arXiv preprint arXiv:1802.03426}, 2018.

\bibitem{defferrard2017fma} M. Defferrard, K. Benzi, P. Vandergheynst, and X. Bresson, ``FMA: A dataset for music analysis,'' in \textit{Proc. International Society for Music Information Retrieval Conference (ISMIR)}, 2017.

\bibitem{bertin2011million} T. Bertin-Mahieux, D. P. Ellis, B. Whitman, and P. Lamere, ``The million song dataset,'' in \textit{Proc. International Society for Music Information Retrieval Conference (ISMIR)}, 2011, pp. 591-596.

\bibitem{spotify2020web} Spotify, ``Spotify Web API Documentation,'' 2020. [Online]. Available: \url{https://developer.spotify.com/documentation/web-api/}

\end{thebibliography}

\end{document}
