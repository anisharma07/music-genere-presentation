{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de84aef8",
   "metadata": {},
   "source": [
    "# FMA Music Genre Clustering - Complete Implementation\n",
    "\n",
    "## Unsupervised Music Genre Discovery Using Audio Feature Learning\n",
    "\n",
    "This notebook implements a comprehensive pipeline for discovering music genres through audio feature analysis and unsupervised clustering algorithms using the FMA (Free Music Archive) dataset.\n",
    "\n",
    "### Pipeline Overview:\n",
    "1. **Feature Extraction**: Extract audio features (MFCCs, Chroma, Spectral features, Tempo)\n",
    "2. **Data Analysis**: Statistical analysis, outlier detection, and data cleaning\n",
    "3. **Data Preprocessing**: Standardization and dimensionality reduction (PCA)\n",
    "4. **Clustering**: K-Means, Spectral Clustering, DBSCAN, and GMM\n",
    "5. **Evaluation**: Multiple internal and external metrics\n",
    "6. **Visualization**: Results comparison and analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6532d39",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d16c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Audio processing\n",
    "import librosa\n",
    "import soundfile\n",
    "\n",
    "# Machine learning and clustering\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans, SpectralClustering, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score,\n",
    "    davies_bouldin_score,\n",
    "    calinski_harabasz_score,\n",
    "    adjusted_rand_score,\n",
    "    normalized_mutual_info_score,\n",
    "    v_measure_score\n",
    ")\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Utilities\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✓ All libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Librosa version: {librosa.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08de016c",
   "metadata": {},
   "source": [
    "## 2. Configuration and Setup\n",
    "\n",
    "Define paths and parameters for the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c80bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'data_path': 'fma_small',\n",
    "    'output_dir': 'output/results',\n",
    "    'sr': 22050,  # Sample rate\n",
    "    'duration': 30,  # Duration in seconds\n",
    "    'n_mfcc': 20,  # Number of MFCC coefficients\n",
    "    'n_clusters': 10,  # Number of clusters\n",
    "    'n_pca_components': 20,  # PCA components\n",
    "    'random_state': 42,\n",
    "    'max_files': None  # Set to a number for testing (e.g., 100)\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(CONFIG['output_dir'], exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e6383b",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction from Audio Files\n",
    "\n",
    "Extract comprehensive audio features including:\n",
    "- **MFCCs** (Mel-Frequency Cepstral Coefficients): 20 coefficients + delta + delta-delta\n",
    "- **Chroma Features**: 12 pitch classes\n",
    "- **Spectral Features**: Centroid, Rolloff, Bandwidth\n",
    "- **Temporal Features**: Zero Crossing Rate, Tempo, RMS Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a2911f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_features(audio_path, sr=22050, duration=30):\n",
    "    \"\"\"\n",
    "    Extract comprehensive audio features from a single audio file.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    audio_path : str\n",
    "        Path to the audio file\n",
    "    sr : int\n",
    "        Sample rate\n",
    "    duration : int\n",
    "        Duration in seconds\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing all extracted features\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load audio\n",
    "        y, sr = librosa.load(audio_path, sr=sr, duration=duration)\n",
    "        \n",
    "        features = {}\n",
    "        \n",
    "        # 1. MFCCs (20 coefficients)\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)\n",
    "        for i in range(20):\n",
    "            features[f'mfcc_{i}_mean'] = np.mean(mfccs[i])\n",
    "            features[f'mfcc_{i}_std'] = np.std(mfccs[i])\n",
    "        \n",
    "        # 2. Delta MFCCs (temporal dynamics)\n",
    "        mfcc_delta = librosa.feature.delta(mfccs)\n",
    "        for i in range(20):\n",
    "            features[f'mfcc_delta_{i}_mean'] = np.mean(mfcc_delta[i])\n",
    "            features[f'mfcc_delta_{i}_std'] = np.std(mfcc_delta[i])\n",
    "        \n",
    "        # 3. Delta-Delta MFCCs (acceleration)\n",
    "        mfcc_delta2 = librosa.feature.delta(mfccs, order=2)\n",
    "        for i in range(20):\n",
    "            features[f'mfcc_delta2_{i}_mean'] = np.mean(mfcc_delta2[i])\n",
    "            features[f'mfcc_delta2_{i}_std'] = np.std(mfcc_delta2[i])\n",
    "        \n",
    "        # 4. Chroma features (12 pitch classes)\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "        for i in range(12):\n",
    "            features[f'chroma_{i}_mean'] = np.mean(chroma[i])\n",
    "            features[f'chroma_{i}_std'] = np.std(chroma[i])\n",
    "        \n",
    "        # 5. Spectral Centroid\n",
    "        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "        features['spectral_centroid_mean'] = np.mean(spectral_centroid)\n",
    "        features['spectral_centroid_std'] = np.std(spectral_centroid)\n",
    "        \n",
    "        # 6. Spectral Rolloff\n",
    "        spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "        features['spectral_rolloff_mean'] = np.mean(spectral_rolloff)\n",
    "        features['spectral_rolloff_std'] = np.std(spectral_rolloff)\n",
    "        \n",
    "        # 7. Spectral Bandwidth\n",
    "        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "        features['spectral_bandwidth_mean'] = np.mean(spectral_bandwidth)\n",
    "        features['spectral_bandwidth_std'] = np.std(spectral_bandwidth)\n",
    "        \n",
    "        # 8. Zero Crossing Rate\n",
    "        zcr = librosa.feature.zero_crossing_rate(y)\n",
    "        features['zcr_mean'] = np.mean(zcr)\n",
    "        features['zcr_std'] = np.std(zcr)\n",
    "        \n",
    "        # 9. Tempo\n",
    "        tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
    "        features['tempo'] = tempo\n",
    "        \n",
    "        # 10. RMS Energy\n",
    "        rms = librosa.feature.rms(y=y)\n",
    "        features['rms_mean'] = np.mean(rms)\n",
    "        features['rms_std'] = np.std(rms)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "print(\"✓ Feature extraction function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b04fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_features(data_path, max_files=None):\n",
    "    \"\"\"\n",
    "    Extract features from all audio files in the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_path : str\n",
    "        Path to FMA dataset directory\n",
    "    max_files : int, optional\n",
    "        Maximum number of files to process\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : DataFrame with extracted features\n",
    "    \"\"\"\n",
    "    features_list = []\n",
    "    audio_files = []\n",
    "    \n",
    "    # Collect all MP3 files\n",
    "    for root, dirs, files in os.walk(data_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.mp3'):\n",
    "                audio_files.append(os.path.join(root, file))\n",
    "    \n",
    "    if max_files:\n",
    "        audio_files = audio_files[:max_files]\n",
    "    \n",
    "    print(f\"Found {len(audio_files)} audio files\")\n",
    "    print(\"Extracting features...\")\n",
    "    \n",
    "    # Extract features with progress bar\n",
    "    for audio_path in tqdm(audio_files):\n",
    "        features = extract_audio_features(\n",
    "            audio_path,\n",
    "            sr=CONFIG['sr'],\n",
    "            duration=CONFIG['duration']\n",
    "        )\n",
    "        \n",
    "        if features:\n",
    "            # Add track_id (from filename)\n",
    "            track_id = os.path.basename(audio_path).replace('.mp3', '')\n",
    "            features['track_id'] = track_id\n",
    "            features_list.append(features)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(features_list)\n",
    "    \n",
    "    # Reorder columns (track_id first)\n",
    "    cols = ['track_id'] + [col for col in df.columns if col != 'track_id']\n",
    "    df = df[cols]\n",
    "    \n",
    "    print(f\"\\n✓ Feature extraction complete!\")\n",
    "    print(f\"  Total tracks processed: {len(df)}\")\n",
    "    print(f\"  Total features per track: {len(df.columns) - 1}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"✓ Batch extraction function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e3f7c2",
   "metadata": {},
   "source": [
    "### 3.1 Extract Features (Optional - Skip if already extracted)\n",
    "\n",
    "**Note**: If you have already extracted features and saved them to a CSV file, you can skip this cell and load the existing features in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4248a0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from audio files\n",
    "# Uncomment to run feature extraction\n",
    "# features_df = extract_all_features(CONFIG['data_path'], max_files=CONFIG['max_files'])\n",
    "# features_df.to_csv(os.path.join(CONFIG['output_dir'], 'extracted_features.csv'), index=False)\n",
    "# print(f\"Features saved to {CONFIG['output_dir']}/extracted_features.csv\")\n",
    "\n",
    "# Load existing features (if already extracted)\n",
    "features_path = os.path.join(CONFIG['output_dir'], 'extracted_features.csv')\n",
    "if os.path.exists(features_path):\n",
    "    features_df = pd.read_csv(features_path)\n",
    "    print(f\"✓ Loaded existing features from {features_path}\")\n",
    "    print(f\"  Shape: {features_df.shape}\")\n",
    "else:\n",
    "    print(f\"⚠ Features file not found at {features_path}\")\n",
    "    print(\"  Please run feature extraction or check the path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea240232",
   "metadata": {},
   "source": [
    "## 4. Data Analysis and Cleaning\n",
    "\n",
    "Perform statistical analysis, outlier detection, and data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23d81ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data information\n",
    "print(\"=\"*70)\n",
    "print(\"DATA OVERVIEW\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Shape: {features_df.shape}\")\n",
    "print(f\"Total tracks: {len(features_df)}\")\n",
    "print(f\"Total features: {len(features_df.columns) - 1}\")\n",
    "print(f\"\\nFirst few columns: {list(features_df.columns[:10])}\")\n",
    "print(f\"\\nData types:\\n{features_df.dtypes.value_counts()}\")\n",
    "\n",
    "# Check for missing values\n",
    "missing = features_df.isnull().sum()\n",
    "if missing.sum() > 0:\n",
    "    print(f\"\\n⚠ Missing values detected:\")\n",
    "    print(missing[missing > 0])\n",
    "else:\n",
    "    print(\"\\n✓ No missing values\")\n",
    "\n",
    "# Display sample\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395aaea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "print(\"=\"*70)\n",
    "print(\"DESCRIPTIVE STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Exclude track_id column\n",
    "numeric_cols = [col for col in features_df.columns if col != 'track_id']\n",
    "stats_df = features_df[numeric_cols].describe()\n",
    "\n",
    "print(stats_df)\n",
    "\n",
    "# Show a subset of features for better visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "fig.suptitle('Distribution of Selected Features', fontsize=16)\n",
    "\n",
    "sample_features = numeric_cols[:6]\n",
    "for idx, feature in enumerate(sample_features):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    ax.hist(features_df[feature], bins=30, edgecolor='black', alpha=0.7)\n",
    "    ax.set_title(feature)\n",
    "    ax.set_xlabel('Value')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd049e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection using IQR method\n",
    "def detect_outliers_iqr(df, columns, threshold=1.5):\n",
    "    \"\"\"Detect outliers using IQR method.\"\"\"\n",
    "    outlier_indices = set()\n",
    "    \n",
    "    for col in columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        lower_bound = Q1 - threshold * IQR\n",
    "        upper_bound = Q3 + threshold * IQR\n",
    "        \n",
    "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)].index\n",
    "        outlier_indices.update(outliers)\n",
    "    \n",
    "    return list(outlier_indices)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"OUTLIER DETECTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "numeric_cols = [col for col in features_df.columns if col != 'track_id']\n",
    "outlier_indices = detect_outliers_iqr(features_df, numeric_cols)\n",
    "\n",
    "print(f\"Total outliers detected: {len(outlier_indices)}\")\n",
    "print(f\"Percentage of outliers: {len(outlier_indices)/len(features_df)*100:.2f}%\")\n",
    "\n",
    "# Option 1: Remove outliers\n",
    "# features_cleaned = features_df.drop(outlier_indices).reset_index(drop=True)\n",
    "\n",
    "# Option 2: Keep all data (recommended for music features)\n",
    "features_cleaned = features_df.copy()\n",
    "\n",
    "print(f\"\\nCleaned data shape: {features_cleaned.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293cc44e",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing\n",
    "\n",
    "Standardize features and apply PCA for dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a15ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"DATA PREPROCESSING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Separate track IDs and features\n",
    "track_ids = features_cleaned['track_id'].values\n",
    "X = features_cleaned.drop('track_id', axis=1).values\n",
    "\n",
    "print(f\"Original feature matrix shape: {X.shape}\")\n",
    "\n",
    "# Step 1: Standardization (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(f\"✓ Features standardized (mean=0, std=1)\")\n",
    "\n",
    "# Step 2: PCA for dimensionality reduction\n",
    "n_components = CONFIG['n_pca_components']\n",
    "pca = PCA(n_components=n_components, random_state=CONFIG['random_state'])\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_.sum()\n",
    "print(f\"✓ PCA applied: {X.shape[1]} features → {n_components} components\")\n",
    "print(f\"  Explained variance: {explained_variance*100:.2f}%\")\n",
    "\n",
    "# Visualize explained variance\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, n_components+1), pca.explained_variance_ratio_, 'bo-')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Scree Plot - Individual Explained Variance')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, n_components+1), np.cumsum(pca.explained_variance_ratio_), 'ro-')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Cumulative Explained Variance')\n",
    "plt.axhline(y=0.95, color='g', linestyle='--', label='95% threshold')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Preprocessed data shape: {X_pca.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8da5fc",
   "metadata": {},
   "source": [
    "## 6. Clustering Algorithms\n",
    "\n",
    "Implement multiple clustering algorithms:\n",
    "1. **K-Means**: Partitioning-based clustering\n",
    "2. **Spectral Clustering**: Graph-based clustering\n",
    "3. **DBSCAN**: Density-based clustering\n",
    "4. **GMM**: Probabilistic model-based clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d255556",
   "metadata": {},
   "source": [
    "### 6.1 K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bd1acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"K-MEANS CLUSTERING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Apply K-Means\n",
    "kmeans = KMeans(\n",
    "    n_clusters=CONFIG['n_clusters'],\n",
    "    random_state=CONFIG['random_state'],\n",
    "    n_init=10,\n",
    "    max_iter=300\n",
    ")\n",
    "\n",
    "labels_kmeans = kmeans.fit_predict(X_pca)\n",
    "\n",
    "print(f\"✓ K-Means clustering complete\")\n",
    "print(f\"  Number of clusters: {CONFIG['n_clusters']}\")\n",
    "print(f\"  Inertia: {kmeans.inertia_:.2f}\")\n",
    "\n",
    "# Cluster distribution\n",
    "unique, counts = np.unique(labels_kmeans, return_counts=True)\n",
    "print(f\"\\nCluster distribution:\")\n",
    "for cluster, count in zip(unique, counts):\n",
    "    print(f\"  Cluster {cluster}: {count} tracks ({count/len(labels_kmeans)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8194314",
   "metadata": {},
   "source": [
    "### 6.2 Spectral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355947d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"SPECTRAL CLUSTERING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Apply Spectral Clustering\n",
    "spectral = SpectralClustering(\n",
    "    n_clusters=CONFIG['n_clusters'],\n",
    "    random_state=CONFIG['random_state'],\n",
    "    affinity='nearest_neighbors',\n",
    "    n_neighbors=10\n",
    ")\n",
    "\n",
    "labels_spectral = spectral.fit_predict(X_pca)\n",
    "\n",
    "print(f\"✓ Spectral clustering complete\")\n",
    "print(f\"  Number of clusters: {CONFIG['n_clusters']}\")\n",
    "\n",
    "# Cluster distribution\n",
    "unique, counts = np.unique(labels_spectral, return_counts=True)\n",
    "print(f\"\\nCluster distribution:\")\n",
    "for cluster, count in zip(unique, counts):\n",
    "    print(f\"  Cluster {cluster}: {count} tracks ({count/len(labels_spectral)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9594e0",
   "metadata": {},
   "source": [
    "### 6.3 DBSCAN Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d81e1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"DBSCAN CLUSTERING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Determine optimal eps using k-distance plot\n",
    "k = 5\n",
    "neighbors = NearestNeighbors(n_neighbors=k)\n",
    "neighbors.fit(X_pca)\n",
    "distances, indices = neighbors.kneighbors(X_pca)\n",
    "distances = np.sort(distances[:, k-1], axis=0)\n",
    "\n",
    "# Plot k-distance\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(distances)\n",
    "plt.xlabel('Data Points sorted by distance')\n",
    "plt.ylabel(f'{k}-NN Distance')\n",
    "plt.title(f'K-Distance Plot (k={k})')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Apply DBSCAN with chosen eps and min_samples\n",
    "eps = 2.5  # Adjust based on the k-distance plot\n",
    "min_samples = 5\n",
    "\n",
    "dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "labels_dbscan = dbscan.fit_predict(X_pca)\n",
    "\n",
    "n_clusters = len(set(labels_dbscan)) - (1 if -1 in labels_dbscan else 0)\n",
    "n_noise = list(labels_dbscan).count(-1)\n",
    "\n",
    "print(f\"✓ DBSCAN clustering complete\")\n",
    "print(f\"  eps: {eps}, min_samples: {min_samples}\")\n",
    "print(f\"  Number of clusters: {n_clusters}\")\n",
    "print(f\"  Number of noise points: {n_noise} ({n_noise/len(labels_dbscan)*100:.1f}%)\")\n",
    "\n",
    "# Cluster distribution (excluding noise)\n",
    "unique, counts = np.unique(labels_dbscan[labels_dbscan != -1], return_counts=True)\n",
    "if len(unique) > 0:\n",
    "    print(f\"\\nCluster distribution (excluding noise):\")\n",
    "    for cluster, count in zip(unique, counts):\n",
    "        print(f\"  Cluster {cluster}: {count} tracks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7280b5",
   "metadata": {},
   "source": [
    "### 6.4 Gaussian Mixture Model (GMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696f4bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"GAUSSIAN MIXTURE MODEL (GMM)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Apply GMM\n",
    "gmm = GaussianMixture(\n",
    "    n_components=CONFIG['n_clusters'],\n",
    "    random_state=CONFIG['random_state'],\n",
    "    covariance_type='full',\n",
    "    max_iter=100\n",
    ")\n",
    "\n",
    "labels_gmm = gmm.fit_predict(X_pca)\n",
    "\n",
    "print(f\"✓ GMM clustering complete\")\n",
    "print(f\"  Number of components: {CONFIG['n_clusters']}\")\n",
    "print(f\"  Converged: {gmm.converged_}\")\n",
    "print(f\"  BIC: {gmm.bic(X_pca):.2f}\")\n",
    "print(f\"  AIC: {gmm.aic(X_pca):.2f}\")\n",
    "\n",
    "# Cluster distribution\n",
    "unique, counts = np.unique(labels_gmm, return_counts=True)\n",
    "print(f\"\\nCluster distribution:\")\n",
    "for cluster, count in zip(unique, counts):\n",
    "    print(f\"  Cluster {cluster}: {count} tracks ({count/len(labels_gmm)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167b5001",
   "metadata": {},
   "source": [
    "## 7. Evaluation Metrics\n",
    "\n",
    "Evaluate clustering quality using multiple internal metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b344e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_clustering(X, labels, algorithm_name):\n",
    "    \"\"\"\n",
    "    Evaluate clustering using internal metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like\n",
    "        Feature matrix\n",
    "    labels : array-like\n",
    "        Cluster labels\n",
    "    algorithm_name : str\n",
    "        Name of the algorithm\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary with evaluation metrics\n",
    "    \"\"\"\n",
    "    # Remove noise points for DBSCAN\n",
    "    if -1 in labels:\n",
    "        mask = labels != -1\n",
    "        X_filtered = X[mask]\n",
    "        labels_filtered = labels[mask]\n",
    "        n_noise = np.sum(labels == -1)\n",
    "    else:\n",
    "        X_filtered = X\n",
    "        labels_filtered = labels\n",
    "        n_noise = 0\n",
    "    \n",
    "    n_clusters = len(np.unique(labels_filtered))\n",
    "    \n",
    "    # Skip if too few clusters\n",
    "    if n_clusters < 2:\n",
    "        return {\n",
    "            'Algorithm': algorithm_name,\n",
    "            'N_Clusters': n_clusters,\n",
    "            'Silhouette': np.nan,\n",
    "            'Davies-Bouldin': np.nan,\n",
    "            'Calinski-Harabasz': np.nan,\n",
    "            'Noise_Points': n_noise\n",
    "        }\n",
    "    \n",
    "    # Calculate metrics\n",
    "    silhouette = silhouette_score(X_filtered, labels_filtered)\n",
    "    davies_bouldin = davies_bouldin_score(X_filtered, labels_filtered)\n",
    "    calinski_harabasz = calinski_harabasz_score(X_filtered, labels_filtered)\n",
    "    \n",
    "    return {\n",
    "        'Algorithm': algorithm_name,\n",
    "        'N_Clusters': n_clusters,\n",
    "        'Silhouette': silhouette,\n",
    "        'Davies-Bouldin': davies_bouldin,\n",
    "        'Calinski-Harabasz': calinski_harabasz,\n",
    "        'Noise_Points': n_noise\n",
    "    }\n",
    "\n",
    "print(\"✓ Evaluation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468a06b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Evaluate all algorithms\n",
    "results = []\n",
    "results.append(evaluate_clustering(X_pca, labels_kmeans, 'K-Means'))\n",
    "results.append(evaluate_clustering(X_pca, labels_spectral, 'Spectral'))\n",
    "results.append(evaluate_clustering(X_pca, labels_dbscan, 'DBSCAN'))\n",
    "results.append(evaluate_clustering(X_pca, labels_gmm, 'GMM'))\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\nInternal Metrics Comparison:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Interpretation guide\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"METRIC INTERPRETATION\")\n",
    "print(\"=\"*70)\n",
    "print(\"Silhouette Score: Range [-1, 1], Higher is better\")\n",
    "print(\"  > 0.7: Strong structure\")\n",
    "print(\"  0.5-0.7: Reasonable structure\")\n",
    "print(\"  0.25-0.5: Weak structure\")\n",
    "print(\"  < 0.25: No substantial structure\")\n",
    "print(\"\\nDavies-Bouldin Index: Lower is better (indicates better separation)\")\n",
    "print(\"\\nCalinski-Harabasz Index: Higher is better (indicates better defined clusters)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fc9c5c",
   "metadata": {},
   "source": [
    "## 8. Visualization\n",
    "\n",
    "Visualize clustering results using 2D projection (first 2 PCA components)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4c2db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2D visualization using first 2 PCA components\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "fig.suptitle('Clustering Results Visualization (First 2 PCA Components)', fontsize=16)\n",
    "\n",
    "algorithms = [\n",
    "    ('K-Means', labels_kmeans),\n",
    "    ('Spectral', labels_spectral),\n",
    "    ('DBSCAN', labels_dbscan),\n",
    "    ('GMM', labels_gmm)\n",
    "]\n",
    "\n",
    "for idx, (name, labels) in enumerate(algorithms):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    # Create scatter plot\n",
    "    scatter = ax.scatter(\n",
    "        X_pca[:, 0],\n",
    "        X_pca[:, 1],\n",
    "        c=labels,\n",
    "        cmap='tab10',\n",
    "        alpha=0.6,\n",
    "        s=30,\n",
    "        edgecolors='black',\n",
    "        linewidth=0.5\n",
    "    )\n",
    "    \n",
    "    ax.set_xlabel('First Principal Component')\n",
    "    ax.set_ylabel('Second Principal Component')\n",
    "    ax.set_title(f'{name} Clustering')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add colorbar\n",
    "    plt.colorbar(scatter, ax=ax, label='Cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36028123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize metrics comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle('Clustering Metrics Comparison', fontsize=16)\n",
    "\n",
    "# Filter valid results\n",
    "valid_results = results_df[results_df['Silhouette'].notna()]\n",
    "\n",
    "# Silhouette Score\n",
    "ax1 = axes[0]\n",
    "bars1 = ax1.bar(valid_results['Algorithm'], valid_results['Silhouette'], color='skyblue', edgecolor='black')\n",
    "ax1.set_ylabel('Silhouette Score')\n",
    "ax1.set_title('Silhouette Score (Higher is Better)')\n",
    "ax1.axhline(y=0.5, color='r', linestyle='--', label='Good threshold (0.5)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# Davies-Bouldin Index\n",
    "ax2 = axes[1]\n",
    "bars2 = ax2.bar(valid_results['Algorithm'], valid_results['Davies-Bouldin'], color='salmon', edgecolor='black')\n",
    "ax2.set_ylabel('Davies-Bouldin Index')\n",
    "ax2.set_title('Davies-Bouldin Index (Lower is Better)')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# Calinski-Harabasz Index\n",
    "ax3 = axes[2]\n",
    "bars3 = ax3.bar(valid_results['Algorithm'], valid_results['Calinski-Harabasz'], color='lightgreen', edgecolor='black')\n",
    "ax3.set_ylabel('Calinski-Harabasz Index')\n",
    "ax3.set_title('Calinski-Harabasz Index (Higher is Better)')\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "for bar in bars3:\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.1f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e5f371",
   "metadata": {},
   "source": [
    "## 9. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1611a7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save clustering results\n",
    "output_df = pd.DataFrame({\n",
    "    'track_id': track_ids,\n",
    "    'kmeans_cluster': labels_kmeans,\n",
    "    'spectral_cluster': labels_spectral,\n",
    "    'dbscan_cluster': labels_dbscan,\n",
    "    'gmm_cluster': labels_gmm\n",
    "})\n",
    "\n",
    "output_path = os.path.join(CONFIG['output_dir'], 'clustering_results.csv')\n",
    "output_df.to_csv(output_path, index=False)\n",
    "print(f\"✓ Clustering results saved to: {output_path}\")\n",
    "\n",
    "# Save evaluation metrics\n",
    "metrics_path = os.path.join(CONFIG['output_dir'], 'evaluation_metrics.csv')\n",
    "results_df.to_csv(metrics_path, index=False)\n",
    "print(f\"✓ Evaluation metrics saved to: {metrics_path}\")\n",
    "\n",
    "# Display sample results\n",
    "print(f\"\\nSample clustering assignments:\")\n",
    "output_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68bd76d",
   "metadata": {},
   "source": [
    "## 10. Summary and Conclusions\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "Based on the evaluation metrics, we can compare the performance of different clustering algorithms:\n",
    "\n",
    "**Best Performing Algorithm:** The algorithm with:\n",
    "- Highest Silhouette Score (best cluster cohesion and separation)\n",
    "- Lowest Davies-Bouldin Index (best cluster separation)\n",
    "- Highest Calinski-Harabasz Index (best defined clusters)\n",
    "\n",
    "### Algorithm Characteristics:\n",
    "\n",
    "1. **K-Means**:\n",
    "   - Fast and scalable\n",
    "   - Works well with spherical clusters\n",
    "   - Requires pre-specified number of clusters\n",
    "   \n",
    "2. **Spectral Clustering**:\n",
    "   - Can capture complex cluster shapes\n",
    "   - Good for non-convex clusters\n",
    "   - More computationally expensive\n",
    "   \n",
    "3. **DBSCAN**:\n",
    "   - Density-based approach\n",
    "   - Can find arbitrary shaped clusters\n",
    "   - Identifies noise points\n",
    "   - Does not require pre-specified number of clusters\n",
    "   \n",
    "4. **GMM (Gaussian Mixture Model)**:\n",
    "   - Probabilistic approach\n",
    "   - Provides soft cluster assignments\n",
    "   - Flexible covariance structures\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Fine-tune parameters**: Adjust eps and min_samples for DBSCAN, number of clusters for K-Means, etc.\n",
    "2. **Feature engineering**: Experiment with different audio features or feature combinations\n",
    "3. **External validation**: If genre labels are available, evaluate using ARI, NMI, Purity\n",
    "4. **Domain analysis**: Examine tracks within each cluster to understand musical characteristics\n",
    "5. **Ensemble methods**: Combine multiple clustering results for more robust assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212f1097",
   "metadata": {},
   "source": [
    "## Bonus: Advanced Analysis (Optional)\n",
    "\n",
    "### Elbow Method for Optimal K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab77b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow method to find optimal number of clusters\n",
    "print(\"Finding optimal number of clusters using Elbow Method...\")\n",
    "\n",
    "k_range = range(2, 21)\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in tqdm(k_range):\n",
    "    kmeans_temp = KMeans(n_clusters=k, random_state=CONFIG['random_state'], n_init=10)\n",
    "    labels_temp = kmeans_temp.fit_predict(X_pca)\n",
    "    inertias.append(kmeans_temp.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X_pca, labels_temp))\n",
    "\n",
    "# Plot results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Elbow curve\n",
    "axes[0].plot(k_range, inertias, 'bo-')\n",
    "axes[0].set_xlabel('Number of Clusters (k)')\n",
    "axes[0].set_ylabel('Inertia (Within-cluster sum of squares)')\n",
    "axes[0].set_title('Elbow Method')\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Silhouette scores\n",
    "axes[1].plot(k_range, silhouette_scores, 'ro-')\n",
    "axes[1].set_xlabel('Number of Clusters (k)')\n",
    "axes[1].set_ylabel('Silhouette Score')\n",
    "axes[1].set_title('Silhouette Score vs. Number of Clusters')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal k based on silhouette score\n",
    "optimal_k = k_range[np.argmax(silhouette_scores)]\n",
    "print(f\"\\nOptimal number of clusters based on Silhouette Score: {optimal_k}\")\n",
    "print(f\"Best Silhouette Score: {max(silhouette_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8891247e",
   "metadata": {},
   "source": [
    "### Cluster Analysis: Feature Importance\n",
    "\n",
    "Analyze which features are most important for cluster separation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6c9e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze cluster characteristics using K-Means results\n",
    "cluster_features = pd.DataFrame(X_scaled, columns=features_cleaned.drop('track_id', axis=1).columns)\n",
    "cluster_features['cluster'] = labels_kmeans\n",
    "\n",
    "# Calculate mean feature values per cluster\n",
    "cluster_means = cluster_features.groupby('cluster').mean()\n",
    "\n",
    "# Visualize top features for each cluster\n",
    "n_top_features = 10\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "fig.suptitle('Top Features by Cluster (K-Means)', fontsize=16)\n",
    "\n",
    "for cluster_id in range(min(10, CONFIG['n_clusters'])):\n",
    "    ax = axes[cluster_id // 5, cluster_id % 5]\n",
    "    \n",
    "    # Get top features for this cluster\n",
    "    cluster_profile = cluster_means.loc[cluster_id].sort_values(ascending=False)[:n_top_features]\n",
    "    \n",
    "    ax.barh(range(len(cluster_profile)), cluster_profile.values)\n",
    "    ax.set_yticks(range(len(cluster_profile)))\n",
    "    ax.set_yticklabels(cluster_profile.index, fontsize=8)\n",
    "    ax.set_xlabel('Mean Value (standardized)')\n",
    "    ax.set_title(f'Cluster {cluster_id}')\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Cluster feature analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ef4577",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated a complete music genre clustering pipeline using the FMA dataset:\n",
    "\n",
    "✅ **Feature Extraction**: Extracted 147 audio features including MFCCs, Chroma, and Spectral features  \n",
    "✅ **Data Preprocessing**: Applied standardization and PCA for dimensionality reduction  \n",
    "✅ **Clustering**: Implemented 4 different algorithms (K-Means, Spectral, DBSCAN, GMM)  \n",
    "✅ **Evaluation**: Compared algorithms using Silhouette, Davies-Bouldin, and Calinski-Harabasz metrics  \n",
    "✅ **Visualization**: Created comprehensive visualizations of results  \n",
    "\n",
    "### References:\n",
    "- FMA Dataset: https://github.com/mdeff/fma\n",
    "- Librosa Documentation: https://librosa.org/\n",
    "- Scikit-learn Clustering: https://scikit-learn.org/stable/modules/clustering.html\n",
    "\n",
    "---\n",
    "\n",
    "**Author**: FMA Music Genre Clustering Project  \n",
    "**Date**: 2025"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
