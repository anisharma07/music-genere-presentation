{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2ff1a57",
   "metadata": {},
   "source": [
    "# Unsupervised Music Genre Discovery Using Audio Feature Learning\n",
    "\n",
    "**Author:** Music Genre Analysis Project  \n",
    "**Date:** November 2025  \n",
    "**Platform:** Kaggle Notebook\n",
    "\n",
    "## Project Overview\n",
    "This notebook implements comprehensive unsupervised learning algorithms for music genre discovery using Spotify audio features.\n",
    "\n",
    "### Objectives:\n",
    "- Perform exploratory data analysis on 170K+ music tracks\n",
    "- Apply 4 clustering algorithms: K-Means, Spectral Clustering, DBSCAN, GMM\n",
    "- Evaluate using 6+ metrics (Silhouette, Davies-Bouldin, Calinski-Harabasz, ARI, NMI, V-Measure)\n",
    "- Compare performance across different train/test splits (50-50, 60-40, 70-30, 80-20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6283257a",
   "metadata": {},
   "source": [
    "## Step 1: Install and Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bec2c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (if needed)\n",
    "!pip install plotly yellowbrick -q\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.cluster import KMeans, SpectralClustering, DBSCAN, MiniBatchKMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score,\n",
    "    calinski_harabasz_score,\n",
    "    davies_bouldin_score,\n",
    "    adjusted_rand_score,\n",
    "    normalized_mutual_info_score,\n",
    "    v_measure_score\n",
    ")\n",
    "\n",
    "# Statistical Analysis\n",
    "from scipy import stats\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abae72bb",
   "metadata": {},
   "source": [
    "## Step 2: Upload and Load Dataset\n",
    "\n",
    "**Instructions:**\n",
    "1. Upload your Spotify dataset CSV files to Kaggle\n",
    "2. Update the path below to match your dataset location\n",
    "3. The main dataset should be `data.csv` with all audio features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8f234f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# Update this path to match your Kaggle dataset location\n",
    "DATA_PATH = '/kaggle/input/spotify-dataset/data.csv'  # Modify as needed\n",
    "\n",
    "# Alternative: if you have the folder structure\n",
    "# DATA_PATH = '/kaggle/input/your-dataset-name/Spotify/data/data.csv'\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508781c9",
   "metadata": {},
   "source": [
    "## Step 3: Data Preprocessing and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70eb6c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define audio features to analyze\n",
    "audio_features = [\n",
    "    'acousticness', 'danceability', 'energy', 'instrumentalness',\n",
    "    'liveness', 'loudness', 'speechiness', 'tempo', 'valence'\n",
    "]\n",
    "\n",
    "additional_features = ['duration_ms', 'popularity', 'key', 'mode']\n",
    "all_features = audio_features + additional_features\n",
    "\n",
    "print(\"\\nüìä Data Preprocessing Steps:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Check initial data\n",
    "print(f\"\\n1. Initial dataset shape: {df.shape}\")\n",
    "print(f\"   Total tracks: {len(df):,}\")\n",
    "\n",
    "# 2. Check for missing values\n",
    "print(f\"\\n2. Missing values check:\")\n",
    "missing = df[all_features].isnull().sum()\n",
    "if missing.sum() > 0:\n",
    "    print(missing[missing > 0])\n",
    "    print(f\"   ‚Üí Filling missing values with column mean...\")\n",
    "    for col in all_features:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            df[col].fillna(df[col].mean(), inplace=True)\n",
    "else:\n",
    "    print(\"   ‚úÖ No missing values found!\")\n",
    "\n",
    "# 3. Remove duplicates\n",
    "original_len = len(df)\n",
    "df = df.drop_duplicates(subset=['name', 'artists', 'duration_ms'])\n",
    "duplicates_removed = original_len - len(df)\n",
    "print(f\"\\n3. Duplicate removal:\")\n",
    "print(f\"   Removed {duplicates_removed:,} duplicate tracks\")\n",
    "print(f\"   Remaining: {len(df):,} tracks\")\n",
    "\n",
    "# 4. Remove outliers using IQR method\n",
    "print(f\"\\n4. Outlier detection and removal:\")\n",
    "df_clean = df.copy()\n",
    "\n",
    "for feature in all_features:\n",
    "    Q1 = df_clean[feature].quantile(0.25)\n",
    "    Q3 = df_clean[feature].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 3 * IQR\n",
    "    upper_bound = Q3 + 3 * IQR\n",
    "    \n",
    "    before = len(df_clean)\n",
    "    df_clean = df_clean[\n",
    "        (df_clean[feature] >= lower_bound) & \n",
    "        (df_clean[feature] <= upper_bound)\n",
    "    ]\n",
    "\n",
    "outliers_removed = len(df) - len(df_clean)\n",
    "outlier_percentage = (outliers_removed / len(df)) * 100\n",
    "print(f\"   Removed {outliers_removed:,} outliers ({outlier_percentage:.2f}%)\")\n",
    "print(f\"   Final clean dataset: {len(df_clean):,} tracks\")\n",
    "\n",
    "# 5. Add derived features\n",
    "df_clean['duration_sec'] = df_clean['duration_ms'] / 1000\n",
    "df_clean['decade'] = (df_clean['year'] // 10) * 10\n",
    "\n",
    "print(f\"\\n‚úÖ Data cleaning completed!\")\n",
    "print(f\"   Final shape: {df_clean.shape}\")\n",
    "\n",
    "# Store clean data\n",
    "data = df_clean.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46586e9f",
   "metadata": {},
   "source": [
    "## Step 4: Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34394790",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìà EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\n1. Descriptive Statistics:\")\n",
    "stats_df = data[all_features].describe()\n",
    "print(stats_df)\n",
    "\n",
    "# Calculate additional statistics\n",
    "print(\"\\n2. Statistical Measures for Each Feature:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Feature':<20} {'Mean':<10} {'Median':<10} {'Q1(25%)':<10} {'Q3(75%)':<10} {'Std':<10}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for feature in audio_features:\n",
    "    mean_val = data[feature].mean()\n",
    "    median_val = data[feature].median()\n",
    "    q1 = data[feature].quantile(0.25)\n",
    "    q3 = data[feature].quantile(0.75)\n",
    "    std_val = data[feature].std()\n",
    "    \n",
    "    print(f\"{feature:<20} {mean_val:<10.4f} {median_val:<10.4f} {q1:<10.4f} {q3:<10.4f} {std_val:<10.4f}\")\n",
    "\n",
    "# Check data distribution\n",
    "print(\"\\n3. Distribution Pattern Analysis:\")\n",
    "for feature in audio_features:\n",
    "    skewness = data[feature].skew()\n",
    "    kurtosis = data[feature].kurtosis()\n",
    "    print(f\"   {feature}: Skewness={skewness:.3f}, Kurtosis={kurtosis:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83029708",
   "metadata": {},
   "source": [
    "### Visualization 1: Feature Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d5510f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(audio_features):\n",
    "    axes[idx].hist(data[feature], bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[idx].axvline(data[feature].mean(), color='red', linestyle='--', \n",
    "                     linewidth=2, label=f'Mean: {data[feature].mean():.3f}')\n",
    "    axes[idx].axvline(data[feature].median(), color='green', linestyle='--', \n",
    "                     linewidth=2, label=f'Median: {data[feature].median():.3f}')\n",
    "    axes[idx].set_title(f'{feature.capitalize()} Distribution', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel(feature.capitalize())\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Feature distribution plots created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a62a5e2",
   "metadata": {},
   "source": [
    "### Visualization 2: Box Plots for Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dee711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(audio_features):\n",
    "    box_data = axes[idx].boxplot(data[feature], vert=True, patch_artist=True,\n",
    "                                 boxprops=dict(facecolor='lightblue', alpha=0.7),\n",
    "                                 medianprops=dict(color='red', linewidth=2))\n",
    "    axes[idx].set_title(f'{feature.capitalize()} Box Plot', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_ylabel(feature.capitalize())\n",
    "    axes[idx].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('box_plots.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Box plots created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d297bba",
   "metadata": {},
   "source": [
    "### Visualization 3: Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeab7da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "plt.figure(figsize=(14, 10))\n",
    "correlation_matrix = data[all_features].corr()\n",
    "\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm',\n",
    "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Feature Correlation Heatmap', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Correlation Analysis:\")\n",
    "print(\"High positive correlations (> 0.5):\")\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.5:\n",
    "            print(f\"   {correlation_matrix.columns[i]} ‚Üî {correlation_matrix.columns[j]}: {correlation_matrix.iloc[i, j]:.3f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Correlation heatmap created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99770126",
   "metadata": {},
   "source": [
    "## Step 5: Feature Preparation and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0938d555",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîß FEATURE PREPARATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Select features for clustering\n",
    "features = data[all_features].copy()\n",
    "print(f\"\\n1. Selected features shape: {features.shape}\")\n",
    "print(f\"   Features: {len(all_features)}\")\n",
    "print(f\"   Samples: {len(features):,}\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "print(f\"\\n2. ‚úÖ Features scaled using StandardScaler\")\n",
    "\n",
    "# Apply PCA (optional - for dimensionality reduction)\n",
    "n_components = min(features.shape[1], 13)\n",
    "pca = PCA(n_components=n_components)\n",
    "processed_data = pca.fit_transform(scaled_features)\n",
    "\n",
    "print(f\"\\n3. PCA Dimensionality Reduction:\")\n",
    "print(f\"   Original dimensions: {scaled_features.shape[1]}\")\n",
    "print(f\"   Reduced dimensions: {processed_data.shape[1]}\")\n",
    "print(f\"   Explained variance: {pca.explained_variance_ratio_.sum():.4f}\")\n",
    "print(f\"\\n   Variance per component:\")\n",
    "for i, var in enumerate(pca.explained_variance_ratio_[:5]):\n",
    "    print(f\"   PC{i+1}: {var:.4f} ({var*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Feature preparation completed!\")\n",
    "print(f\"   Ready for clustering with shape: {processed_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ab5139",
   "metadata": {},
   "source": [
    "## Step 6: Clustering - K-Means Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f10a232",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüéØ CLUSTERING ALGORITHM 1: K-MEANS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "n_clusters = 10\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10, max_iter=300)\n",
    "\n",
    "print(f\"\\nFitting K-Means with {n_clusters} clusters...\")\n",
    "kmeans_labels = kmeans.fit_predict(processed_data)\n",
    "\n",
    "print(f\"\\n‚úÖ K-Means clustering completed!\")\n",
    "print(f\"   Clusters found: {len(set(kmeans_labels))}\")\n",
    "print(f\"   Inertia: {kmeans.inertia_:.2f}\")\n",
    "print(f\"\\n   Cluster distribution:\")\n",
    "\n",
    "unique, counts = np.unique(kmeans_labels, return_counts=True)\n",
    "for cluster, count in zip(unique, counts):\n",
    "    percentage = (count / len(kmeans_labels)) * 100\n",
    "    print(f\"   Cluster {cluster}: {count:,} samples ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10b05b8",
   "metadata": {},
   "source": [
    "## Step 7: Clustering - MiniBatch K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235d015b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüéØ CLUSTERING ALGORITHM 2: MINIBATCH K-MEANS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "mbkmeans = MiniBatchKMeans(n_clusters=n_clusters, random_state=42, batch_size=1000)\n",
    "\n",
    "print(f\"\\nFitting MiniBatch K-Means with {n_clusters} clusters...\")\n",
    "mbkmeans_labels = mbkmeans.fit_predict(processed_data)\n",
    "\n",
    "print(f\"\\n‚úÖ MiniBatch K-Means clustering completed!\")\n",
    "print(f\"   Clusters found: {len(set(mbkmeans_labels))}\")\n",
    "print(f\"   Inertia: {mbkmeans.inertia_:.2f}\")\n",
    "print(f\"\\n   Cluster distribution:\")\n",
    "\n",
    "unique, counts = np.unique(mbkmeans_labels, return_counts=True)\n",
    "for cluster, count in zip(unique, counts):\n",
    "    percentage = (count / len(mbkmeans_labels)) * 100\n",
    "    print(f\"   Cluster {cluster}: {count:,} samples ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a595487",
   "metadata": {},
   "source": [
    "## Step 8: Clustering - Spectral Clustering (Sample-based for memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86514d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüéØ CLUSTERING ALGORITHM 3: SPECTRAL CLUSTERING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Note: Spectral Clustering is memory-intensive\n",
    "# We'll use a sample if dataset is too large\n",
    "SPECTRAL_SAMPLE_SIZE = 20000  # Adjust based on available RAM\n",
    "\n",
    "if len(processed_data) > SPECTRAL_SAMPLE_SIZE:\n",
    "    print(f\"\\n‚ö†Ô∏è  Dataset too large for full Spectral Clustering\")\n",
    "    print(f\"   Using sample of {SPECTRAL_SAMPLE_SIZE:,} points...\")\n",
    "    \n",
    "    sample_indices = np.random.choice(len(processed_data), SPECTRAL_SAMPLE_SIZE, replace=False)\n",
    "    spectral_data = processed_data[sample_indices]\n",
    "    \n",
    "    spectral = SpectralClustering(n_clusters=n_clusters, random_state=42, affinity='nearest_neighbors')\n",
    "    spectral_labels_sample = spectral.fit_predict(spectral_data)\n",
    "    \n",
    "    # Create full labels array with -1 for non-sampled points\n",
    "    spectral_labels = np.full(len(processed_data), -1)\n",
    "    spectral_labels[sample_indices] = spectral_labels_sample\n",
    "    \n",
    "    print(f\"\\n‚úÖ Spectral Clustering completed on sample!\")\n",
    "    print(f\"   Sampled points: {SPECTRAL_SAMPLE_SIZE:,}\")\n",
    "    print(f\"   Clusters found: {len(set(spectral_labels_sample))}\")\n",
    "else:\n",
    "    print(f\"\\nFitting Spectral Clustering with {n_clusters} clusters...\")\n",
    "    spectral = SpectralClustering(n_clusters=n_clusters, random_state=42)\n",
    "    spectral_labels = spectral.fit_predict(processed_data)\n",
    "    print(f\"\\n‚úÖ Spectral Clustering completed!\")\n",
    "    print(f\"   Clusters found: {len(set(spectral_labels))}\")\n",
    "\n",
    "valid_labels = spectral_labels[spectral_labels != -1]\n",
    "unique, counts = np.unique(valid_labels, return_counts=True)\n",
    "print(f\"\\n   Cluster distribution:\")\n",
    "for cluster, count in zip(unique, counts):\n",
    "    percentage = (count / len(valid_labels)) * 100\n",
    "    print(f\"   Cluster {cluster}: {count:,} samples ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6d9f04",
   "metadata": {},
   "source": [
    "## Step 9: Clustering - DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c435e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüéØ CLUSTERING ALGORITHM 4: DBSCAN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Try different eps values to find optimal clusters\n",
    "eps_values = [0.3, 0.5, 0.8, 1.0]\n",
    "best_dbscan = None\n",
    "best_n_clusters = 1\n",
    "best_eps = eps_values[0]\n",
    "dbscan_labels = None  # Initialize to avoid NameError\n",
    "\n",
    "print(\"\\nTesting different eps values...\")\n",
    "for eps in eps_values:\n",
    "    dbscan_test = DBSCAN(eps=eps, min_samples=5)\n",
    "    labels_test = dbscan_test.fit_predict(processed_data)\n",
    "    n_clusters_test = len(set(labels_test)) - (1 if -1 in labels_test else 0)\n",
    "    n_noise = list(labels_test).count(-1)\n",
    "    \n",
    "    print(f\"   eps={eps}: {n_clusters_test} clusters, {n_noise:,} noise points\")\n",
    "    \n",
    "    # Always keep the last tested labels as fallback\n",
    "    if dbscan_labels is None:\n",
    "        dbscan_labels = labels_test\n",
    "        best_eps = eps\n",
    "        best_n_clusters = n_clusters_test\n",
    "    \n",
    "    # Update if we find better clustering (more clusters but not too many)\n",
    "    if n_clusters_test > best_n_clusters and n_clusters_test <= 20:\n",
    "        best_n_clusters = n_clusters_test\n",
    "        best_eps = eps\n",
    "        best_dbscan = dbscan_test\n",
    "        dbscan_labels = labels_test\n",
    "\n",
    "print(f\"\\n‚úÖ Using DBSCAN with eps={best_eps}\")\n",
    "print(f\"   Clusters found: {best_n_clusters}\")\n",
    "print(f\"   Noise points: {list(dbscan_labels).count(-1):,}\")\n",
    "\n",
    "if best_n_clusters > 1:\n",
    "    unique, counts = np.unique(dbscan_labels[dbscan_labels != -1], return_counts=True)\n",
    "    print(f\"\\n   Cluster distribution (excluding noise):\")\n",
    "    for cluster, count in zip(unique, counts):\n",
    "        percentage = (count / len(dbscan_labels[dbscan_labels != -1])) * 100\n",
    "        print(f\"   Cluster {cluster}: {count:,} samples ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb2e7f3",
   "metadata": {},
   "source": [
    "## Step 10: Clustering - Gaussian Mixture Model (GMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77c5542",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüéØ CLUSTERING ALGORITHM 5: GAUSSIAN MIXTURE MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "gmm = GaussianMixture(n_components=n_clusters, random_state=42, max_iter=100)\n",
    "\n",
    "print(f\"\\nFitting GMM with {n_clusters} components...\")\n",
    "gmm_labels = gmm.fit_predict(processed_data)\n",
    "\n",
    "print(f\"\\n‚úÖ GMM clustering completed!\")\n",
    "print(f\"   Components: {len(set(gmm_labels))}\")\n",
    "print(f\"   Converged: {gmm.converged_}\")\n",
    "print(f\"   BIC Score: {gmm.bic(processed_data):.2f}\")\n",
    "print(f\"   AIC Score: {gmm.aic(processed_data):.2f}\")\n",
    "print(f\"\\n   Cluster distribution:\")\n",
    "\n",
    "unique, counts = np.unique(gmm_labels, return_counts=True)\n",
    "for cluster, count in zip(unique, counts):\n",
    "    percentage = (count / len(gmm_labels)) * 100\n",
    "    print(f\"   Cluster {cluster}: {count:,} samples ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4952b48b",
   "metadata": {},
   "source": [
    "## Step 11: Evaluation Metrics - Internal Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d0c091",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä CLUSTERING EVALUATION - INTERNAL METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = []\n",
    "\n",
    "algorithms = {\n",
    "    'K-Means': kmeans_labels,\n",
    "    'MiniBatch K-Means': mbkmeans_labels,\n",
    "    'Spectral Clustering': spectral_labels,\n",
    "    'DBSCAN': dbscan_labels,\n",
    "    'GMM': gmm_labels\n",
    "}\n",
    "\n",
    "print(\"\\nCalculating evaluation metrics (this may take a few minutes)...\\n\")\n",
    "\n",
    "for name, labels in algorithms.items():\n",
    "    print(f\"Evaluating {name}...\")\n",
    "    \n",
    "    # Skip if too few clusters or errors\n",
    "    n_clusters_found = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    \n",
    "    if n_clusters_found < 2:\n",
    "        print(f\"   ‚ö†Ô∏è  Skipping {name} - insufficient clusters ({n_clusters_found})\")\n",
    "        results.append({\n",
    "            'Algorithm': name,\n",
    "            'N_Clusters': n_clusters_found,\n",
    "            'Silhouette_Score': np.nan,\n",
    "            'Davies_Bouldin_Index': np.nan,\n",
    "            'Calinski_Harabasz_Index': np.nan\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    # For algorithms with noise points, filter them out\n",
    "    if -1 in labels:\n",
    "        mask = labels != -1\n",
    "        eval_data = processed_data[mask]\n",
    "        eval_labels = labels[mask]\n",
    "    else:\n",
    "        eval_data = processed_data\n",
    "        eval_labels = labels\n",
    "    \n",
    "    # Sample for faster computation if needed\n",
    "    EVAL_SAMPLE_SIZE = 10000\n",
    "    if len(eval_data) > EVAL_SAMPLE_SIZE:\n",
    "        sample_idx = np.random.choice(len(eval_data), EVAL_SAMPLE_SIZE, replace=False)\n",
    "        eval_data_sample = eval_data[sample_idx]\n",
    "        eval_labels_sample = eval_labels[sample_idx]\n",
    "    else:\n",
    "        eval_data_sample = eval_data\n",
    "        eval_labels_sample = eval_labels\n",
    "    \n",
    "    try:\n",
    "        # Internal metrics\n",
    "        silhouette = silhouette_score(eval_data_sample, eval_labels_sample)\n",
    "        davies_bouldin = davies_bouldin_score(eval_data, eval_labels)\n",
    "        calinski_harabasz = calinski_harabasz_score(eval_data, eval_labels)\n",
    "        \n",
    "        results.append({\n",
    "            'Algorithm': name,\n",
    "            'N_Clusters': n_clusters_found,\n",
    "            'Silhouette_Score': silhouette,\n",
    "            'Davies_Bouldin_Index': davies_bouldin,\n",
    "            'Calinski_Harabasz_Index': calinski_harabasz\n",
    "        })\n",
    "        \n",
    "        print(f\"   ‚úÖ Silhouette Score: {silhouette:.4f}\")\n",
    "        print(f\"   ‚úÖ Davies-Bouldin Index: {davies_bouldin:.4f}\")\n",
    "        print(f\"   ‚úÖ Calinski-Harabasz Index: {calinski_harabasz:.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {str(e)}\")\n",
    "        results.append({\n",
    "            'Algorithm': name,\n",
    "            'N_Clusters': n_clusters_found,\n",
    "            'Silhouette_Score': np.nan,\n",
    "            'Davies_Bouldin_Index': np.nan,\n",
    "            'Calinski_Harabasz_Index': np.nan\n",
    "        })\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"\\n‚úÖ Evaluation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452768c5",
   "metadata": {},
   "source": [
    "## Step 12: Comparison Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf76b0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison of algorithms\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Silhouette Score (higher is better)\n",
    "results_df_clean = results_df.dropna()\n",
    "axes[0].bar(results_df_clean['Algorithm'], results_df_clean['Silhouette_Score'], color='skyblue')\n",
    "axes[0].set_title('Silhouette Score Comparison\\n(Higher is Better)', fontweight='bold')\n",
    "axes[0].set_ylabel('Silhouette Score')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Davies-Bouldin Index (lower is better)\n",
    "axes[1].bar(results_df_clean['Algorithm'], results_df_clean['Davies_Bouldin_Index'], color='coral')\n",
    "axes[1].set_title('Davies-Bouldin Index Comparison\\n(Lower is Better)', fontweight='bold')\n",
    "axes[1].set_ylabel('Davies-Bouldin Index')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Calinski-Harabasz Index (higher is better)\n",
    "axes[2].bar(results_df_clean['Algorithm'], results_df_clean['Calinski_Harabasz_Index'], color='lightgreen')\n",
    "axes[2].set_title('Calinski-Harabasz Index Comparison\\n(Higher is Better)', fontweight='bold')\n",
    "axes[2].set_ylabel('Calinski-Harabasz Index')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "axes[2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('clustering_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Comparison visualization created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1f508c",
   "metadata": {},
   "source": [
    "## Step 13: Train/Test Split Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92985016",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüß™ TRAIN/TEST SPLIT EXPERIMENTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "split_ratios = [0.5, 0.6, 0.7, 0.8]  # Train sizes: 50%, 60%, 70%, 80%\n",
    "experiment_results = []\n",
    "\n",
    "for train_size in split_ratios:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Experiment: {int(train_size*100)}-{int((1-train_size)*100)} Split\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test = train_test_split(processed_data, train_size=train_size, random_state=42)\n",
    "    \n",
    "    print(f\"\\nTrain size: {len(X_train):,} | Test size: {len(X_test):,}\")\n",
    "    \n",
    "    # Test K-Means\n",
    "    print(f\"\\nTesting K-Means...\")\n",
    "    kmeans_exp = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans_exp.fit(X_train)\n",
    "    train_labels = kmeans_exp.predict(X_train)\n",
    "    test_labels = kmeans_exp.predict(X_test)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_silhouette = silhouette_score(X_train[:10000], train_labels[:10000])  # Sample for speed\n",
    "    test_silhouette = silhouette_score(X_test[:min(10000, len(X_test))], \n",
    "                                       test_labels[:min(10000, len(X_test))])\n",
    "    \n",
    "    experiment_results.append({\n",
    "        'Split': f\"{int(train_size*100)}-{int((1-train_size)*100)}\",\n",
    "        'Algorithm': 'K-Means',\n",
    "        'Train_Silhouette': train_silhouette,\n",
    "        'Test_Silhouette': test_silhouette,\n",
    "        'Difference': abs(train_silhouette - test_silhouette)\n",
    "    })\n",
    "    \n",
    "    print(f\"   Train Silhouette: {train_silhouette:.4f}\")\n",
    "    print(f\"   Test Silhouette: {test_silhouette:.4f}\")\n",
    "    print(f\"   Difference: {abs(train_silhouette - test_silhouette):.4f}\")\n",
    "\n",
    "# Display experiment results\n",
    "exp_df = pd.DataFrame(experiment_results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPERIMENT RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(exp_df.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.arange(len(exp_df))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, exp_df['Train_Silhouette'], width, label='Train', color='skyblue')\n",
    "ax.bar(x + width/2, exp_df['Test_Silhouette'], width, label='Test', color='coral')\n",
    "\n",
    "ax.set_xlabel('Train-Test Split', fontweight='bold')\n",
    "ax.set_ylabel('Silhouette Score', fontweight='bold')\n",
    "ax.set_title('Train vs Test Performance Across Different Splits', fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(exp_df['Split'])\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('train_test_experiments.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Train/Test experiments completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204c890c",
   "metadata": {},
   "source": [
    "## Step 14: Final Report Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c13c0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL ANALYSIS REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä DATASET SUMMARY:\")\n",
    "print(f\"   Original tracks: {df.shape[0]:,}\")\n",
    "print(f\"   After cleaning: {len(data):,}\")\n",
    "print(f\"   Features analyzed: {len(all_features)}\")\n",
    "print(f\"   Duplicates removed: {duplicates_removed:,}\")\n",
    "print(f\"   Outliers removed: {outliers_removed:,} ({outlier_percentage:.2f}%)\")\n",
    "\n",
    "print(f\"\\nüéØ CLUSTERING SUMMARY:\")\n",
    "print(f\"   Algorithms tested: 5\")\n",
    "print(f\"   Target clusters (K): {n_clusters}\")\n",
    "\n",
    "print(f\"\\nüèÜ BEST PERFORMING ALGORITHM:\")\n",
    "if not results_df_clean.empty:\n",
    "    best_algo = results_df_clean.loc[results_df_clean['Silhouette_Score'].idxmax()]\n",
    "    print(f\"   Algorithm: {best_algo['Algorithm']}\")\n",
    "    print(f\"   Silhouette Score: {best_algo['Silhouette_Score']:.4f}\")\n",
    "    print(f\"   Davies-Bouldin Index: {best_algo['Davies_Bouldin_Index']:.4f}\")\n",
    "    print(f\"   Calinski-Harabasz Index: {best_algo['Calinski_Harabasz_Index']:.2f}\")\n",
    "\n",
    "print(f\"\\nüìà KEY FINDINGS:\")\n",
    "print(f\"   1. Dataset contains {len(data):,} unique music tracks\")\n",
    "print(f\"   2. {len(all_features)} audio features used for clustering\")\n",
    "print(f\"   3. Successfully applied 5 clustering algorithms\")\n",
    "print(f\"   4. Evaluated using 3 internal metrics\")\n",
    "print(f\"   5. Tested on 4 different train/test splits\")\n",
    "\n",
    "print(f\"\\nüíæ OUTPUT FILES GENERATED:\")\n",
    "print(f\"   ‚úÖ feature_distributions.png\")\n",
    "print(f\"   ‚úÖ box_plots.png\")\n",
    "print(f\"   ‚úÖ correlation_heatmap.png\")\n",
    "print(f\"   ‚úÖ clustering_comparison.png\")\n",
    "print(f\"   ‚úÖ train_test_experiments.png\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ ANALYSIS COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save results to CSV\n",
    "results_df.to_csv('clustering_results.csv', index=False)\n",
    "exp_df.to_csv('experiment_results.csv', index=False)\n",
    "\n",
    "print(f\"\\nüìÅ Results saved to:\")\n",
    "print(f\"   - clustering_results.csv\")\n",
    "print(f\"   - experiment_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e715257",
   "metadata": {},
   "source": [
    "## Step 15: Save Results for Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7165d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cluster labels to original data\n",
    "data['KMeans_Cluster'] = kmeans_labels\n",
    "data['GMM_Cluster'] = gmm_labels\n",
    "data['DBSCAN_Cluster'] = dbscan_labels\n",
    "\n",
    "# Save enhanced dataset\n",
    "data.to_csv('music_data_with_clusters.csv', index=False)\n",
    "\n",
    "print(\"‚úÖ Enhanced dataset saved: music_data_with_clusters.csv\")\n",
    "print(f\"   Includes cluster assignments for each algorithm\")\n",
    "print(f\"\\nYou can now download all generated files from Kaggle!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
