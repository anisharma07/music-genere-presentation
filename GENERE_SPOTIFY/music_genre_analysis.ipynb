{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Music Genre Discovery Using Audio Feature Learning\n",
    "\n",
    "This notebook provides an interactive analysis of music genre discovery using clustering algorithms on Spotify audio features.\n",
    "\n",
    "## Overview\n",
    "- **Dataset**: Spotify audio features (~170K tracks)\n",
    "- **Algorithms**: K-Means, Spectral, DBSCAN, GMM\n",
    "- **Evaluation**: Internal and external clustering metrics\n",
    "- **Visualization**: Comprehensive plots and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from music_genre_analysis import MusicGenreAnalyzer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize Analyzer and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the analyzer\n",
    "analyzer = MusicGenreAnalyzer()\n",
    "\n",
    "# Load and preprocess data\n",
    "print(\"Loading and preprocessing data...\")\n",
    "data = analyzer.load_and_preprocess_data()\n",
    "\n",
    "# Display basic information\n",
    "print(f\"\\nDataset Shape: {data.shape}\")\n",
    "print(f\"Columns: {list(data.columns)}\")\n",
    "print(f\"Year Range: {data['year'].min()} - {data['year'].max()}\")\n",
    "\n",
    "# Show first few rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform comprehensive EDA\n",
    "print(\"Performing Exploratory Data Analysis...\")\n",
    "eda_results = analyzer.exploratory_data_analysis()\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\nBasic Statistics:\")\n",
    "eda_results['basic_stats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature distributions\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, feature in enumerate(analyzer.audio_features):\n",
    "    if i < len(axes):\n",
    "        data[feature].hist(bins=50, ax=axes[i], alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        axes[i].set_title(f'Distribution of {feature.capitalize()}', fontsize=12, fontweight='bold')\n",
    "        axes[i].set_xlabel(feature.capitalize())\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Audio Feature Distributions', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = data[analyzer.all_features].corr()\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='RdBu_r',\n",
    "           center=0, square=True, fmt='.2f', \n",
    "           cbar_kws={\"shrink\": .8, \"label\": \"Correlation Coefficient\"})\n",
    "\n",
    "plt.title('Audio Features Correlation Matrix', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display highly correlated pairs\n",
    "corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_val = correlation_matrix.iloc[i, j]\n",
    "        if abs(corr_val) > 0.5:  # Threshold for high correlation\n",
    "            corr_pairs.append({\n",
    "                'Feature 1': correlation_matrix.columns[i],\n",
    "                'Feature 2': correlation_matrix.columns[j],\n",
    "                'Correlation': corr_val\n",
    "            })\n",
    "\n",
    "if corr_pairs:\n",
    "    corr_df = pd.DataFrame(corr_pairs).sort_values('Correlation', key=abs, ascending=False)\n",
    "    print(\"\\nHighly Correlated Feature Pairs (|correlation| > 0.5):\")\n",
    "    print(corr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for outlier detection\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, feature in enumerate(analyzer.audio_features):\n",
    "    if i < len(axes):\n",
    "        bp = axes[i].boxplot(data[feature], patch_artist=True)\n",
    "        bp['boxes'][0].set_facecolor('lightblue')\n",
    "        bp['boxes'][0].set_alpha(0.7)\n",
    "        axes[i].set_title(f'Box Plot: {feature.capitalize()}', fontsize=12, fontweight='bold')\n",
    "        axes[i].set_ylabel(feature.capitalize())\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Box Plots for Outlier Detection', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for clustering\n",
    "print(\"Preparing features for clustering...\")\n",
    "processed_features = analyzer.prepare_features()\n",
    "\n",
    "print(f\"Original features shape: {analyzer.features.shape}\")\n",
    "print(f\"Processed features shape: {processed_features.shape}\")\n",
    "print(f\"Explained variance ratio (cumulative): {analyzer.pca.explained_variance_ratio_.sum():.3f}\")\n",
    "\n",
    "# Visualize PCA components\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(range(1, len(analyzer.pca.explained_variance_ratio_) + 1), \n",
    "        analyzer.pca.explained_variance_ratio_, alpha=0.7, color='steelblue')\n",
    "plt.title('PCA Explained Variance Ratio', fontweight='bold')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "cumsum = np.cumsum(analyzer.pca.explained_variance_ratio_)\n",
    "plt.plot(range(1, len(cumsum) + 1), cumsum, marker='o', linewidth=2, color='darkred')\n",
    "plt.title('Cumulative Explained Variance', fontweight='bold')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0.95, color='green', linestyle='--', alpha=0.7, label='95% threshold')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clustering Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform clustering with all algorithms\n",
    "n_clusters = 10  # Adjust as needed\n",
    "print(f\"Performing clustering with {n_clusters} clusters...\")\n",
    "\n",
    "clustering_results = analyzer.perform_clustering(n_clusters=n_clusters)\n",
    "\n",
    "# Display clustering summary\n",
    "print(\"\\nClustering Results Summary:\")\n",
    "print(\"=\" * 50)\n",
    "for name, results in clustering_results.items():\n",
    "    if 'error' in results:\n",
    "        print(f\"{name}: ERROR - {results['error']}\")\n",
    "    else:\n",
    "        n_clusters_found = results['n_clusters']\n",
    "        print(f\"{name}: {n_clusters_found} clusters found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clustering results in PCA space\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "valid_results = [(name, results) for name, results in clustering_results.items() \n",
    "                if results['labels'] is not None and 'error' not in results]\n",
    "\n",
    "for i, (algorithm_name, results) in enumerate(valid_results[:6]):\n",
    "    if i >= len(axes):\n",
    "        break\n",
    "        \n",
    "    ax = axes[i]\n",
    "    labels = results['labels']\n",
    "    \n",
    "    # Create scatter plot\n",
    "    scatter = ax.scatter(processed_features[:, 0], processed_features[:, 1],\n",
    "                        c=labels, cmap='tab20', alpha=0.6, s=20, edgecolors='black', linewidth=0.1)\n",
    "    \n",
    "    ax.set_title(f'{algorithm_name}\\n({results[\"n_clusters\"]} clusters)', \n",
    "                fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('First Principal Component')\n",
    "    ax.set_ylabel('Second Principal Component')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add colorbar for non-DBSCAN algorithms\n",
    "    if algorithm_name != 'DBSCAN' or -1 not in labels:\n",
    "        plt.colorbar(scatter, ax=ax, label='Cluster ID')\n",
    "\n",
    "# Remove empty subplots\n",
    "for j in range(len(valid_results), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Clustering Results Visualization (PCA Space)', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Clustering Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate clustering performance\n",
    "print(\"Evaluating clustering results...\")\n",
    "evaluation_results = analyzer.evaluate_clustering()\n",
    "\n",
    "if not evaluation_results.empty:\n",
    "    # Display results table\n",
    "    print(\"\\nClustering Evaluation Results:\")\n",
    "    print(\"=\" * 100)\n",
    "    print(evaluation_results.to_string(index=False))\n",
    "    \n",
    "    # Find best performing algorithms\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"PERFORMANCE HIGHLIGHTS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    best_silhouette = evaluation_results.loc[evaluation_results['Silhouette_Score'].idxmax()]\n",
    "    print(f\"Best Silhouette Score: {best_silhouette['Algorithm']} ({best_silhouette['Silhouette_Score']:.4f})\")\n",
    "    \n",
    "    best_dbi = evaluation_results.loc[evaluation_results['Davies_Bouldin_Index'].idxmin()]\n",
    "    print(f\"Best Davies-Bouldin Index: {best_dbi['Algorithm']} ({best_dbi['Davies_Bouldin_Index']:.4f})\")\n",
    "    \n",
    "    best_chi = evaluation_results.loc[evaluation_results['Calinski_Harabasz_Index'].idxmax()]\n",
    "    print(f\"Best Calinski-Harabasz Index: {best_chi['Algorithm']} ({best_chi['Calinski_Harabasz_Index']:.2f})\")\n",
    "else:\n",
    "    print(\"No evaluation results available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize evaluation metrics\n",
    "if not evaluation_results.empty:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Internal metrics\n",
    "    metrics_to_plot = [\n",
    "        ('Silhouette_Score', 'Silhouette Score', 'Higher is Better'),\n",
    "        ('Davies_Bouldin_Index', 'Davies-Bouldin Index', 'Lower is Better'),\n",
    "        ('Calinski_Harabasz_Index', 'Calinski-Harabasz Index', 'Higher is Better'),\n",
    "        ('Normalized_Mutual_Info', 'Normalized Mutual Information', 'Higher is Better')\n",
    "    ]\n",
    "    \n",
    "    for i, (metric, title, note) in enumerate(metrics_to_plot):\n",
    "        ax = axes[i//2, i%2]\n",
    "        \n",
    "        bars = ax.bar(evaluation_results['Algorithm'], evaluation_results[metric], \n",
    "                     alpha=0.7, color='steelblue', edgecolor='black')\n",
    "        \n",
    "        ax.set_title(f'{title}\\n({note})', fontweight='bold')\n",
    "        ax.set_xlabel('Algorithm')\n",
    "        ax.set_ylabel(title)\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars, evaluation_results[metric]):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + ax.get_ylim()[1]*0.01,\n",
    "                   f'{value:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Comprehensive metrics heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Select numeric metrics for heatmap\n",
    "    numeric_cols = ['Silhouette_Score', 'Davies_Bouldin_Index', 'Calinski_Harabasz_Index',\n",
    "                   'Adjusted_Rand_Index', 'Normalized_Mutual_Info', 'V_Measure', 'Purity']\n",
    "    \n",
    "    # Create normalized heatmap data for better visualization\n",
    "    heatmap_data = evaluation_results[numeric_cols].T\n",
    "    heatmap_data.columns = evaluation_results['Algorithm']\n",
    "    \n",
    "    # Normalize metrics (invert Davies-Bouldin since lower is better)\n",
    "    normalized_data = heatmap_data.copy()\n",
    "    for col in normalized_data.columns:\n",
    "        if 'Davies_Bouldin' in normalized_data.index:\n",
    "            normalized_data.loc['Davies_Bouldin_Index', col] = 1 - normalized_data.loc['Davies_Bouldin_Index', col]\n",
    "    \n",
    "    sns.heatmap(heatmap_data, annot=True, cmap='RdYlBu_r', center=0,\n",
    "               fmt='.3f', cbar_kws={'label': 'Metric Value'})\n",
    "    \n",
    "    plt.title('Clustering Evaluation Metrics Comparison', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Clustering Algorithm')\n",
    "    plt.ylabel('Evaluation Metric')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cluster Analysis and Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze cluster characteristics for best performing algorithm\n",
    "if not evaluation_results.empty:\n",
    "    best_algorithm = evaluation_results.loc[evaluation_results['Silhouette_Score'].idxmax(), 'Algorithm']\n",
    "    best_labels = clustering_results[best_algorithm]['labels']\n",
    "    \n",
    "    print(f\"Analyzing cluster characteristics for: {best_algorithm}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Add cluster labels to original data\n",
    "    data_with_clusters = data.copy()\n",
    "    data_with_clusters['cluster'] = best_labels\n",
    "    \n",
    "    # Calculate cluster statistics\n",
    "    cluster_stats = data_with_clusters.groupby('cluster')[analyzer.audio_features].agg(['mean', 'std']).round(3)\n",
    "    \n",
    "    # Display cluster sizes\n",
    "    cluster_sizes = data_with_clusters['cluster'].value_counts().sort_index()\n",
    "    print(\"\\nCluster Sizes:\")\n",
    "    for cluster_id, size in cluster_sizes.items():\n",
    "        if cluster_id != -1:  # Exclude noise points for DBSCAN\n",
    "            percentage = (size / len(data_with_clusters)) * 100\n",
    "            print(f\"Cluster {cluster_id}: {size:,} tracks ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Visualize cluster characteristics\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Select key features for radar chart\n",
    "    key_features = ['danceability', 'energy', 'valence', 'acousticness', 'instrumentalness']\n",
    "    \n",
    "    # Create subplot for each cluster (limit to first 6 clusters)\n",
    "    valid_clusters = [c for c in cluster_sizes.index if c != -1][:6]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, cluster_id in enumerate(valid_clusters):\n",
    "        if i >= len(axes):\n",
    "            break\n",
    "            \n",
    "        ax = axes[i]\n",
    "        cluster_data = data_with_clusters[data_with_clusters['cluster'] == cluster_id]\n",
    "        \n",
    "        # Create bar plot for cluster characteristics\n",
    "        feature_means = cluster_data[key_features].mean()\n",
    "        bars = ax.bar(key_features, feature_means, alpha=0.7, color=f'C{i}', edgecolor='black')\n",
    "        \n",
    "        ax.set_title(f'Cluster {cluster_id} Characteristics\\n({len(cluster_data):,} tracks)', \n",
    "                    fontweight='bold')\n",
    "        ax.set_ylabel('Average Feature Value')\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, value in zip(bars, feature_means):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                   f'{value:.2f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # Remove empty subplots\n",
    "    for j in range(len(valid_clusters), len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f'{best_algorithm} - Cluster Characteristics Analysis', \n",
    "                fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal analysis of clusters\n",
    "if not evaluation_results.empty and 'data_with_clusters' in locals():\n",
    "    # Analyze cluster evolution over decades\n",
    "    decade_cluster_analysis = data_with_clusters.groupby(['decade', 'cluster']).size().unstack(fill_value=0)\n",
    "    \n",
    "    # Calculate proportions\n",
    "    decade_proportions = decade_cluster_analysis.div(decade_cluster_analysis.sum(axis=1), axis=0)\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Stacked area plot\n",
    "    valid_clusters_for_plot = [c for c in decade_proportions.columns if c != -1][:8]  # Limit colors\n",
    "    \n",
    "    plt.stackplot(decade_proportions.index, \n",
    "                 *[decade_proportions[cluster] for cluster in valid_clusters_for_plot],\n",
    "                 labels=[f'Cluster {c}' for c in valid_clusters_for_plot],\n",
    "                 alpha=0.8)\n",
    "    \n",
    "    plt.title('Evolution of Music Clusters Over Decades', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Decade')\n",
    "    plt.ylabel('Proportion of Tracks')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show most representative tracks for each cluster\n",
    "    print(\"\\nMost Representative Tracks by Cluster:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for cluster_id in valid_clusters[:3]:  # Show first 3 clusters\n",
    "        cluster_tracks = data_with_clusters[data_with_clusters['cluster'] == cluster_id]\n",
    "        \n",
    "        # Sample popular tracks from the cluster\n",
    "        representative_tracks = cluster_tracks.nlargest(5, 'popularity')[['name', 'artists', 'year', 'popularity']]\n",
    "        \n",
    "        print(f\"\\nCluster {cluster_id} - Top 5 Popular Tracks:\")\n",
    "        print(\"-\" * 50)\n",
    "        for idx, track in representative_tracks.iterrows():\n",
    "            print(f\"‚Ä¢ {track['name']} by {track['artists']} ({track['year']}) - Popularity: {track['popularity']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced Analysis and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "if hasattr(analyzer, 'pca'):\n",
    "    # Get PCA component loadings\n",
    "    feature_names = analyzer.all_features\n",
    "    components = analyzer.pca.components_\n",
    "    \n",
    "    # Create feature importance matrix\n",
    "    feature_importance = pd.DataFrame(\n",
    "        components[:5].T,  # First 5 components\n",
    "        index=feature_names,\n",
    "        columns=[f'PC{i+1}' for i in range(5)]\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(feature_importance, annot=True, cmap='RdBu_r', center=0,\n",
    "               fmt='.3f', cbar_kws={'label': 'Component Loading'})\n",
    "    \n",
    "    plt.title('Feature Loadings on Principal Components', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Principal Component')\n",
    "    plt.ylabel('Audio Feature')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show top contributing features for each component\n",
    "    print(\"\\nTop Contributing Features for Each Principal Component:\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for i in range(3):  # First 3 components\n",
    "        pc_name = f'PC{i+1}'\n",
    "        top_features = feature_importance[pc_name].abs().nlargest(5)\n",
    "        \n",
    "        print(f\"\\n{pc_name} (Explains {analyzer.pca.explained_variance_ratio_[i]:.1%} of variance):\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for feature, loading in top_features.items():\n",
    "            direction = \"positively\" if feature_importance.loc[feature, pc_name] > 0 else \"negatively\"\n",
    "            print(f\"‚Ä¢ {feature}: {loading:.3f} ({direction} correlated)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYSIS SUMMARY AND CONCLUSIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nüìä Dataset Overview:\")\n",
    "print(f\"   ‚Ä¢ Total tracks analyzed: {len(data):,}\")\n",
    "print(f\"   ‚Ä¢ Audio features: {len(analyzer.audio_features)}\")\n",
    "print(f\"   ‚Ä¢ Year range: {data['year'].min()} - {data['year'].max()}\")\n",
    "print(f\"   ‚Ä¢ Dimensionality reduction: {analyzer.features.shape[1]} ‚Üí {analyzer.processed_data.shape[1]} features\")\n",
    "\n",
    "if not evaluation_results.empty:\n",
    "    print(f\"\\nüéØ Clustering Results:\")\n",
    "    print(f\"   ‚Ä¢ Algorithms tested: {len(evaluation_results)}\")\n",
    "    \n",
    "    best_silhouette_row = evaluation_results.loc[evaluation_results['Silhouette_Score'].idxmax()]\n",
    "    print(f\"   ‚Ä¢ Best overall performance: {best_silhouette_row['Algorithm']}\")\n",
    "    print(f\"     - Silhouette Score: {best_silhouette_row['Silhouette_Score']:.4f}\")\n",
    "    print(f\"     - Davies-Bouldin Index: {best_silhouette_row['Davies_Bouldin_Index']:.4f}\")\n",
    "    print(f\"     - Clusters found: {best_silhouette_row['N_Clusters']}\")\n",
    "    \n",
    "    print(f\"\\nüìà Key Findings:\")\n",
    "    \n",
    "    # Algorithm performance ranking\n",
    "    silhouette_ranking = evaluation_results.sort_values('Silhouette_Score', ascending=False)\n",
    "    print(f\"   ‚Ä¢ Algorithm ranking (by Silhouette Score):\")\n",
    "    for i, (_, row) in enumerate(silhouette_ranking.iterrows(), 1):\n",
    "        print(f\"     {i}. {row['Algorithm']}: {row['Silhouette_Score']:.4f}\")\n",
    "    \n",
    "    # Feature insights\n",
    "    high_corr_features = []\n",
    "    corr_matrix = data[analyzer.audio_features].corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            if abs(corr_matrix.iloc[i, j]) > 0.6:\n",
    "                high_corr_features.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n",
    "    \n",
    "    if high_corr_features:\n",
    "        print(f\"\\n   ‚Ä¢ Highly correlated features (|r| > 0.6):\")\n",
    "        for feat1, feat2, corr in sorted(high_corr_features, key=lambda x: abs(x[2]), reverse=True)[:3]:\n",
    "            print(f\"     - {feat1} ‚Üî {feat2}: {corr:.3f}\")\n",
    "\n",
    "print(f\"\\nüí° Recommendations:\")\n",
    "print(f\"   ‚Ä¢ Use {evaluation_results.loc[evaluation_results['Silhouette_Score'].idxmax(), 'Algorithm']} for production clustering\")\n",
    "print(f\"   ‚Ä¢ Consider ensemble methods combining top-performing algorithms\")\n",
    "print(f\"   ‚Ä¢ Explore genre-specific feature engineering\")\n",
    "print(f\"   ‚Ä¢ Implement hierarchical clustering for genre taxonomies\")\n",
    "\n",
    "print(f\"\\nüìÅ Generated Outputs:\")\n",
    "print(f\"   ‚Ä¢ All visualizations and analysis plots\")\n",
    "print(f\"   ‚Ä¢ Processed feature matrices and cluster assignments\")\n",
    "print(f\"   ‚Ä¢ Comprehensive evaluation metrics\")\n",
    "print(f\"   ‚Ä¢ Interactive analysis results\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Analysis completed successfully! üéµüìä\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results and generate report\n",
    "print(\"Saving results and generating report...\")\n",
    "\n",
    "# Save all results\n",
    "analyzer.save_results(\"notebook_results/\")\n",
    "\n",
    "# Generate HTML report\n",
    "report_path = analyzer.generate_report(\"notebook_analysis_report.html\")\n",
    "\n",
    "print(f\"\\n‚úÖ Results saved successfully!\")\n",
    "print(f\"üìä HTML Report: {report_path}\")\n",
    "print(f\"üìÅ Numerical Results: notebook_results/\")\n",
    "print(f\"\\nYou can now view the comprehensive HTML report for detailed findings.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}